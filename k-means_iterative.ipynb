{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# K-means iterativo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Spiegazione codice:\n",
    "\n",
    "clustering iterativo bilanciato basato su K-means, progettato per dividere i clienti in zone geografiche da servire con un vincolo temporale forte (8 ore di turno massimo per corriere). Ecco come funziona dettagliatamente:\n",
    "\n",
    "1. setup:\n",
    "- Riceve un DataFrame con i punti di consegna (colonne: location_id, lat, lon).\n",
    "- Standardizza le coordinate geografiche (latitudine e longitudine) per rendere pi√π efficace il clustering K-means.\n",
    "- Calcola un numero iniziale di cluster K adattivo sulla base delle dimensioni del dataset (es. un cluster ogni ~200 punti).\n",
    "\n",
    "2. L‚Äôalgoritmo si basa su una procedura iterativa (massimo max_iterations):\n",
    "- Esegue K-means con il K corrente per assegnare ogni punto a un cluster.\n",
    "- Costruisce un dizionario di cluster con le liste di location_id assegnati a ciascun cluster.\n",
    "- Per ogni cluster, calcola le statistiche di performance tramite la funzione single_cluster_stats_with_cache.\n",
    "- Ottiene il tempo medio massimo di consegna (mean_minutes) per ogni cluster.\n",
    "- Individua i cluster il cui tempo medio massimo supera il vincolo di 8 ore (+ una tolleranza configurabile).\n",
    "- Se nessun cluster supera il limite, si ferma con successo.\n",
    "- Tiene traccia della migliore soluzione finora trovata (minore numero di cluster problematici).\n",
    "\n",
    "3. Riclusterizzazione Intelligente:\n",
    "- Calcola, per ogni cluster problematico, quanti sotto-cluster creare, usando una funzione basata sul rapporto tra tempo stimato e limite orario, e la dimensione del cluster.\n",
    "- Estrae i punti dei cluster problematici dal dataset.\n",
    "- Riclusterizza solo quei punti in nuovi cluster pi√π piccoli, aggiornando le assegnazioni.\n",
    "- Incrementa K in modo dinamico per suddividere meglio i cluster che superano il vincolo.\n",
    "\n",
    "4. Condizioni di Uscita: \n",
    "- Tutti i cluster rispettano il vincolo di tempo o sono entro la tolleranza.\n",
    "- Mancano miglioramenti per 3 iterazioni consecutive.\n",
    "- Viene raggiunto il numero massimo di iterazioni.\n",
    "\n",
    "5. Parallelismo e Memoria:\n",
    "- Il calcolo delle performance per ogni cluster √® eseguito in parallelo usando ThreadPoolExecutor, sfruttando al massimo i core CPU disponibili.\n",
    "- Implementa una cache persistente su disco per salvare i risultati gi√† calcolati, evitando di rifare i calcoli pesanti per cluster gi√† analizzati."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# K-means iterativo v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Descrizione e differenze col precedente\n",
    "\n",
    "1. Merge di cluster piccoli confinanti\n",
    "- Trova cluster confinanti usando triangolazione di Delaunay\n",
    "- Controlla se la somma dei tempi medi giornalieri non supera 8 ore\n",
    "- Unisce automaticamente cluster troppo piccoli (< 15 punti) se compatibili\n",
    "\n",
    "2. Ottimizzazioni di velocit√†\n",
    "- Usa calc_clusters_stats invece di single_cluster_stats_with_cache per evitare nested parallelism\n",
    "- Controlli merge in parallelo per tutte le coppie candidate\n",
    "- Cache pi√π efficiente con informazioni per weekday\n",
    "\n",
    "3. Flusso ottimizzato\n",
    "- K-means tradizionale\n",
    "- Merge cluster piccoli confinanti\n",
    "- Identifica cluster problematici (> 8 ore)\n",
    "- Riclusterizza solo quelli problematici\n",
    "- Ripete fino a convergenza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 650547 entries, 0 to 650546\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   location_id     650547 non-null  int64  \n",
      " 1   lat             650547 non-null  float64\n",
      " 2   lon             650547 non-null  float64\n",
      " 3   quantity        650547 non-null  int64  \n",
      " 4   delivery_date   650547 non-null  object \n",
      " 5   window_start_0  639992 non-null  object \n",
      " 6   window_end_0    639992 non-null  object \n",
      " 7   is_event        650547 non-null  int64  \n",
      "dtypes: float64(2), int64(3), object(3)\n",
      "memory usage: 39.7+ MB\n",
      "‚ùå Valori non convertiti in 'delivery_date':\n",
      "[]\n",
      "\n",
      "‚ùå Valori non convertibili in booleani in 'is_event':\n",
      "[]\n",
      "‚ùå Valori non convertiti in window_start_0:\n",
      "[None]\n",
      "\n",
      "‚ùå Valori non convertiti in window_end_0:\n",
      "[None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:11: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[ True False False ...  True  True  True]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 63733 entries, 0 to 650546\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   location_id     63733 non-null  int64         \n",
      " 1   lat             63733 non-null  float64       \n",
      " 2   lon             63733 non-null  float64       \n",
      " 3   quantity        63733 non-null  int64         \n",
      " 4   delivery_date   63733 non-null  datetime64[ns]\n",
      " 5   window_start_0  63590 non-null  object        \n",
      " 6   window_end_0    63590 non-null  object        \n",
      " 7   is_event        63733 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(2), int64(2), object(3)\n",
      "memory usage: 4.4+ MB\n",
      "Punti di consegna unici trovati: 3766\n",
      "Dopo l'eliminazione di punti troppo distanti, sono rimasti 3764 punti di consegna\n",
      "delivery points esempio:\n",
      "   location_id        lat        lon\n",
      "0         2884  45.710720  10.047550\n",
      "1         2885  45.670435   9.935415\n",
      "2         2886  45.671596   9.931096\n",
      "3         2888  45.683688  10.020058\n",
      "4         2889  45.666919   9.660539\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "from scipy.spatial import Delaunay\n",
    "from itertools import combinations\n",
    "# import del notebook per il calcolo del routing\n",
    "import import_ipynb\n",
    "import performance_calc as pc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## k=50 full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Dataset: 3764 punti\n",
      "üéØ K iniziale: 50\n",
      "üöÄ Inizializzato con 8 core CPU\n",
      "üéØ Inizio clustering bilanciato ottimizzato per 3764 punti\n",
      "üìÇ Caricata cache con 1875 entries\n",
      "üìä K iniziale adattivo: 50\n",
      "üîÑ Calcolo performance di 50 cluster con calc_clusters_stats...\n",
      "‚úÖ Completata valutazione ottimizzata in 993.4s\n",
      "üîÑ Calcolo performance di 44 cluster con calc_clusters_stats...\n",
      "‚úÖ Completata valutazione ottimizzata in 1015.7s\n",
      "‚úÖ Early stopping: solo 3 cluster problematici\n",
      "üíæ Salvata cache con 1878 entries\n"
     ]
    }
   ],
   "source": [
    "class OptimizedBalancedClustering:\n",
    "    def __init__(self, \n",
    "                 max_shift_time_min: int = 480,\n",
    "                 n_cores: int = None,\n",
    "                 cache_dir: str = \"./cluster_cache\"):\n",
    "        \n",
    "        self.max_shift_time_min = max_shift_time_min\n",
    "        self.n_cores = n_cores or max(1, mp.cpu_count() - 1)\n",
    "        self.cache_dir = cache_dir\n",
    "        self.global_cache = {}\n",
    "        \n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        print(f\"üöÄ Inizializzato con {self.n_cores} core CPU\")\n",
    "    \n",
    "    def _cache_key(self, location_ids):\n",
    "        return hash(tuple(sorted(location_ids)))\n",
    "    \n",
    "    def _load_cache(self):\n",
    "        cache_file = os.path.join(self.cache_dir, \"cluster_performance_cache.pkl\")\n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                with open(cache_file, 'rb') as f:\n",
    "                    self.global_cache = pickle.load(f)\n",
    "                print(f\"üìÇ Caricata cache con {len(self.global_cache)} entries\")\n",
    "            except:\n",
    "                self.global_cache = {}\n",
    "    \n",
    "    def _save_cache(self):\n",
    "        cache_file = os.path.join(self.cache_dir, \"cluster_performance_cache.pkl\")\n",
    "        try:\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(self.global_cache, f)\n",
    "            print(f\"üíæ Salvata cache con {len(self.global_cache)} entries\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Errore salvataggio cache: {e}\")\n",
    "    \n",
    "    def _find_neighboring_clusters_delaunay(self, delivery_points):\n",
    "        \"\"\"\n",
    "        Trova cluster confinanti usando triangolazione di Delaunay per ottimizzare velocit√†\n",
    "        \"\"\"\n",
    "        try:\n",
    "            coords = delivery_points[['lat', 'lon']].values\n",
    "            tri = Delaunay(coords)\n",
    "            \n",
    "            neighbors_dict = {}\n",
    "            for cluster_id in delivery_points['cluster'].unique():\n",
    "                neighbors_dict[cluster_id] = set()\n",
    "            \n",
    "            # Per ogni triangolo, trova cluster coinvolti\n",
    "            for triangle in tri.simplices:\n",
    "                clusters_in_triangle = delivery_points.iloc[triangle]['cluster'].unique()\n",
    "                \n",
    "                if len(clusters_in_triangle) > 1:\n",
    "                    for i, cluster1 in enumerate(clusters_in_triangle):\n",
    "                        for j, cluster2 in enumerate(clusters_in_triangle):\n",
    "                            if i != j:\n",
    "                                neighbors_dict[cluster1].add(cluster2)\n",
    "            \n",
    "            # Converte set in list\n",
    "            neighbors_dict = {k: list(v) for k, v in neighbors_dict.items()}\n",
    "            return neighbors_dict\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Errore calcolo neighbors: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _can_merge_clusters(self, times1, times2, threshold=480):\n",
    "        \"\"\"\n",
    "        Verifica se due cluster possono essere uniti controllando che la somma \n",
    "        dei tempi medi per ogni giorno non superi il limite\n",
    "        \"\"\"\n",
    "        if len(times1) != len(times2):\n",
    "            return False, None\n",
    "            \n",
    "        sum_times = np.array(times1) + np.array(times2)\n",
    "        return np.all(sum_times <= threshold), sum_times.tolist()\n",
    "    \n",
    "    def _parallel_cluster_evaluation_optimized(self, cluster_dict, time_limit=3):\n",
    "        \"\"\"\n",
    "        Versione ottimizzata che usa calc_clusters_stats per evitare nested parallelism\n",
    "        \"\"\"\n",
    "        print(f\"üîÑ Calcolo performance di {len(cluster_dict)} cluster con calc_clusters_stats...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Filtra cluster non vuoti\n",
    "        valid_clusters = {cid: locs for cid, locs in cluster_dict.items() if len(locs) > 0}\n",
    "        \n",
    "        if not valid_clusters:\n",
    "            return {}\n",
    "        \n",
    "        # Usa calc_clusters_stats che √® pi√π efficiente per molti cluster\n",
    "        clusters_list = list(valid_clusters.values())\n",
    "        \n",
    "        try:\n",
    "            performance_df = pc.calc_clusters_stats(\n",
    "                clusters=clusters_list,\n",
    "                time_limit=time_limit,\n",
    "                parallel=True,\n",
    "                max_workers=self.n_cores,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Converti risultati in formato compatibile\n",
    "            results = {}\n",
    "            cluster_ids = list(valid_clusters.keys())\n",
    "            \n",
    "            for i, cluster_id in enumerate(cluster_ids):\n",
    "                cluster_data = performance_df[performance_df['cluster'] == f'Cluster {i+1}']\n",
    "                \n",
    "                if not cluster_data.empty:\n",
    "                    max_time = cluster_data['mean_minutes'].max()\n",
    "                    avg_time = cluster_data['mean_minutes'].mean()\n",
    "                    \n",
    "                    # Estrai tempi per giorno della settimana per merge analysis\n",
    "                    weekday_times = []\n",
    "                    for weekday in ['Luned√¨', 'Marted√¨', 'Mercoled√¨', 'Gioved√¨', 'Venerd√¨']:\n",
    "                        weekday_data = cluster_data[cluster_data['weekday'] == weekday]\n",
    "                        if not weekday_data.empty:\n",
    "                            weekday_times.append(weekday_data['mean_minutes'].iloc[0])\n",
    "                        else:\n",
    "                            weekday_times.append(0)\n",
    "                    \n",
    "                    # Aggiungi cache entry\n",
    "                    cache_key = self._cache_key(valid_clusters[cluster_id])\n",
    "                    result = {\n",
    "                        'max_time': max_time,\n",
    "                        'avg_time': avg_time,\n",
    "                        'cluster_size': len(valid_clusters[cluster_id]),\n",
    "                        'feasible': max_time <= self.max_shift_time_min + 30,\n",
    "                        'weekday_times': weekday_times  # Nuovo campo per merge analysis\n",
    "                    }\n",
    "                    \n",
    "                    self.global_cache[cache_key] = result\n",
    "                    results[cluster_id] = result\n",
    "                else:\n",
    "                    results[cluster_id] = {\n",
    "                        'max_time': float('inf'),\n",
    "                        'avg_time': float('inf'),\n",
    "                        'cluster_size': len(valid_clusters[cluster_id]),\n",
    "                        'feasible': False,\n",
    "                        'weekday_times': [float('inf')] * 5\n",
    "                    }\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"‚úÖ Completata valutazione ottimizzata in {elapsed:.1f}s\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Errore in calc_clusters_stats: {e}\")\n",
    "            # Fallback al metodo originale\n",
    "            return self._parallel_cluster_evaluation_fallback(cluster_dict, time_limit)\n",
    "    \n",
    "    def _parallel_cluster_evaluation_fallback(self, cluster_dict, time_limit=3):\n",
    "        \"\"\"Metodo fallback originale\"\"\"\n",
    "        print(f\"üîÑ Fallback: calcolo performance di {len(cluster_dict)} cluster...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        jobs = [(cid, loc_ids, time_limit) for cid, loc_ids in cluster_dict.items() if len(loc_ids) > 0]\n",
    "        results = {}\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=self.n_cores) as executor:\n",
    "            future_to_cluster = {\n",
    "                executor.submit(self._compute_cluster_performance_cached, loc_ids, time_limit): cid \n",
    "                for cid, loc_ids, time_limit in jobs\n",
    "            }\n",
    "            \n",
    "            completed = 0\n",
    "            for future in as_completed(future_to_cluster):\n",
    "                cluster_id = future_to_cluster[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results[cluster_id] = result\n",
    "                    completed += 1\n",
    "                    \n",
    "                    if completed % 10 == 0:\n",
    "                        elapsed = time.time() - start_time\n",
    "                        print(f\"  üìä Completati {completed}/{len(jobs)} cluster in {elapsed:.1f}s\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Errore cluster {cluster_id}: {e}\")\n",
    "                    results[cluster_id] = {\n",
    "                        'max_time': float('inf'),\n",
    "                        'avg_time': float('inf'), \n",
    "                        'cluster_size': 0,\n",
    "                        'feasible': False,\n",
    "                        'weekday_times': [float('inf')] * 5\n",
    "                    }\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"‚úÖ Completata valutazione fallback in {elapsed:.1f}s\")\n",
    "        return results\n",
    "    \n",
    "    def _compute_cluster_performance_cached(self, location_ids, time_limit=3):\n",
    "        cache_key = self._cache_key(location_ids)\n",
    "        \n",
    "        if cache_key in self.global_cache:\n",
    "            return self.global_cache[cache_key]\n",
    "        \n",
    "        try:\n",
    "            stats_df, _ = pc.single_cluster_stats_with_cache(\n",
    "                cluster_location_ids=location_ids,\n",
    "                time_limit=time_limit,\n",
    "                verbose=False,\n",
    "                max_workers=1\n",
    "            )\n",
    "            \n",
    "            if stats_df is not None and not stats_df.empty:\n",
    "                max_time = stats_df['mean_minutes'].max()\n",
    "                avg_time = stats_df['mean_minutes'].mean()\n",
    "                \n",
    "                # Estrai tempi per weekday per merge analysis\n",
    "                weekday_times = []\n",
    "                for weekday in ['Luned√¨', 'Marted√¨', 'Mercoled√¨', 'Gioved√¨', 'Venerd√¨']:\n",
    "                    weekday_data = stats_df[stats_df['weekday'] == weekday]\n",
    "                    if not weekday_data.empty:\n",
    "                        weekday_times.append(weekday_data['mean_minutes'].iloc[0])\n",
    "                    else:\n",
    "                        weekday_times.append(0)\n",
    "                \n",
    "                result = {\n",
    "                    'max_time': max_time,\n",
    "                    'avg_time': avg_time,\n",
    "                    'cluster_size': len(location_ids),\n",
    "                    'feasible': max_time <= self.max_shift_time_min + 30,\n",
    "                    'weekday_times': weekday_times\n",
    "                }\n",
    "            else:\n",
    "                result = {\n",
    "                    'max_time': float('inf'),\n",
    "                    'avg_time': float('inf'),\n",
    "                    'cluster_size': len(location_ids),\n",
    "                    'feasible': False,\n",
    "                    'weekday_times': [float('inf')] * 5\n",
    "                }\n",
    "            \n",
    "            self.global_cache[cache_key] = result\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Errore calcolo cluster {len(location_ids)} punti: {e}\")\n",
    "            return {\n",
    "                'max_time': float('inf'),\n",
    "                'avg_time': float('inf'),\n",
    "                'cluster_size': len(location_ids),\n",
    "                'feasible': False,\n",
    "                'weekday_times': [float('inf')] * 5\n",
    "            }\n",
    "    \n",
    "    def _merge_small_neighboring_clusters(self, cluster_dict, cluster_results, neighbors_dict, \n",
    "                                         points_df, min_cluster_size=15, verbose=True):\n",
    "        \"\"\"\n",
    "        Unisce cluster piccoli confinanti se la somma dei tempi non supera il limite\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"üîó Analisi merge cluster piccoli (< {min_cluster_size} punti)...\")\n",
    "        \n",
    "        merged = set()\n",
    "        new_cluster_dict = cluster_dict.copy()\n",
    "        merge_count = 0\n",
    "        \n",
    "        # Identifica cluster piccoli\n",
    "        small_clusters = [c for c, locs in cluster_dict.items() \n",
    "                         if len(locs) <= min_cluster_size and c in cluster_results]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  üìä Trovati {len(small_clusters)} cluster piccoli da analizzare\")\n",
    "        \n",
    "        # Prepara dati per controllo parallelo\n",
    "        merge_candidates = []\n",
    "        \n",
    "        for c in small_clusters:\n",
    "            if c in merged or c not in neighbors_dict:\n",
    "                continue\n",
    "                \n",
    "            c_times = cluster_results[c].get('weekday_times', [])\n",
    "            if not c_times or any(t == float('inf') for t in c_times):\n",
    "                continue\n",
    "            \n",
    "            for n in neighbors_dict[c]:\n",
    "                if n in merged or n == c or n not in cluster_results:\n",
    "                    continue\n",
    "                \n",
    "                n_times = cluster_results[n].get('weekday_times', [])\n",
    "                if not n_times or any(t == float('inf') for t in n_times):\n",
    "                    continue\n",
    "                \n",
    "                merge_candidates.append((c, n, c_times, n_times))\n",
    "        \n",
    "        # Controllo parallelo merge feasibility\n",
    "        if merge_candidates:\n",
    "            if verbose:\n",
    "                print(f\"  üîÑ Controllo {len(merge_candidates)} coppie candidate in parallelo...\")\n",
    "            \n",
    "            def check_merge_candidate(candidate):\n",
    "                c, n, c_times, n_times = candidate\n",
    "                can_merge, sum_times = self._can_merge_clusters(c_times, n_times, self.max_shift_time_min)\n",
    "                return (c, n, can_merge, sum_times)\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=self.n_cores) as executor:\n",
    "                futures = [executor.submit(check_merge_candidate, candidate) \n",
    "                          for candidate in merge_candidates]\n",
    "                \n",
    "                for future in as_completed(futures):\n",
    "                    c, n, can_merge, sum_times = future.result()\n",
    "                    \n",
    "                    if can_merge and c not in merged and n not in merged:\n",
    "                        # Esegui merge\n",
    "                        new_locs = new_cluster_dict[c] + new_cluster_dict[n]\n",
    "                        new_cluster_dict[c] = new_locs\n",
    "                        del new_cluster_dict[n]\n",
    "                        merged.add(c)\n",
    "                        merged.add(n)\n",
    "                        merge_count += 1\n",
    "                        \n",
    "                        if verbose:\n",
    "                            print(f\"    ‚úÖ Merged cluster {c} ({len(cluster_dict[c])} punti) + \"\n",
    "                                  f\"cluster {n} ({len(cluster_dict[n])} punti) = \"\n",
    "                                  f\"{len(new_locs)} punti\")\n",
    "        \n",
    "        # Rinumera cluster per eliminare gap\n",
    "        final_cluster_dict = {}\n",
    "        for new_id, (old_id, location_ids) in enumerate(new_cluster_dict.items()):\n",
    "            if len(location_ids) > 0:\n",
    "                final_cluster_dict[new_id] = location_ids\n",
    "        \n",
    "        # Aggiorna mapping cluster nel DataFrame punti\n",
    "        location_to_new_cluster = {}\n",
    "        for new_cluster_id, location_ids in final_cluster_dict.items():\n",
    "            for loc_id in location_ids:\n",
    "                location_to_new_cluster[loc_id] = new_cluster_id\n",
    "        \n",
    "        updated_points = points_df.copy()\n",
    "        updated_points['cluster'] = updated_points['location_id'].map(location_to_new_cluster)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  üéØ Completati {merge_count} merge. Cluster finali: {len(final_cluster_dict)}\")\n",
    "        \n",
    "        return final_cluster_dict, updated_points\n",
    "    \n",
    "    def _smart_reclustering_strategy(self, problematic_clusters, cluster_dict, cluster_results):\n",
    "        reclustering_plan = {}\n",
    "        \n",
    "        for cluster_id in problematic_clusters:\n",
    "            cluster_size = cluster_results[cluster_id]['cluster_size']\n",
    "            max_time = cluster_results[cluster_id]['max_time']\n",
    "            \n",
    "            if max_time == float('inf'):\n",
    "                suggested_splits = 3\n",
    "            else:\n",
    "                time_ratio = max_time / self.max_shift_time_min\n",
    "                size_factor = max(1, cluster_size / 50)\n",
    "                suggested_splits = max(2, min(8, int(np.ceil(time_ratio * 1.2 + size_factor * 0.1))))\n",
    "            \n",
    "            max_feasible_splits = min(suggested_splits, cluster_size // 2)\n",
    "            reclustering_plan[cluster_id] = max(2, max_feasible_splits)\n",
    "        \n",
    "        return reclustering_plan\n",
    "    \n",
    "    def run_optimized_clustering(self,\n",
    "                                delivery_points: pd.DataFrame,\n",
    "                                initial_k: int = 20,\n",
    "                                max_iterations: int = 15,\n",
    "                                time_limit_per_tsp: int = 3,\n",
    "                                early_stopping_threshold: int = 3,\n",
    "                                verbose: bool = False):\n",
    "        \n",
    "        print(f\"üéØ Inizio clustering bilanciato ottimizzato per {len(delivery_points)} punti\")\n",
    "        \n",
    "        self._load_cache()\n",
    "        \n",
    "        points = delivery_points.copy()\n",
    "        scaler = StandardScaler()\n",
    "        points_scaled = scaler.fit_transform(points[['lat', 'lon']])\n",
    "        \n",
    "        adaptive_k = max(initial_k, len(delivery_points) // 200)\n",
    "        current_k = min(adaptive_k, len(delivery_points) // 5)\n",
    "        \n",
    "        print(f\"üìä K iniziale adattivo: {current_k}\")\n",
    "        \n",
    "        best_solution = None\n",
    "        best_score = float('inf')\n",
    "        iterations_without_improvement = 0\n",
    "        \n",
    "        total_start_time = time.time()\n",
    "        \n",
    "        for iteration in range(1, max_iterations + 1):\n",
    "            iter_start = time.time()\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\nüîÑ Iterazione {iteration}/{max_iterations} - K = {current_k}\")\n",
    "            \n",
    "            # K-means ottimizzato\n",
    "            kmeans = KMeans(\n",
    "                n_clusters=current_k, \n",
    "                random_state=42 + iteration,\n",
    "                n_init=5,\n",
    "                max_iter=100,\n",
    "                tol=1e-3\n",
    "            )\n",
    "            \n",
    "            cluster_labels = kmeans.fit_predict(points_scaled)\n",
    "            points['cluster'] = cluster_labels\n",
    "            \n",
    "            # Crea dizionario cluster\n",
    "            cluster_dict = {}\n",
    "            for c in range(current_k):\n",
    "                cluster_locations = points.loc[points['cluster'] == c, 'location_id'].tolist()\n",
    "                if len(cluster_locations) > 0:\n",
    "                    cluster_dict[c] = cluster_locations\n",
    "            \n",
    "            if verbose:\n",
    "                sizes = [len(locs) for locs in cluster_dict.values()]\n",
    "                print(f\"  üìè Dimensioni cluster: min={min(sizes)}, max={max(sizes)}, media={np.mean(sizes):.1f}\")\n",
    "            \n",
    "            # Valutazione performance ottimizzata\n",
    "            cluster_results = self._parallel_cluster_evaluation_optimized(cluster_dict, time_limit_per_tsp)\n",
    "            \n",
    "            # *** NUOVA FASE: MERGE CLUSTER PICCOLI CONFINANTI ***\n",
    "            if len(cluster_dict) > 5:  # Solo se ha senso fare merge\n",
    "                neighbors_dict = self._find_neighboring_clusters_delaunay(points)\n",
    "                cluster_dict, points = self._merge_small_neighboring_clusters(\n",
    "                    cluster_dict, cluster_results, neighbors_dict, points, \n",
    "                    min_cluster_size=15, verbose=verbose\n",
    "                )\n",
    "                \n",
    "                # Ricalcola risultati dopo merge\n",
    "                if len(cluster_dict) != len(cluster_results):\n",
    "                    if verbose:\n",
    "                        print(\"  üîÑ Ricalcolo performance dopo merge...\")\n",
    "                    cluster_results = self._parallel_cluster_evaluation_optimized(cluster_dict, time_limit_per_tsp)\n",
    "            \n",
    "            # Identifica problematici\n",
    "            problematic_clusters = []\n",
    "            cluster_stats = []\n",
    "            \n",
    "            for cluster_id, result in cluster_results.items():\n",
    "                cluster_stats.append((cluster_id, result['cluster_size'], result['max_time']))\n",
    "                if not result['feasible']:\n",
    "                    problematic_clusters.append(cluster_id)\n",
    "            \n",
    "            # Valuta soluzione\n",
    "            num_problematic = len(problematic_clusters)\n",
    "            if num_problematic < best_score:\n",
    "                best_score = num_problematic\n",
    "                best_solution = cluster_dict.copy()\n",
    "                iterations_without_improvement = 0\n",
    "            else:\n",
    "                iterations_without_improvement += 1\n",
    "            \n",
    "            iter_elapsed = time.time() - iter_start\n",
    "            \n",
    "            if verbose:\n",
    "                problematic_stats = [(cid, size, time_min) for cid, size, time_min in cluster_stats \n",
    "                                   if cid in problematic_clusters]\n",
    "                problematic_stats.sort(key=lambda x: x[2], reverse=True)\n",
    "                \n",
    "                print(f\"  üìä Cluster problematici: {num_problematic}/{len(cluster_dict)}\")\n",
    "                print(f\"  ‚è±Ô∏è Tempo iterazione: {iter_elapsed:.1f}s\")\n",
    "                \n",
    "                if problematic_stats:\n",
    "                    print(\"  üîç Top 5 cluster problematici:\")\n",
    "                    for cid, size, time_min in problematic_stats[:5]:\n",
    "                        print(f\"    Cluster {cid}: {size} punti, {time_min:.1f} min\")\n",
    "            \n",
    "            # Condizioni di uscita\n",
    "            if num_problematic <= early_stopping_threshold:\n",
    "                print(f\"‚úÖ Early stopping: solo {num_problematic} cluster problematici\")\n",
    "                break\n",
    "            \n",
    "            if iterations_without_improvement >= 3:\n",
    "                print(f\"üîÑ Nessun miglioramento per 3 iterazioni, fermata anticipata\")\n",
    "                break\n",
    "            \n",
    "            # Preparazione iterazione successiva - RECLUSTERING\n",
    "            if iteration < max_iterations and num_problematic > 0:\n",
    "                reclustering_plan = self._smart_reclustering_strategy(\n",
    "                    problematic_clusters, cluster_dict, cluster_results\n",
    "                )\n",
    "                \n",
    "                problematic_points_mask = points['cluster'].isin(problematic_clusters)\n",
    "                problematic_points = points[problematic_points_mask].copy()\n",
    "                good_points = points[~problematic_points_mask].copy()\n",
    "                \n",
    "                if len(problematic_points) > 0:\n",
    "                    total_new_clusters = sum(reclustering_plan.values())\n",
    "                    \n",
    "                    if total_new_clusters < len(problematic_points):\n",
    "                        problematic_scaled = scaler.transform(problematic_points[['lat', 'lon']])\n",
    "                        \n",
    "                        sub_kmeans = KMeans(\n",
    "                            n_clusters=min(total_new_clusters, len(problematic_points)),\n",
    "                            random_state=42 + iteration * 10,\n",
    "                            n_init=3,\n",
    "                            max_iter=50\n",
    "                        )\n",
    "                        \n",
    "                        sub_labels = sub_kmeans.fit_predict(problematic_scaled)\n",
    "                        \n",
    "                        max_existing = good_points['cluster'].max() if len(good_points) > 0 else -1\n",
    "                        problematic_points['cluster'] = sub_labels + max_existing + 1\n",
    "                        \n",
    "                        points = pd.concat([good_points, problematic_points], ignore_index=True)\n",
    "                        current_k = points['cluster'].nunique()\n",
    "                        \n",
    "                        if verbose:\n",
    "                            print(f\"  üîß Riclusterizzati {len(problematic_clusters)} ‚Üí {total_new_clusters} nuovi cluster\")\n",
    "                    else:\n",
    "                        current_k += len(problematic_clusters)\n",
    "                        if verbose:\n",
    "                            print(f\"  üìà Incremento K globale: {current_k}\")\n",
    "        \n",
    "        # Finalizzazione\n",
    "        total_elapsed = time.time() - total_start_time\n",
    "        self._save_cache()\n",
    "        \n",
    "        if best_solution is None:\n",
    "            best_solution = cluster_dict\n",
    "        \n",
    "        # Rinumera cluster finale\n",
    "        final_clusters = {}\n",
    "        for new_id, (old_id, location_ids) in enumerate(best_solution.items()):\n",
    "            if len(location_ids) > 0:\n",
    "                final_clusters[new_id] = location_ids\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nüèÅ COMPLETATO in {total_elapsed:.1f}s totali ({total_elapsed/60:.1f} minuti)\")\n",
    "            print(f\"üìä Soluzione finale: {len(final_clusters)} cluster\")\n",
    "            \n",
    "            sizes = [len(locs) for locs in final_clusters.values()]\n",
    "            print(f\"üìè Dimensioni cluster: min={min(sizes)}, max={max(sizes)}, media={np.mean(sizes):.1f}\")\n",
    "        \n",
    "        # *** CALCOLA OUTPUT CON calc_clusters_stats ***\n",
    "        if verbose:\n",
    "            print(\"üìä Calcolo performance cluster finale con calc_clusters_stats...\")\n",
    "        \n",
    "        clusters_list = list(final_clusters.values())\n",
    "        \n",
    "        performance_df = pc.calc_clusters_stats(\n",
    "            clusters=clusters_list,\n",
    "            time_limit=time_limit_per_tsp,\n",
    "            parallel=True,\n",
    "            max_workers=self.n_cores,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        return final_clusters, performance_df\n",
    "\n",
    "\n",
    "# Funzione wrapper semplice\n",
    "def run_optimized_balanced_clustering(delivery_points, \n",
    "                                     initial_k=None, \n",
    "                                     max_iterations=15, \n",
    "                                     n_cores=None):\n",
    "    \n",
    "    if initial_k is None:\n",
    "        initial_k = max(10, len(delivery_points) // 150)\n",
    "        initial_k = min(initial_k, 50)\n",
    "        print(f\"üéØ K iniziale stimato: {initial_k}\")\n",
    "    \n",
    "    print(f\"üéØ Dataset: {len(delivery_points)} punti\")\n",
    "    print(f\"üéØ K iniziale: {initial_k}\")\n",
    "    \n",
    "    clusterer = OptimizedBalancedClustering(\n",
    "        max_shift_time_min=480,\n",
    "        n_cores=n_cores\n",
    "    )\n",
    "    \n",
    "    return clusterer.run_optimized_clustering(\n",
    "        delivery_points=delivery_points,\n",
    "        initial_k=initial_k,\n",
    "        max_iterations=max_iterations\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "cluster_dict, performance_df = run_optimized_balanced_clustering(pc.delivery_points, initial_k=50, n_cores=8, max_iterations=25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Salvataggio degli output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df.to_csv(\"clustering_methods_performances/k-means_iterative_v2(k=50)_5.csv\", index=False)\n",
    "\n",
    "with open('cluster_dicts/cluster_dict_k_means_iter_v2(k=50)_5.pkl', 'wb') as f:\n",
    "    pickle.dump(cluster_dict, f)\n",
    "\n",
    "# # Caricamento veloce  \n",
    "# with open('cluster_dicts/cluster_dict_k_means_iter_v2.pkl', 'rb') as f:\n",
    "#     cluster_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# k50 AS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Dataset: 2972 punti\n",
      "üéØ K iniziale: 50\n",
      "üöÄ Inizializzato con 8 core CPU\n",
      "üéØ Inizio clustering bilanciato ottimizzato per 2972 punti\n",
      "üìÇ Caricata cache con 1878 entries\n",
      "üìä K iniziale adattivo: 50\n",
      "üîÑ Calcolo performance di 50 cluster con calc_clusters_stats...\n",
      "‚úÖ Completata valutazione ottimizzata in 369.1s\n",
      "üîÑ Calcolo performance di 42 cluster con calc_clusters_stats...\n",
      "‚úÖ Completata valutazione ottimizzata in 353.2s\n",
      "‚úÖ Early stopping: solo 1 cluster problematici\n",
      "üíæ Salvata cache con 1881 entries\n",
      "Tempo di esecuzione algoritmo: 17.70 min\n"
     ]
    }
   ],
   "source": [
    "class OptimizedBalancedClustering:\n",
    "    def __init__(self, \n",
    "                 max_shift_time_min: int = 480,\n",
    "                 n_cores: int = None,\n",
    "                 cache_dir: str = \"./cluster_cache\"):\n",
    "        \n",
    "        self.max_shift_time_min = max_shift_time_min\n",
    "        self.n_cores = n_cores or max(1, mp.cpu_count() - 1)\n",
    "        self.cache_dir = cache_dir\n",
    "        self.global_cache = {}\n",
    "        \n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        print(f\"üöÄ Inizializzato con {self.n_cores} core CPU\")\n",
    "    \n",
    "    def _cache_key(self, location_ids):\n",
    "        return hash(tuple(sorted(location_ids)))\n",
    "    \n",
    "    def _load_cache(self):\n",
    "        cache_file = os.path.join(self.cache_dir, \"cluster_performance_cache.pkl\")\n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                with open(cache_file, 'rb') as f:\n",
    "                    self.global_cache = pickle.load(f)\n",
    "                print(f\"üìÇ Caricata cache con {len(self.global_cache)} entries\")\n",
    "            except:\n",
    "                self.global_cache = {}\n",
    "    \n",
    "    def _save_cache(self):\n",
    "        cache_file = os.path.join(self.cache_dir, \"cluster_performance_cache.pkl\")\n",
    "        try:\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(self.global_cache, f)\n",
    "            print(f\"üíæ Salvata cache con {len(self.global_cache)} entries\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Errore salvataggio cache: {e}\")\n",
    "    \n",
    "    def _find_neighboring_clusters_delaunay(self, delivery_points):\n",
    "        \"\"\"\n",
    "        Trova cluster confinanti usando triangolazione di Delaunay per ottimizzare velocit√†\n",
    "        \"\"\"\n",
    "        try:\n",
    "            coords = delivery_points[['lat', 'lon']].values\n",
    "            tri = Delaunay(coords)\n",
    "            \n",
    "            neighbors_dict = {}\n",
    "            for cluster_id in delivery_points['cluster'].unique():\n",
    "                neighbors_dict[cluster_id] = set()\n",
    "            \n",
    "            # Per ogni triangolo, trova cluster coinvolti\n",
    "            for triangle in tri.simplices:\n",
    "                clusters_in_triangle = delivery_points.iloc[triangle]['cluster'].unique()\n",
    "                \n",
    "                if len(clusters_in_triangle) > 1:\n",
    "                    for i, cluster1 in enumerate(clusters_in_triangle):\n",
    "                        for j, cluster2 in enumerate(clusters_in_triangle):\n",
    "                            if i != j:\n",
    "                                neighbors_dict[cluster1].add(cluster2)\n",
    "            \n",
    "            # Converte set in list\n",
    "            neighbors_dict = {k: list(v) for k, v in neighbors_dict.items()}\n",
    "            return neighbors_dict\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Errore calcolo neighbors: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _can_merge_clusters(self, times1, times2, threshold=480):\n",
    "        \"\"\"\n",
    "        Verifica se due cluster possono essere uniti controllando che la somma \n",
    "        dei tempi medi per ogni giorno non superi il limite\n",
    "        \"\"\"\n",
    "        if len(times1) != len(times2):\n",
    "            return False, None\n",
    "            \n",
    "        sum_times = np.array(times1) + np.array(times2)\n",
    "        return np.all(sum_times <= threshold), sum_times.tolist()\n",
    "    \n",
    "    def _parallel_cluster_evaluation_optimized(self, cluster_dict, time_limit=3):\n",
    "        \"\"\"\n",
    "        Versione ottimizzata che usa calc_clusters_stats per evitare nested parallelism\n",
    "        \"\"\"\n",
    "        print(f\"üîÑ Calcolo performance di {len(cluster_dict)} cluster con calc_clusters_stats...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Filtra cluster non vuoti\n",
    "        valid_clusters = {cid: locs for cid, locs in cluster_dict.items() if len(locs) > 0}\n",
    "        \n",
    "        if not valid_clusters:\n",
    "            return {}\n",
    "        \n",
    "        # Usa calc_clusters_stats che √® pi√π efficiente per molti cluster\n",
    "        clusters_list = list(valid_clusters.values())\n",
    "        \n",
    "        try:\n",
    "            performance_df = pc.calc_clusters_stats_AS(\n",
    "                clusters=clusters_list,\n",
    "                time_limit=time_limit,\n",
    "                parallel=True,\n",
    "                max_workers=self.n_cores,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Converti risultati in formato compatibile\n",
    "            results = {}\n",
    "            cluster_ids = list(valid_clusters.keys())\n",
    "            \n",
    "            for i, cluster_id in enumerate(cluster_ids):\n",
    "                cluster_data = performance_df[performance_df['cluster'] == f'Cluster {i+1}']\n",
    "                \n",
    "                if not cluster_data.empty:\n",
    "                    max_time = cluster_data['mean_minutes'].max()\n",
    "                    avg_time = cluster_data['mean_minutes'].mean()\n",
    "                    \n",
    "                    # Estrai tempi per giorno della settimana per merge analysis\n",
    "                    weekday_times = []\n",
    "                    for weekday in ['Luned√¨', 'Marted√¨', 'Mercoled√¨', 'Gioved√¨', 'Venerd√¨']:\n",
    "                        weekday_data = cluster_data[cluster_data['weekday'] == weekday]\n",
    "                        if not weekday_data.empty:\n",
    "                            weekday_times.append(weekday_data['mean_minutes'].iloc[0])\n",
    "                        else:\n",
    "                            weekday_times.append(0)\n",
    "                    \n",
    "                    # Aggiungi cache entry\n",
    "                    cache_key = self._cache_key(valid_clusters[cluster_id])\n",
    "                    result = {\n",
    "                        'max_time': max_time,\n",
    "                        'avg_time': avg_time,\n",
    "                        'cluster_size': len(valid_clusters[cluster_id]),\n",
    "                        'feasible': max_time <= self.max_shift_time_min + 30,\n",
    "                        'weekday_times': weekday_times  # Nuovo campo per merge analysis\n",
    "                    }\n",
    "                    \n",
    "                    self.global_cache[cache_key] = result\n",
    "                    results[cluster_id] = result\n",
    "                else:\n",
    "                    results[cluster_id] = {\n",
    "                        'max_time': float('inf'),\n",
    "                        'avg_time': float('inf'),\n",
    "                        'cluster_size': len(valid_clusters[cluster_id]),\n",
    "                        'feasible': False,\n",
    "                        'weekday_times': [float('inf')] * 5\n",
    "                    }\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"‚úÖ Completata valutazione ottimizzata in {elapsed:.1f}s\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Errore in calc_clusters_stats: {e}\")\n",
    "            # Fallback al metodo originale\n",
    "            return self._parallel_cluster_evaluation_fallback(cluster_dict, time_limit)\n",
    "    \n",
    "    def _parallel_cluster_evaluation_fallback(self, cluster_dict, time_limit=3):\n",
    "        \"\"\"Metodo fallback originale\"\"\"\n",
    "        print(f\"üîÑ Fallback: calcolo performance di {len(cluster_dict)} cluster...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        jobs = [(cid, loc_ids, time_limit) for cid, loc_ids in cluster_dict.items() if len(loc_ids) > 0]\n",
    "        results = {}\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=self.n_cores) as executor:\n",
    "            future_to_cluster = {\n",
    "                executor.submit(self._compute_cluster_performance_cached, loc_ids, time_limit): cid \n",
    "                for cid, loc_ids, time_limit in jobs\n",
    "            }\n",
    "            \n",
    "            completed = 0\n",
    "            for future in as_completed(future_to_cluster):\n",
    "                cluster_id = future_to_cluster[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results[cluster_id] = result\n",
    "                    completed += 1\n",
    "                    \n",
    "                    if completed % 10 == 0:\n",
    "                        elapsed = time.time() - start_time\n",
    "                        print(f\"  üìä Completati {completed}/{len(jobs)} cluster in {elapsed:.1f}s\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Errore cluster {cluster_id}: {e}\")\n",
    "                    results[cluster_id] = {\n",
    "                        'max_time': float('inf'),\n",
    "                        'avg_time': float('inf'), \n",
    "                        'cluster_size': 0,\n",
    "                        'feasible': False,\n",
    "                        'weekday_times': [float('inf')] * 5\n",
    "                    }\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"‚úÖ Completata valutazione fallback in {elapsed:.1f}s\")\n",
    "        return results\n",
    "    \n",
    "    def _compute_cluster_performance_cached(self, location_ids, time_limit=3):\n",
    "        cache_key = self._cache_key(location_ids)\n",
    "        \n",
    "        if cache_key in self.global_cache:\n",
    "            return self.global_cache[cache_key]\n",
    "        \n",
    "        try:\n",
    "            stats_df, _ = pc.single_cluster_stats_with_cache_AS(\n",
    "                cluster_location_ids=location_ids,\n",
    "                time_limit=time_limit,\n",
    "                verbose=False,\n",
    "                max_workers=1\n",
    "            )\n",
    "            \n",
    "            if stats_df is not None and not stats_df.empty:\n",
    "                max_time = stats_df['mean_minutes'].max()\n",
    "                avg_time = stats_df['mean_minutes'].mean()\n",
    "                \n",
    "                # Estrai tempi per weekday per merge analysis\n",
    "                weekday_times = []\n",
    "                for weekday in ['Luned√¨', 'Marted√¨', 'Mercoled√¨', 'Gioved√¨', 'Venerd√¨']:\n",
    "                    weekday_data = stats_df[stats_df['weekday'] == weekday]\n",
    "                    if not weekday_data.empty:\n",
    "                        weekday_times.append(weekday_data['mean_minutes'].iloc[0])\n",
    "                    else:\n",
    "                        weekday_times.append(0)\n",
    "                \n",
    "                result = {\n",
    "                    'max_time': max_time,\n",
    "                    'avg_time': avg_time,\n",
    "                    'cluster_size': len(location_ids),\n",
    "                    'feasible': max_time <= self.max_shift_time_min + 30,\n",
    "                    'weekday_times': weekday_times\n",
    "                }\n",
    "            else:\n",
    "                result = {\n",
    "                    'max_time': float('inf'),\n",
    "                    'avg_time': float('inf'),\n",
    "                    'cluster_size': len(location_ids),\n",
    "                    'feasible': False,\n",
    "                    'weekday_times': [float('inf')] * 5\n",
    "                }\n",
    "            \n",
    "            self.global_cache[cache_key] = result\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Errore calcolo cluster {len(location_ids)} punti: {e}\")\n",
    "            return {\n",
    "                'max_time': float('inf'),\n",
    "                'avg_time': float('inf'),\n",
    "                'cluster_size': len(location_ids),\n",
    "                'feasible': False,\n",
    "                'weekday_times': [float('inf')] * 5\n",
    "            }\n",
    "    \n",
    "    def _merge_small_neighboring_clusters(self, cluster_dict, cluster_results, neighbors_dict, \n",
    "                                         points_df, min_cluster_size=15, verbose=True):\n",
    "        \"\"\"\n",
    "        Unisce cluster piccoli confinanti se la somma dei tempi non supera il limite\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"üîó Analisi merge cluster piccoli (< {min_cluster_size} punti)...\")\n",
    "        \n",
    "        merged = set()\n",
    "        new_cluster_dict = cluster_dict.copy()\n",
    "        merge_count = 0\n",
    "        \n",
    "        # Identifica cluster piccoli\n",
    "        small_clusters = [c for c, locs in cluster_dict.items() \n",
    "                         if len(locs) <= min_cluster_size and c in cluster_results]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  üìä Trovati {len(small_clusters)} cluster piccoli da analizzare\")\n",
    "        \n",
    "        # Prepara dati per controllo parallelo\n",
    "        merge_candidates = []\n",
    "        \n",
    "        for c in small_clusters:\n",
    "            if c in merged or c not in neighbors_dict:\n",
    "                continue\n",
    "                \n",
    "            c_times = cluster_results[c].get('weekday_times', [])\n",
    "            if not c_times or any(t == float('inf') for t in c_times):\n",
    "                continue\n",
    "            \n",
    "            for n in neighbors_dict[c]:\n",
    "                if n in merged or n == c or n not in cluster_results:\n",
    "                    continue\n",
    "                \n",
    "                n_times = cluster_results[n].get('weekday_times', [])\n",
    "                if not n_times or any(t == float('inf') for t in n_times):\n",
    "                    continue\n",
    "                \n",
    "                merge_candidates.append((c, n, c_times, n_times))\n",
    "        \n",
    "        # Controllo parallelo merge feasibility\n",
    "        if merge_candidates:\n",
    "            if verbose:\n",
    "                print(f\"  üîÑ Controllo {len(merge_candidates)} coppie candidate in parallelo...\")\n",
    "            \n",
    "            def check_merge_candidate(candidate):\n",
    "                c, n, c_times, n_times = candidate\n",
    "                can_merge, sum_times = self._can_merge_clusters(c_times, n_times, self.max_shift_time_min)\n",
    "                return (c, n, can_merge, sum_times)\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=self.n_cores) as executor:\n",
    "                futures = [executor.submit(check_merge_candidate, candidate) \n",
    "                          for candidate in merge_candidates]\n",
    "                \n",
    "                for future in as_completed(futures):\n",
    "                    c, n, can_merge, sum_times = future.result()\n",
    "                    \n",
    "                    if can_merge and c not in merged and n not in merged:\n",
    "                        # Esegui merge\n",
    "                        new_locs = new_cluster_dict[c] + new_cluster_dict[n]\n",
    "                        new_cluster_dict[c] = new_locs\n",
    "                        del new_cluster_dict[n]\n",
    "                        merged.add(c)\n",
    "                        merged.add(n)\n",
    "                        merge_count += 1\n",
    "                        \n",
    "                        if verbose:\n",
    "                            print(f\"    ‚úÖ Merged cluster {c} ({len(cluster_dict[c])} punti) + \"\n",
    "                                  f\"cluster {n} ({len(cluster_dict[n])} punti) = \"\n",
    "                                  f\"{len(new_locs)} punti\")\n",
    "        \n",
    "        # Rinumera cluster per eliminare gap\n",
    "        final_cluster_dict = {}\n",
    "        for new_id, (old_id, location_ids) in enumerate(new_cluster_dict.items()):\n",
    "            if len(location_ids) > 0:\n",
    "                final_cluster_dict[new_id] = location_ids\n",
    "        \n",
    "        # Aggiorna mapping cluster nel DataFrame punti\n",
    "        location_to_new_cluster = {}\n",
    "        for new_cluster_id, location_ids in final_cluster_dict.items():\n",
    "            for loc_id in location_ids:\n",
    "                location_to_new_cluster[loc_id] = new_cluster_id\n",
    "        \n",
    "        updated_points = points_df.copy()\n",
    "        updated_points['cluster'] = updated_points['location_id'].map(location_to_new_cluster)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  üéØ Completati {merge_count} merge. Cluster finali: {len(final_cluster_dict)}\")\n",
    "        \n",
    "        return final_cluster_dict, updated_points\n",
    "    \n",
    "    def _smart_reclustering_strategy(self, problematic_clusters, cluster_dict, cluster_results):\n",
    "        reclustering_plan = {}\n",
    "        \n",
    "        for cluster_id in problematic_clusters:\n",
    "            cluster_size = cluster_results[cluster_id]['cluster_size']\n",
    "            max_time = cluster_results[cluster_id]['max_time']\n",
    "            \n",
    "            if max_time == float('inf'):\n",
    "                suggested_splits = 3\n",
    "            else:\n",
    "                time_ratio = max_time / self.max_shift_time_min\n",
    "                size_factor = max(1, cluster_size / 50)\n",
    "                suggested_splits = max(2, min(8, int(np.ceil(time_ratio * 1.2 + size_factor * 0.1))))\n",
    "            \n",
    "            max_feasible_splits = min(suggested_splits, cluster_size // 2)\n",
    "            reclustering_plan[cluster_id] = max(2, max_feasible_splits)\n",
    "        \n",
    "        return reclustering_plan\n",
    "    \n",
    "    def run_optimized_clustering(self,\n",
    "                                delivery_points: pd.DataFrame,\n",
    "                                initial_k: int = 20,\n",
    "                                max_iterations: int = 15,\n",
    "                                time_limit_per_tsp: int = 3,\n",
    "                                early_stopping_threshold: int = 3,\n",
    "                                verbose: bool = False):\n",
    "        \n",
    "        print(f\"üéØ Inizio clustering bilanciato ottimizzato per {len(delivery_points)} punti\")\n",
    "        \n",
    "        self._load_cache()\n",
    "        \n",
    "        points = delivery_points.copy()\n",
    "        scaler = StandardScaler()\n",
    "        points_scaled = scaler.fit_transform(points[['lat', 'lon']])\n",
    "        \n",
    "        adaptive_k = max(initial_k, len(delivery_points) // 200)\n",
    "        current_k = min(adaptive_k, len(delivery_points) // 5)\n",
    "        \n",
    "        print(f\"üìä K iniziale adattivo: {current_k}\")\n",
    "        \n",
    "        best_solution = None\n",
    "        best_score = float('inf')\n",
    "        iterations_without_improvement = 0\n",
    "        \n",
    "        total_start_time = time.time()\n",
    "        \n",
    "        for iteration in range(1, max_iterations + 1):\n",
    "            iter_start = time.time()\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\nüîÑ Iterazione {iteration}/{max_iterations} - K = {current_k}\")\n",
    "            \n",
    "            # K-means ottimizzato\n",
    "            kmeans = KMeans(\n",
    "                n_clusters=current_k, \n",
    "                random_state=42 + iteration,\n",
    "                n_init=5,\n",
    "                max_iter=100,\n",
    "                tol=1e-3\n",
    "            )\n",
    "            \n",
    "            cluster_labels = kmeans.fit_predict(points_scaled)\n",
    "            points['cluster'] = cluster_labels\n",
    "            \n",
    "            # Crea dizionario cluster\n",
    "            cluster_dict = {}\n",
    "            for c in range(current_k):\n",
    "                cluster_locations = points.loc[points['cluster'] == c, 'location_id'].tolist()\n",
    "                if len(cluster_locations) > 0:\n",
    "                    cluster_dict[c] = cluster_locations\n",
    "            \n",
    "            if verbose:\n",
    "                sizes = [len(locs) for locs in cluster_dict.values()]\n",
    "                print(f\"  üìè Dimensioni cluster: min={min(sizes)}, max={max(sizes)}, media={np.mean(sizes):.1f}\")\n",
    "            \n",
    "            # Valutazione performance ottimizzata\n",
    "            cluster_results = self._parallel_cluster_evaluation_optimized(cluster_dict, time_limit_per_tsp)\n",
    "            \n",
    "            # *** NUOVA FASE: MERGE CLUSTER PICCOLI CONFINANTI ***\n",
    "            if len(cluster_dict) > 5:  # Solo se ha senso fare merge\n",
    "                neighbors_dict = self._find_neighboring_clusters_delaunay(points)\n",
    "                cluster_dict, points = self._merge_small_neighboring_clusters(\n",
    "                    cluster_dict, cluster_results, neighbors_dict, points, \n",
    "                    min_cluster_size=15, verbose=verbose\n",
    "                )\n",
    "                \n",
    "                # Ricalcola risultati dopo merge\n",
    "                if len(cluster_dict) != len(cluster_results):\n",
    "                    if verbose:\n",
    "                        print(\"  üîÑ Ricalcolo performance dopo merge...\")\n",
    "                    cluster_results = self._parallel_cluster_evaluation_optimized(cluster_dict, time_limit_per_tsp)\n",
    "            \n",
    "            # Identifica problematici\n",
    "            problematic_clusters = []\n",
    "            cluster_stats = []\n",
    "            \n",
    "            for cluster_id, result in cluster_results.items():\n",
    "                cluster_stats.append((cluster_id, result['cluster_size'], result['max_time']))\n",
    "                if not result['feasible']:\n",
    "                    problematic_clusters.append(cluster_id)\n",
    "            \n",
    "            # Valuta soluzione\n",
    "            num_problematic = len(problematic_clusters)\n",
    "            if num_problematic < best_score:\n",
    "                best_score = num_problematic\n",
    "                best_solution = cluster_dict.copy()\n",
    "                iterations_without_improvement = 0\n",
    "            else:\n",
    "                iterations_without_improvement += 1\n",
    "            \n",
    "            iter_elapsed = time.time() - iter_start\n",
    "            \n",
    "            if verbose:\n",
    "                problematic_stats = [(cid, size, time_min) for cid, size, time_min in cluster_stats \n",
    "                                   if cid in problematic_clusters]\n",
    "                problematic_stats.sort(key=lambda x: x[2], reverse=True)\n",
    "                \n",
    "                print(f\"  üìä Cluster problematici: {num_problematic}/{len(cluster_dict)}\")\n",
    "                print(f\"  ‚è±Ô∏è Tempo iterazione: {iter_elapsed:.1f}s\")\n",
    "                \n",
    "                if problematic_stats:\n",
    "                    print(\"  üîç Top 5 cluster problematici:\")\n",
    "                    for cid, size, time_min in problematic_stats[:5]:\n",
    "                        print(f\"    Cluster {cid}: {size} punti, {time_min:.1f} min\")\n",
    "            \n",
    "            # Condizioni di uscita\n",
    "            if num_problematic <= early_stopping_threshold:\n",
    "                print(f\"‚úÖ Early stopping: solo {num_problematic} cluster problematici\")\n",
    "                break\n",
    "            \n",
    "            if iterations_without_improvement >= 3:\n",
    "                print(f\"üîÑ Nessun miglioramento per 3 iterazioni, fermata anticipata\")\n",
    "                break\n",
    "            \n",
    "            # Preparazione iterazione successiva - RECLUSTERING\n",
    "            if iteration < max_iterations and num_problematic > 0:\n",
    "                reclustering_plan = self._smart_reclustering_strategy(\n",
    "                    problematic_clusters, cluster_dict, cluster_results\n",
    "                )\n",
    "                \n",
    "                problematic_points_mask = points['cluster'].isin(problematic_clusters)\n",
    "                problematic_points = points[problematic_points_mask].copy()\n",
    "                good_points = points[~problematic_points_mask].copy()\n",
    "                \n",
    "                if len(problematic_points) > 0:\n",
    "                    total_new_clusters = sum(reclustering_plan.values())\n",
    "                    \n",
    "                    if total_new_clusters < len(problematic_points):\n",
    "                        problematic_scaled = scaler.transform(problematic_points[['lat', 'lon']])\n",
    "                        \n",
    "                        sub_kmeans = KMeans(\n",
    "                            n_clusters=min(total_new_clusters, len(problematic_points)),\n",
    "                            random_state=42 + iteration * 10,\n",
    "                            n_init=3,\n",
    "                            max_iter=50\n",
    "                        )\n",
    "                        \n",
    "                        sub_labels = sub_kmeans.fit_predict(problematic_scaled)\n",
    "                        \n",
    "                        max_existing = good_points['cluster'].max() if len(good_points) > 0 else -1\n",
    "                        problematic_points['cluster'] = sub_labels + max_existing + 1\n",
    "                        \n",
    "                        points = pd.concat([good_points, problematic_points], ignore_index=True)\n",
    "                        current_k = points['cluster'].nunique()\n",
    "                        \n",
    "                        if verbose:\n",
    "                            print(f\"  üîß Riclusterizzati {len(problematic_clusters)} ‚Üí {total_new_clusters} nuovi cluster\")\n",
    "                    else:\n",
    "                        current_k += len(problematic_clusters)\n",
    "                        if verbose:\n",
    "                            print(f\"  üìà Incremento K globale: {current_k}\")\n",
    "        \n",
    "        # Finalizzazione\n",
    "        total_elapsed = time.time() - total_start_time\n",
    "        self._save_cache()\n",
    "        \n",
    "        if best_solution is None:\n",
    "            best_solution = cluster_dict\n",
    "        \n",
    "        # Rinumera cluster finale\n",
    "        final_clusters = {}\n",
    "        for new_id, (old_id, location_ids) in enumerate(best_solution.items()):\n",
    "            if len(location_ids) > 0:\n",
    "                final_clusters[new_id] = location_ids\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nüèÅ COMPLETATO in {total_elapsed:.1f}s totali ({total_elapsed/60:.1f} minuti)\")\n",
    "            print(f\"üìä Soluzione finale: {len(final_clusters)} cluster\")\n",
    "            \n",
    "            sizes = [len(locs) for locs in final_clusters.values()]\n",
    "            print(f\"üìè Dimensioni cluster: min={min(sizes)}, max={max(sizes)}, media={np.mean(sizes):.1f}\")\n",
    "        \n",
    "        # *** CALCOLA OUTPUT CON calc_clusters_stats ***\n",
    "        if verbose:\n",
    "            print(\"üìä Calcolo performance cluster finale con calc_clusters_stats...\")\n",
    "        \n",
    "        clusters_list = list(final_clusters.values())\n",
    "        \n",
    "        performance_df = pc.calc_clusters_stats_AS(\n",
    "            clusters=clusters_list,\n",
    "            time_limit=time_limit_per_tsp,\n",
    "            parallel=True,\n",
    "            max_workers=self.n_cores,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        return final_clusters, performance_df\n",
    "\n",
    "\n",
    "# Funzione wrapper semplice\n",
    "def run_optimized_balanced_clustering(delivery_points, \n",
    "                                     initial_k=None, \n",
    "                                     max_iterations=15, \n",
    "                                     n_cores=None):\n",
    "    \n",
    "    if initial_k is None:\n",
    "        initial_k = max(10, len(delivery_points) // 150)\n",
    "        initial_k = min(initial_k, 50)\n",
    "        print(f\"üéØ K iniziale stimato: {initial_k}\")\n",
    "    \n",
    "    print(f\"üéØ Dataset: {len(delivery_points)} punti\")\n",
    "    print(f\"üéØ K iniziale: {initial_k}\")\n",
    "    \n",
    "    clusterer = OptimizedBalancedClustering(\n",
    "        max_shift_time_min=480,\n",
    "        n_cores=n_cores\n",
    "    )\n",
    "    \n",
    "    return clusterer.run_optimized_clustering(\n",
    "        delivery_points=delivery_points,\n",
    "        initial_k=initial_k,\n",
    "        max_iterations=max_iterations\n",
    "    )\n",
    "\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "cluster_dict, performance_df = run_optimized_balanced_clustering(pc.delivery_points_AS, initial_k=50, n_cores=8, max_iterations=25)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Tempo di esecuzione algoritmo: {(end - start)/60:.2f} min\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Salvataggio output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df.to_csv(\"clustering_methods_performances/k-means_iterative_v2(k=50)_AS_5.csv\", index=False)\n",
    "\n",
    "with open('cluster_dicts/cluster_dict_k_means_iter_v2(k=50)_AS_5.pkl', 'wb') as f:\n",
    "    pickle.dump(cluster_dict, f)\n",
    "\n",
    "# # Caricamento veloce  \n",
    "# with open('cluster_dicts/cluster_dict_k_means_iter_v2.pkl', 'rb') as f:\n",
    "#     cluster_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# k50 ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Dataset: 3219 punti\n",
      "üéØ K iniziale: 50\n",
      "üöÄ Inizializzato con 8 core CPU\n",
      "üéØ Inizio clustering bilanciato ottimizzato per 3219 punti\n",
      "üìÇ Caricata cache con 1881 entries\n",
      "üìä K iniziale adattivo: 50\n",
      "üîÑ Calcolo performance di 50 cluster con calc_clusters_stats...\n",
      "‚úÖ Completata valutazione ottimizzata in 372.3s\n",
      "üîÑ Calcolo performance di 45 cluster con calc_clusters_stats...\n",
      "‚úÖ Completata valutazione ottimizzata in 365.1s\n",
      "üîÑ Calcolo performance di 51 cluster con calc_clusters_stats...\n",
      "‚úÖ Completata valutazione ottimizzata in 434.8s\n",
      "üîÑ Calcolo performance di 46 cluster con calc_clusters_stats...\n",
      "‚úÖ Completata valutazione ottimizzata in 443.3s\n",
      "üîÑ Calcolo performance di 55 cluster con calc_clusters_stats...\n",
      "‚úÖ Completata valutazione ottimizzata in 484.4s\n",
      "üîÑ Calcolo performance di 46 cluster con calc_clusters_stats...\n",
      "‚úÖ Completata valutazione ottimizzata in 470.0s\n",
      "üîÑ Calcolo performance di 58 cluster con calc_clusters_stats...\n",
      "‚úÖ Completata valutazione ottimizzata in 507.6s\n",
      "üîÑ Calcolo performance di 47 cluster con calc_clusters_stats...\n",
      "‚úÖ Completata valutazione ottimizzata in 460.8s\n",
      "üîÑ Nessun miglioramento per 3 iterazioni, fermata anticipata\n",
      "üíæ Salvata cache con 2018 entries\n",
      "Tempo di esecuzione algoritmo: 65.44 min\n"
     ]
    }
   ],
   "source": [
    "class OptimizedBalancedClustering:\n",
    "    def __init__(self, \n",
    "                 max_shift_time_min: int = 480,\n",
    "                 n_cores: int = None,\n",
    "                 cache_dir: str = \"./cluster_cache\"):\n",
    "        \n",
    "        self.max_shift_time_min = max_shift_time_min\n",
    "        self.n_cores = n_cores or max(1, mp.cpu_count() - 1)\n",
    "        self.cache_dir = cache_dir\n",
    "        self.global_cache = {}\n",
    "        \n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        print(f\"üöÄ Inizializzato con {self.n_cores} core CPU\")\n",
    "    \n",
    "    def _cache_key(self, location_ids):\n",
    "        return hash(tuple(sorted(location_ids)))\n",
    "    \n",
    "    def _load_cache(self):\n",
    "        cache_file = os.path.join(self.cache_dir, \"cluster_performance_cache.pkl\")\n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                with open(cache_file, 'rb') as f:\n",
    "                    self.global_cache = pickle.load(f)\n",
    "                print(f\"üìÇ Caricata cache con {len(self.global_cache)} entries\")\n",
    "            except:\n",
    "                self.global_cache = {}\n",
    "    \n",
    "    def _save_cache(self):\n",
    "        cache_file = os.path.join(self.cache_dir, \"cluster_performance_cache.pkl\")\n",
    "        try:\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(self.global_cache, f)\n",
    "            print(f\"üíæ Salvata cache con {len(self.global_cache)} entries\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Errore salvataggio cache: {e}\")\n",
    "    \n",
    "    def _find_neighboring_clusters_delaunay(self, delivery_points):\n",
    "        \"\"\"\n",
    "        Trova cluster confinanti usando triangolazione di Delaunay per ottimizzare velocit√†\n",
    "        \"\"\"\n",
    "        try:\n",
    "            coords = delivery_points[['lat', 'lon']].values\n",
    "            tri = Delaunay(coords)\n",
    "            \n",
    "            neighbors_dict = {}\n",
    "            for cluster_id in delivery_points['cluster'].unique():\n",
    "                neighbors_dict[cluster_id] = set()\n",
    "            \n",
    "            # Per ogni triangolo, trova cluster coinvolti\n",
    "            for triangle in tri.simplices:\n",
    "                clusters_in_triangle = delivery_points.iloc[triangle]['cluster'].unique()\n",
    "                \n",
    "                if len(clusters_in_triangle) > 1:\n",
    "                    for i, cluster1 in enumerate(clusters_in_triangle):\n",
    "                        for j, cluster2 in enumerate(clusters_in_triangle):\n",
    "                            if i != j:\n",
    "                                neighbors_dict[cluster1].add(cluster2)\n",
    "            \n",
    "            # Converte set in list\n",
    "            neighbors_dict = {k: list(v) for k, v in neighbors_dict.items()}\n",
    "            return neighbors_dict\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Errore calcolo neighbors: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _can_merge_clusters(self, times1, times2, threshold=480):\n",
    "        \"\"\"\n",
    "        Verifica se due cluster possono essere uniti controllando che la somma \n",
    "        dei tempi medi per ogni giorno non superi il limite\n",
    "        \"\"\"\n",
    "        if len(times1) != len(times2):\n",
    "            return False, None\n",
    "            \n",
    "        sum_times = np.array(times1) + np.array(times2)\n",
    "        return np.all(sum_times <= threshold), sum_times.tolist()\n",
    "    \n",
    "    def _parallel_cluster_evaluation_optimized(self, cluster_dict, time_limit=3):\n",
    "        \"\"\"\n",
    "        Versione ottimizzata che usa calc_clusters_stats per evitare nested parallelism\n",
    "        \"\"\"\n",
    "        print(f\"üîÑ Calcolo performance di {len(cluster_dict)} cluster con calc_clusters_stats...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Filtra cluster non vuoti\n",
    "        valid_clusters = {cid: locs for cid, locs in cluster_dict.items() if len(locs) > 0}\n",
    "        \n",
    "        if not valid_clusters:\n",
    "            return {}\n",
    "        \n",
    "        # Usa calc_clusters_stats che √® pi√π efficiente per molti cluster\n",
    "        clusters_list = list(valid_clusters.values())\n",
    "        \n",
    "        try:\n",
    "            performance_df = pc.calc_clusters_stats_ON(\n",
    "                clusters=clusters_list,\n",
    "                time_limit=time_limit,\n",
    "                parallel=True,\n",
    "                max_workers=self.n_cores,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Converti risultati in formato compatibile\n",
    "            results = {}\n",
    "            cluster_ids = list(valid_clusters.keys())\n",
    "            \n",
    "            for i, cluster_id in enumerate(cluster_ids):\n",
    "                cluster_data = performance_df[performance_df['cluster'] == f'Cluster {i+1}']\n",
    "                \n",
    "                if not cluster_data.empty:\n",
    "                    max_time = cluster_data['mean_minutes'].max()\n",
    "                    avg_time = cluster_data['mean_minutes'].mean()\n",
    "                    \n",
    "                    # Estrai tempi per giorno della settimana per merge analysis\n",
    "                    weekday_times = []\n",
    "                    for weekday in ['Luned√¨', 'Marted√¨', 'Mercoled√¨', 'Gioved√¨', 'Venerd√¨']:\n",
    "                        weekday_data = cluster_data[cluster_data['weekday'] == weekday]\n",
    "                        if not weekday_data.empty:\n",
    "                            weekday_times.append(weekday_data['mean_minutes'].iloc[0])\n",
    "                        else:\n",
    "                            weekday_times.append(0)\n",
    "                    \n",
    "                    # Aggiungi cache entry\n",
    "                    cache_key = self._cache_key(valid_clusters[cluster_id])\n",
    "                    result = {\n",
    "                        'max_time': max_time,\n",
    "                        'avg_time': avg_time,\n",
    "                        'cluster_size': len(valid_clusters[cluster_id]),\n",
    "                        'feasible': max_time <= self.max_shift_time_min + 30,\n",
    "                        'weekday_times': weekday_times  # Nuovo campo per merge analysis\n",
    "                    }\n",
    "                    \n",
    "                    self.global_cache[cache_key] = result\n",
    "                    results[cluster_id] = result\n",
    "                else:\n",
    "                    results[cluster_id] = {\n",
    "                        'max_time': float('inf'),\n",
    "                        'avg_time': float('inf'),\n",
    "                        'cluster_size': len(valid_clusters[cluster_id]),\n",
    "                        'feasible': False,\n",
    "                        'weekday_times': [float('inf')] * 5\n",
    "                    }\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"‚úÖ Completata valutazione ottimizzata in {elapsed:.1f}s\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Errore in calc_clusters_stats: {e}\")\n",
    "            # Fallback al metodo originale\n",
    "            return self._parallel_cluster_evaluation_fallback(cluster_dict, time_limit)\n",
    "    \n",
    "    def _parallel_cluster_evaluation_fallback(self, cluster_dict, time_limit=3):\n",
    "        \"\"\"Metodo fallback originale\"\"\"\n",
    "        print(f\"üîÑ Fallback: calcolo performance di {len(cluster_dict)} cluster...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        jobs = [(cid, loc_ids, time_limit) for cid, loc_ids in cluster_dict.items() if len(loc_ids) > 0]\n",
    "        results = {}\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=self.n_cores) as executor:\n",
    "            future_to_cluster = {\n",
    "                executor.submit(self._compute_cluster_performance_cached, loc_ids, time_limit): cid \n",
    "                for cid, loc_ids, time_limit in jobs\n",
    "            }\n",
    "            \n",
    "            completed = 0\n",
    "            for future in as_completed(future_to_cluster):\n",
    "                cluster_id = future_to_cluster[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results[cluster_id] = result\n",
    "                    completed += 1\n",
    "                    \n",
    "                    if completed % 10 == 0:\n",
    "                        elapsed = time.time() - start_time\n",
    "                        print(f\"  üìä Completati {completed}/{len(jobs)} cluster in {elapsed:.1f}s\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Errore cluster {cluster_id}: {e}\")\n",
    "                    results[cluster_id] = {\n",
    "                        'max_time': float('inf'),\n",
    "                        'avg_time': float('inf'), \n",
    "                        'cluster_size': 0,\n",
    "                        'feasible': False,\n",
    "                        'weekday_times': [float('inf')] * 5\n",
    "                    }\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"‚úÖ Completata valutazione fallback in {elapsed:.1f}s\")\n",
    "        return results\n",
    "    \n",
    "    def _compute_cluster_performance_cached(self, location_ids, time_limit=3):\n",
    "        cache_key = self._cache_key(location_ids)\n",
    "        \n",
    "        if cache_key in self.global_cache:\n",
    "            return self.global_cache[cache_key]\n",
    "        \n",
    "        try:\n",
    "            stats_df, _ = pc.single_cluster_stats_with_cache_ON(\n",
    "                cluster_location_ids=location_ids,\n",
    "                time_limit=time_limit,\n",
    "                verbose=False,\n",
    "                max_workers=1\n",
    "            )\n",
    "            \n",
    "            if stats_df is not None and not stats_df.empty:\n",
    "                max_time = stats_df['mean_minutes'].max()\n",
    "                avg_time = stats_df['mean_minutes'].mean()\n",
    "                \n",
    "                # Estrai tempi per weekday per merge analysis\n",
    "                weekday_times = []\n",
    "                for weekday in ['Luned√¨', 'Marted√¨', 'Mercoled√¨', 'Gioved√¨', 'Venerd√¨']:\n",
    "                    weekday_data = stats_df[stats_df['weekday'] == weekday]\n",
    "                    if not weekday_data.empty:\n",
    "                        weekday_times.append(weekday_data['mean_minutes'].iloc[0])\n",
    "                    else:\n",
    "                        weekday_times.append(0)\n",
    "                \n",
    "                result = {\n",
    "                    'max_time': max_time,\n",
    "                    'avg_time': avg_time,\n",
    "                    'cluster_size': len(location_ids),\n",
    "                    'feasible': max_time <= self.max_shift_time_min + 30,\n",
    "                    'weekday_times': weekday_times\n",
    "                }\n",
    "            else:\n",
    "                result = {\n",
    "                    'max_time': float('inf'),\n",
    "                    'avg_time': float('inf'),\n",
    "                    'cluster_size': len(location_ids),\n",
    "                    'feasible': False,\n",
    "                    'weekday_times': [float('inf')] * 5\n",
    "                }\n",
    "            \n",
    "            self.global_cache[cache_key] = result\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Errore calcolo cluster {len(location_ids)} punti: {e}\")\n",
    "            return {\n",
    "                'max_time': float('inf'),\n",
    "                'avg_time': float('inf'),\n",
    "                'cluster_size': len(location_ids),\n",
    "                'feasible': False,\n",
    "                'weekday_times': [float('inf')] * 5\n",
    "            }\n",
    "    \n",
    "    def _merge_small_neighboring_clusters(self, cluster_dict, cluster_results, neighbors_dict, \n",
    "                                         points_df, min_cluster_size=15, verbose=True):\n",
    "        \"\"\"\n",
    "        Unisce cluster piccoli confinanti se la somma dei tempi non supera il limite\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"üîó Analisi merge cluster piccoli (< {min_cluster_size} punti)...\")\n",
    "        \n",
    "        merged = set()\n",
    "        new_cluster_dict = cluster_dict.copy()\n",
    "        merge_count = 0\n",
    "        \n",
    "        # Identifica cluster piccoli\n",
    "        small_clusters = [c for c, locs in cluster_dict.items() \n",
    "                         if len(locs) <= min_cluster_size and c in cluster_results]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  üìä Trovati {len(small_clusters)} cluster piccoli da analizzare\")\n",
    "        \n",
    "        # Prepara dati per controllo parallelo\n",
    "        merge_candidates = []\n",
    "        \n",
    "        for c in small_clusters:\n",
    "            if c in merged or c not in neighbors_dict:\n",
    "                continue\n",
    "                \n",
    "            c_times = cluster_results[c].get('weekday_times', [])\n",
    "            if not c_times or any(t == float('inf') for t in c_times):\n",
    "                continue\n",
    "            \n",
    "            for n in neighbors_dict[c]:\n",
    "                if n in merged or n == c or n not in cluster_results:\n",
    "                    continue\n",
    "                \n",
    "                n_times = cluster_results[n].get('weekday_times', [])\n",
    "                if not n_times or any(t == float('inf') for t in n_times):\n",
    "                    continue\n",
    "                \n",
    "                merge_candidates.append((c, n, c_times, n_times))\n",
    "        \n",
    "        # Controllo parallelo merge feasibility\n",
    "        if merge_candidates:\n",
    "            if verbose:\n",
    "                print(f\"  üîÑ Controllo {len(merge_candidates)} coppie candidate in parallelo...\")\n",
    "            \n",
    "            def check_merge_candidate(candidate):\n",
    "                c, n, c_times, n_times = candidate\n",
    "                can_merge, sum_times = self._can_merge_clusters(c_times, n_times, self.max_shift_time_min)\n",
    "                return (c, n, can_merge, sum_times)\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=self.n_cores) as executor:\n",
    "                futures = [executor.submit(check_merge_candidate, candidate) \n",
    "                          for candidate in merge_candidates]\n",
    "                \n",
    "                for future in as_completed(futures):\n",
    "                    c, n, can_merge, sum_times = future.result()\n",
    "                    \n",
    "                    if can_merge and c not in merged and n not in merged:\n",
    "                        # Esegui merge\n",
    "                        new_locs = new_cluster_dict[c] + new_cluster_dict[n]\n",
    "                        new_cluster_dict[c] = new_locs\n",
    "                        del new_cluster_dict[n]\n",
    "                        merged.add(c)\n",
    "                        merged.add(n)\n",
    "                        merge_count += 1\n",
    "                        \n",
    "                        if verbose:\n",
    "                            print(f\"    ‚úÖ Merged cluster {c} ({len(cluster_dict[c])} punti) + \"\n",
    "                                  f\"cluster {n} ({len(cluster_dict[n])} punti) = \"\n",
    "                                  f\"{len(new_locs)} punti\")\n",
    "        \n",
    "        # Rinumera cluster per eliminare gap\n",
    "        final_cluster_dict = {}\n",
    "        for new_id, (old_id, location_ids) in enumerate(new_cluster_dict.items()):\n",
    "            if len(location_ids) > 0:\n",
    "                final_cluster_dict[new_id] = location_ids\n",
    "        \n",
    "        # Aggiorna mapping cluster nel DataFrame punti\n",
    "        location_to_new_cluster = {}\n",
    "        for new_cluster_id, location_ids in final_cluster_dict.items():\n",
    "            for loc_id in location_ids:\n",
    "                location_to_new_cluster[loc_id] = new_cluster_id\n",
    "        \n",
    "        updated_points = points_df.copy()\n",
    "        updated_points['cluster'] = updated_points['location_id'].map(location_to_new_cluster)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  üéØ Completati {merge_count} merge. Cluster finali: {len(final_cluster_dict)}\")\n",
    "        \n",
    "        return final_cluster_dict, updated_points\n",
    "    \n",
    "    def _smart_reclustering_strategy(self, problematic_clusters, cluster_dict, cluster_results):\n",
    "        reclustering_plan = {}\n",
    "        \n",
    "        for cluster_id in problematic_clusters:\n",
    "            cluster_size = cluster_results[cluster_id]['cluster_size']\n",
    "            max_time = cluster_results[cluster_id]['max_time']\n",
    "            \n",
    "            if max_time == float('inf'):\n",
    "                suggested_splits = 3\n",
    "            else:\n",
    "                time_ratio = max_time / self.max_shift_time_min\n",
    "                size_factor = max(1, cluster_size / 50)\n",
    "                suggested_splits = max(2, min(8, int(np.ceil(time_ratio * 1.2 + size_factor * 0.1))))\n",
    "            \n",
    "            max_feasible_splits = min(suggested_splits, cluster_size // 2)\n",
    "            reclustering_plan[cluster_id] = max(2, max_feasible_splits)\n",
    "        \n",
    "        return reclustering_plan\n",
    "    \n",
    "    def run_optimized_clustering(self,\n",
    "                                delivery_points: pd.DataFrame,\n",
    "                                initial_k: int = 20,\n",
    "                                max_iterations: int = 15,\n",
    "                                time_limit_per_tsp: int = 3,\n",
    "                                early_stopping_threshold: int = 3,\n",
    "                                verbose: bool = False):\n",
    "        \n",
    "        print(f\"üéØ Inizio clustering bilanciato ottimizzato per {len(delivery_points)} punti\")\n",
    "        \n",
    "        self._load_cache()\n",
    "        \n",
    "        points = delivery_points.copy()\n",
    "        scaler = StandardScaler()\n",
    "        points_scaled = scaler.fit_transform(points[['lat', 'lon']])\n",
    "        \n",
    "        adaptive_k = max(initial_k, len(delivery_points) // 200)\n",
    "        current_k = min(adaptive_k, len(delivery_points) // 5)\n",
    "        \n",
    "        print(f\"üìä K iniziale adattivo: {current_k}\")\n",
    "        \n",
    "        best_solution = None\n",
    "        best_score = float('inf')\n",
    "        iterations_without_improvement = 0\n",
    "        \n",
    "        total_start_time = time.time()\n",
    "        \n",
    "        for iteration in range(1, max_iterations + 1):\n",
    "            iter_start = time.time()\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\nüîÑ Iterazione {iteration}/{max_iterations} - K = {current_k}\")\n",
    "            \n",
    "            # K-means ottimizzato\n",
    "            kmeans = KMeans(\n",
    "                n_clusters=current_k, \n",
    "                random_state=42 + iteration,\n",
    "                n_init=5,\n",
    "                max_iter=100,\n",
    "                tol=1e-3\n",
    "            )\n",
    "            \n",
    "            cluster_labels = kmeans.fit_predict(points_scaled)\n",
    "            points['cluster'] = cluster_labels\n",
    "            \n",
    "            # Crea dizionario cluster\n",
    "            cluster_dict = {}\n",
    "            for c in range(current_k):\n",
    "                cluster_locations = points.loc[points['cluster'] == c, 'location_id'].tolist()\n",
    "                if len(cluster_locations) > 0:\n",
    "                    cluster_dict[c] = cluster_locations\n",
    "            \n",
    "            if verbose:\n",
    "                sizes = [len(locs) for locs in cluster_dict.values()]\n",
    "                print(f\"  üìè Dimensioni cluster: min={min(sizes)}, max={max(sizes)}, media={np.mean(sizes):.1f}\")\n",
    "            \n",
    "            # Valutazione performance ottimizzata\n",
    "            cluster_results = self._parallel_cluster_evaluation_optimized(cluster_dict, time_limit_per_tsp)\n",
    "            \n",
    "            # *** NUOVA FASE: MERGE CLUSTER PICCOLI CONFINANTI ***\n",
    "            if len(cluster_dict) > 5:  # Solo se ha senso fare merge\n",
    "                neighbors_dict = self._find_neighboring_clusters_delaunay(points)\n",
    "                cluster_dict, points = self._merge_small_neighboring_clusters(\n",
    "                    cluster_dict, cluster_results, neighbors_dict, points, \n",
    "                    min_cluster_size=15, verbose=verbose\n",
    "                )\n",
    "                \n",
    "                # Ricalcola risultati dopo merge\n",
    "                if len(cluster_dict) != len(cluster_results):\n",
    "                    if verbose:\n",
    "                        print(\"  üîÑ Ricalcolo performance dopo merge...\")\n",
    "                    cluster_results = self._parallel_cluster_evaluation_optimized(cluster_dict, time_limit_per_tsp)\n",
    "            \n",
    "            # Identifica problematici\n",
    "            problematic_clusters = []\n",
    "            cluster_stats = []\n",
    "            \n",
    "            for cluster_id, result in cluster_results.items():\n",
    "                cluster_stats.append((cluster_id, result['cluster_size'], result['max_time']))\n",
    "                if not result['feasible']:\n",
    "                    problematic_clusters.append(cluster_id)\n",
    "            \n",
    "            # Valuta soluzione\n",
    "            num_problematic = len(problematic_clusters)\n",
    "            if num_problematic < best_score:\n",
    "                best_score = num_problematic\n",
    "                best_solution = cluster_dict.copy()\n",
    "                iterations_without_improvement = 0\n",
    "            else:\n",
    "                iterations_without_improvement += 1\n",
    "            \n",
    "            iter_elapsed = time.time() - iter_start\n",
    "            \n",
    "            if verbose:\n",
    "                problematic_stats = [(cid, size, time_min) for cid, size, time_min in cluster_stats \n",
    "                                   if cid in problematic_clusters]\n",
    "                problematic_stats.sort(key=lambda x: x[2], reverse=True)\n",
    "                \n",
    "                print(f\"  üìä Cluster problematici: {num_problematic}/{len(cluster_dict)}\")\n",
    "                print(f\"  ‚è±Ô∏è Tempo iterazione: {iter_elapsed:.1f}s\")\n",
    "                \n",
    "                if problematic_stats:\n",
    "                    print(\"  üîç Top 5 cluster problematici:\")\n",
    "                    for cid, size, time_min in problematic_stats[:5]:\n",
    "                        print(f\"    Cluster {cid}: {size} punti, {time_min:.1f} min\")\n",
    "            \n",
    "            # Condizioni di uscita\n",
    "            if num_problematic <= early_stopping_threshold:\n",
    "                print(f\"‚úÖ Early stopping: solo {num_problematic} cluster problematici\")\n",
    "                break\n",
    "            \n",
    "            if iterations_without_improvement >= 3:\n",
    "                print(f\"üîÑ Nessun miglioramento per 3 iterazioni, fermata anticipata\")\n",
    "                break\n",
    "            \n",
    "            # Preparazione iterazione successiva - RECLUSTERING\n",
    "            if iteration < max_iterations and num_problematic > 0:\n",
    "                reclustering_plan = self._smart_reclustering_strategy(\n",
    "                    problematic_clusters, cluster_dict, cluster_results\n",
    "                )\n",
    "                \n",
    "                problematic_points_mask = points['cluster'].isin(problematic_clusters)\n",
    "                problematic_points = points[problematic_points_mask].copy()\n",
    "                good_points = points[~problematic_points_mask].copy()\n",
    "                \n",
    "                if len(problematic_points) > 0:\n",
    "                    total_new_clusters = sum(reclustering_plan.values())\n",
    "                    \n",
    "                    if total_new_clusters < len(problematic_points):\n",
    "                        problematic_scaled = scaler.transform(problematic_points[['lat', 'lon']])\n",
    "                        \n",
    "                        sub_kmeans = KMeans(\n",
    "                            n_clusters=min(total_new_clusters, len(problematic_points)),\n",
    "                            random_state=42 + iteration * 10,\n",
    "                            n_init=3,\n",
    "                            max_iter=50\n",
    "                        )\n",
    "                        \n",
    "                        sub_labels = sub_kmeans.fit_predict(problematic_scaled)\n",
    "                        \n",
    "                        max_existing = good_points['cluster'].max() if len(good_points) > 0 else -1\n",
    "                        problematic_points['cluster'] = sub_labels + max_existing + 1\n",
    "                        \n",
    "                        points = pd.concat([good_points, problematic_points], ignore_index=True)\n",
    "                        current_k = points['cluster'].nunique()\n",
    "                        \n",
    "                        if verbose:\n",
    "                            print(f\"  üîß Riclusterizzati {len(problematic_clusters)} ‚Üí {total_new_clusters} nuovi cluster\")\n",
    "                    else:\n",
    "                        current_k += len(problematic_clusters)\n",
    "                        if verbose:\n",
    "                            print(f\"  üìà Incremento K globale: {current_k}\")\n",
    "        \n",
    "        # Finalizzazione\n",
    "        total_elapsed = time.time() - total_start_time\n",
    "        self._save_cache()\n",
    "        \n",
    "        if best_solution is None:\n",
    "            best_solution = cluster_dict\n",
    "        \n",
    "        # Rinumera cluster finale\n",
    "        final_clusters = {}\n",
    "        for new_id, (old_id, location_ids) in enumerate(best_solution.items()):\n",
    "            if len(location_ids) > 0:\n",
    "                final_clusters[new_id] = location_ids\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nüèÅ COMPLETATO in {total_elapsed:.1f}s totali ({total_elapsed/60:.1f} minuti)\")\n",
    "            print(f\"üìä Soluzione finale: {len(final_clusters)} cluster\")\n",
    "            \n",
    "            sizes = [len(locs) for locs in final_clusters.values()]\n",
    "            print(f\"üìè Dimensioni cluster: min={min(sizes)}, max={max(sizes)}, media={np.mean(sizes):.1f}\")\n",
    "        \n",
    "        # *** CALCOLA OUTPUT CON calc_clusters_stats ***\n",
    "        if verbose:\n",
    "            print(\"üìä Calcolo performance cluster finale con calc_clusters_stats...\")\n",
    "        \n",
    "        clusters_list = list(final_clusters.values())\n",
    "        \n",
    "        performance_df = pc.calc_clusters_stats_ON(\n",
    "            clusters=clusters_list,\n",
    "            time_limit=time_limit_per_tsp,\n",
    "            parallel=True,\n",
    "            max_workers=self.n_cores,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        return final_clusters, performance_df\n",
    "\n",
    "\n",
    "# Funzione wrapper semplice\n",
    "def run_optimized_balanced_clustering(delivery_points, \n",
    "                                     initial_k=None, \n",
    "                                     max_iterations=15, \n",
    "                                     n_cores=None):\n",
    "    \n",
    "    if initial_k is None:\n",
    "        initial_k = max(10, len(delivery_points) // 150)\n",
    "        initial_k = min(initial_k, 50)\n",
    "        print(f\"üéØ K iniziale stimato: {initial_k}\")\n",
    "    \n",
    "    print(f\"üéØ Dataset: {len(delivery_points)} punti\")\n",
    "    print(f\"üéØ K iniziale: {initial_k}\")\n",
    "    \n",
    "    clusterer = OptimizedBalancedClustering(\n",
    "        max_shift_time_min=480,\n",
    "        n_cores=n_cores\n",
    "    )\n",
    "    \n",
    "    return clusterer.run_optimized_clustering(\n",
    "        delivery_points=delivery_points,\n",
    "        initial_k=initial_k,\n",
    "        max_iterations=max_iterations\n",
    "    )\n",
    "\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "cluster_dict, performance_df = run_optimized_balanced_clustering(pc.delivery_points_ON, initial_k=50, n_cores=8, max_iterations=25)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Tempo di esecuzione algoritmo: {(end - start)/60:.2f} min\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Salvataggio output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df.to_csv(\"clustering_methods_performances/k-means_iterative_v2(k=50)_ON_5.csv\", index=False)\n",
    "\n",
    "\n",
    "with open('cluster_dicts/cluster_dict_k_means_iter_v2(k=50)_ON_5.pkl', 'wb') as f:\n",
    "    pickle.dump(cluster_dict, f)\n",
    "\n",
    "# # Caricamento veloce  \n",
    "# with open('cluster_dicts/cluster_dict_k_means_iter_v2.pkl', 'rb') as f:\n",
    "#     cluster_dict = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

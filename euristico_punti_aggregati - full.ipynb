{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Modello euristico con gruppi pre-aggregati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Import pacchetti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 0) IMPORT\n",
    "# ------------------------------------------------------------\n",
    "from tabnanny import verbose\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pickle\n",
    "\n",
    "# Notebook esterno con tutte le funzioni di routing\n",
    "import import_ipynb\n",
    "import performance_calc as pc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Caricamento dati pre-aggregati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il dizionario dei gruppi pre-aggregati\n",
    "with open('clusters_output_punti_simili_full.pkl', 'rb') as f:\n",
    "    aggregated_groups = pickle.load(f)\n",
    "\n",
    "print(f\"Gruppi caricati: {len(aggregated_groups)}\")\n",
    "print(f\"Esempio gruppo: {list(aggregated_groups.items())[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Funzioni di supporto per gestire i gruppi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_aggregated_data(aggregated_groups, delivery_points):\n",
    "    \"\"\"\n",
    "    Prepara i dati per il clustering considerando i gruppi aggregati.\n",
    "    \n",
    "    Args:\n",
    "        aggregated_groups: dizionario {chiave: (centroid_id, [location_ids])}\n",
    "        delivery_points: DataFrame originale con tutti i punti\n",
    "    \n",
    "    Returns:\n",
    "        aggregated_points_df: DataFrame con i centroidi dei gruppi\n",
    "        group_mapping: dizionario {centroid_id: [location_ids nel gruppo]}\n",
    "        group_sizes: dizionario {centroid_id: numero di punti nel gruppo}\n",
    "    \"\"\"\n",
    "    group_mapping = {}\n",
    "    group_sizes = {}\n",
    "    centroid_ids = []\n",
    "    \n",
    "    for key, (centroid_id, location_ids) in aggregated_groups.items():\n",
    "        group_mapping[centroid_id] = location_ids\n",
    "        group_sizes[centroid_id] = len(location_ids)\n",
    "        centroid_ids.append(centroid_id)\n",
    "    \n",
    "    # Crea DataFrame con solo i centroidi\n",
    "    aggregated_points_df = delivery_points[\n",
    "        delivery_points['location_id'].isin(centroid_ids)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Gruppi totali: {len(group_mapping)}\")\n",
    "    print(f\"Punti totali nei gruppi: {sum(group_sizes.values())}\")\n",
    "    print(f\"Punti medi per gruppo: {sum(group_sizes.values()) / len(group_sizes):.2f}\")\n",
    "    \n",
    "    return aggregated_points_df, group_mapping, group_sizes\n",
    "\n",
    "\n",
    "def expand_cluster_with_groups(cluster_centroid_ids, group_mapping):\n",
    "    \"\"\"\n",
    "    Espande un cluster da centroidi a tutti i location_id reali.\n",
    "    \n",
    "    Args:\n",
    "        cluster_centroid_ids: lista di centroid_id\n",
    "        group_mapping: dizionario {centroid_id: [location_ids]}\n",
    "    \n",
    "    Returns:\n",
    "        lista di tutti i location_id espansi\n",
    "    \"\"\"\n",
    "    expanded_cluster = []\n",
    "    for centroid_id in cluster_centroid_ids:\n",
    "        expanded_cluster.extend(group_mapping[centroid_id])\n",
    "    return expanded_cluster\n",
    "\n",
    "\n",
    "def calculate_cluster_unloading_time(cluster_centroid_ids, group_sizes, unloading_time_per_point=10):\n",
    "    \"\"\"\n",
    "    Calcola il tempo totale di scarico considerando tutti i punti nei gruppi.\n",
    "    \n",
    "    Args:\n",
    "        cluster_centroid_ids: lista di centroid_id nel cluster\n",
    "        group_sizes: dizionario {centroid_id: numero di punti}\n",
    "        unloading_time_per_point: minuti per punto (default 10)\n",
    "    \n",
    "    Returns:\n",
    "        tempo totale di scarico in minuti\n",
    "    \"\"\"\n",
    "    total_points = sum(group_sizes[cid] for cid in cluster_centroid_ids)\n",
    "    return total_points * unloading_time_per_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Algoritmo di clustering adattato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_euristico_multicluster_aggregated(full_time_mat_min: np.ndarray,\n",
    "                                                  index_to_location_id: list[int],\n",
    "                                                  aggregated_points: pd.DataFrame,\n",
    "                                                  group_sizes: dict,\n",
    "                                                  max_time: int = 500,\n",
    "                                                  unloading_time_per_point: int = 10):\n",
    "    \"\"\"\n",
    "    Algoritmo Cluster-First, Route-Second adattato per gruppi pre-aggregati.\n",
    "    \n",
    "    â€¢ seed = gruppo piÃ¹ distante dal deposito\n",
    "    â€¢ greedy: aggiunge il gruppo che allunga meno il tour\n",
    "    â€¢ stop quando supererebbe max_time\n",
    "    â€¢ Il tempo di unloading considera TUTTI i punti nel gruppo (non solo il centroide)\n",
    "    \"\"\"\n",
    "    n_groups = full_time_mat_min.shape[0]\n",
    "    unassigned = set(range(1, n_groups))  # deposito Ã¨ indice 0\n",
    "    clusters = []\n",
    "    \n",
    "    while unassigned:\n",
    "        # --- nuovo cluster\n",
    "        first_idx = max(unassigned, key=lambda i: full_time_mat_min[0, i])\n",
    "        cluster = [first_idx]\n",
    "        unassigned.remove(first_idx)\n",
    "        \n",
    "        # IMPORTANTE: il tempo di unloading considera TUTTI i punti nel gruppo\n",
    "        first_centroid_id = index_to_location_id[first_idx]\n",
    "        first_group_points = group_sizes[first_centroid_id]\n",
    "        cluster_time = full_time_mat_min[0, first_idx] * 2 + (first_group_points * unloading_time_per_point)\n",
    "        \n",
    "        # --- inserimento greedy\n",
    "        while unassigned:\n",
    "            # tempo minimo dal cluster al candidato\n",
    "            cand, min_cost = min(\n",
    "                ((c, min(full_time_mat_min[p, c] for p in cluster)) for c in unassigned),\n",
    "                key=lambda x: x[1]\n",
    "            )\n",
    "            \n",
    "            # IMPORTANTE: calcola unloading time per tutti i punti del gruppo candidato\n",
    "            cand_centroid_id = index_to_location_id[cand]\n",
    "            cand_group_points = group_sizes[cand_centroid_id]\n",
    "            cand_unloading_time = cand_group_points * unloading_time_per_point\n",
    "            \n",
    "            if cluster_time + min_cost + cand_unloading_time > max_time:\n",
    "                break\n",
    "            \n",
    "            cluster.append(cand)\n",
    "            unassigned.remove(cand)\n",
    "            cluster_time += min_cost + cand_unloading_time\n",
    "        \n",
    "        # Converti indici in centroid_ids\n",
    "        clusters.append([index_to_location_id[i] for i in cluster])\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(clusters, delivery_points, title):\n",
    "    \"\"\"Visual plot rapido\"\"\"\n",
    "    cmap = plt.cm.get_cmap(\"tab20\", len(clusters))\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    for i, cl in enumerate(clusters):\n",
    "        sub = delivery_points[delivery_points[\"location_id\"].isin(cl)]\n",
    "        plt.scatter(sub[\"lon\"], sub[\"lat\"], c=[cmap(i)],\n",
    "                   label=f\"C{i+1}\", alpha=.7, s=20, edgecolors=\"k\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"lon\"); plt.ylabel(\"lat\")\n",
    "    plt.legend(ncol=2, fontsize=\"small\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def deduplicate_clusters(clusters, already_assigned_groups):\n",
    "    \"\"\"\n",
    "    Rimuove gruppi giÃ  assegnati dai cluster e restituisce solo cluster validi\n",
    "    \n",
    "    Args:\n",
    "        clusters: lista di cluster (liste di centroid_id)\n",
    "        already_assigned_groups: set di centroid_id giÃ  assegnati\n",
    "    \n",
    "    Returns:\n",
    "        result: lista di cluster puliti\n",
    "        assigned: set aggiornato di gruppi assegnati\n",
    "    \"\"\"\n",
    "    assigned = set(already_assigned_groups)\n",
    "    result = []\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        clean_cluster = [cid for cid in cluster if cid not in assigned]\n",
    "        if clean_cluster:  # Solo se il cluster ha almeno un gruppo\n",
    "            result.append(clean_cluster)\n",
    "            assigned.update(clean_cluster)\n",
    "    \n",
    "    return result, assigned\n",
    "\n",
    "\n",
    "def get_last_iteration_rejected_clusters(all_results, already_assigned_groups, verbose=True):\n",
    "    \"\"\"\n",
    "    Recupera i cluster RIFIUTATI dell'ULTIMA iterazione, filtrando i gruppi giÃ  assegnati\n",
    "    \n",
    "    Args:\n",
    "        all_results: dizionario con i risultati di tutte le iterazioni\n",
    "        already_assigned_groups: set di centroid_id giÃ  assegnati\n",
    "        verbose: stampa messaggi di debug\n",
    "    \n",
    "    Returns:\n",
    "        lista di cluster filtrati\n",
    "    \"\"\"\n",
    "    if not all_results:\n",
    "        return []\n",
    "    \n",
    "    # Prendi l'ultima iterazione\n",
    "    last_iteration = max(all_results.keys())\n",
    "    last_results = all_results[last_iteration]\n",
    "    \n",
    "    # Recupera i cluster rifiutati\n",
    "    rejected_cluster_indices = last_results['rejected_indices']\n",
    "    all_clusters = last_results['clusters']\n",
    "    rejected_clusters = [all_clusters[i] for i in rejected_cluster_indices]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\" ðŸ” Recuperando cluster rifiutati dall'ultima iterazione ({last_iteration})\")\n",
    "        print(f\" ðŸ“‹ Cluster rifiutati disponibili: {len(rejected_clusters)}\")\n",
    "    \n",
    "    # Filtra gruppi giÃ  assegnati\n",
    "    filtered_clusters, _ = deduplicate_clusters(rejected_clusters, already_assigned_groups)\n",
    "    \n",
    "    if verbose:\n",
    "        total_groups = sum(len(cl) for cl in filtered_clusters)\n",
    "        print(f\" âœ… Cluster recuperati dopo filtro: {len(filtered_clusters)} ({total_groups} gruppi)\")\n",
    "    \n",
    "    return filtered_clusters\n",
    "\n",
    "\n",
    "def count_clusters_with_excessive_overtime(performance_df, threshold=1):\n",
    "    \"\"\"\n",
    "    Conta quanti cluster hanno piÃ¹ di 'threshold' giorni di overtime\n",
    "    \n",
    "    Args:\n",
    "        performance_df: DataFrame con le performance dei cluster\n",
    "        threshold: soglia di overtime (default 1)\n",
    "    \n",
    "    Returns:\n",
    "        numero di cluster con overtime > threshold\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for cluster_name, cluster_data in performance_df.groupby('cluster'):\n",
    "        total_overtime = cluster_data['n_overtime_days'].sum()\n",
    "        if total_overtime > threshold:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def evaluate_and_accept_clusters(clusters, performance_df, verbose=True):\n",
    "    \"\"\"\n",
    "    Valuta i cluster e decide quali accettare basandosi sui criteri:\n",
    "    1) Media giornaliera tra 390-480 minuti\n",
    "    2) Overtime accettabile (massimo 1 giorno per tipo di giorno della settimana)\n",
    "    \n",
    "    Returns:\n",
    "        accepted_clusters: list di cluster (liste di centroid_id) accettati\n",
    "        rejected_indices: list di indici dei cluster rifiutati\n",
    "    \"\"\"\n",
    "    accepted_clusters = []\n",
    "    rejected_indices = []\n",
    "    \n",
    "    # Raggruppa performance per cluster\n",
    "    for cluster_name, cluster_data in performance_df.groupby('cluster'):\n",
    "        cluster_index = int(cluster_name.split()[-1]) - 1  # \"Cluster 1\" -> index 0\n",
    "        cluster = clusters[cluster_index]\n",
    "        \n",
    "        # CRITERIO 1: Media giornaliera tra 390-480 minuti\n",
    "        mean_minutes_range = cluster_data['mean_minutes'].between(390, 480).any()\n",
    "        \n",
    "        # CRITERIO 2: Overtime accettabile\n",
    "        overtime_acceptable = check_overtime_acceptable(cluster_data)\n",
    "        \n",
    "        if mean_minutes_range or overtime_acceptable:\n",
    "            accepted_clusters.append(cluster)\n",
    "            if verbose:\n",
    "                max_mean = cluster_data['mean_minutes'].max()\n",
    "                total_overtime = cluster_data['n_overtime_days'].sum()\n",
    "                reason = \"mean_range\" if mean_minutes_range else \"overtime_ok\"\n",
    "                print(f\" âœ… {cluster_name}: {len(cluster)} gruppi, \"\n",
    "                     f\"max_mean={max_mean:.1f}min, overtime={total_overtime}, \"\n",
    "                     f\"motivo={reason}\")\n",
    "        else:\n",
    "            rejected_indices.append(cluster_index)\n",
    "            if verbose:\n",
    "                max_mean = cluster_data['mean_minutes'].max()\n",
    "                total_overtime = cluster_data['n_overtime_days'].sum()\n",
    "                print(f\" âŒ {cluster_name}: {len(cluster)} gruppi, \"\n",
    "                     f\"max_mean={max_mean:.1f}min, overtime={total_overtime}, \"\n",
    "                     f\"RIFIUTATO\")\n",
    "    \n",
    "    return accepted_clusters, rejected_indices\n",
    "\n",
    "\n",
    "def check_overtime_acceptable(cluster_data):\n",
    "    \"\"\"\n",
    "    Controlla se l'overtime Ã¨ accettabile:\n",
    "    massimo 1 giorno di overtime per ogni giorno della settimana\n",
    "    \n",
    "    Args:\n",
    "        cluster_data: DataFrame con performance di un singolo cluster\n",
    "    \n",
    "    Returns:\n",
    "        bool: True se overtime accettabile, False altrimenti\n",
    "    \"\"\"\n",
    "    # Controlla per ogni giorno della settimana\n",
    "    for _, row in cluster_data.iterrows():\n",
    "        if row['n_overtime_days'] > 1:  # PiÃ¹ di 1 giorno di overtime -> non accettato\n",
    "            return False\n",
    "        elif row['n_overtime_days'] == 1:\n",
    "            return True  # Ok, 1 giorno di overtime Ã¨ accettabile\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Funzione principale iterativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic_iterative_clustering_aggregated(aggregated_groups: dict,\n",
    "                                              delivery_points: pd.DataFrame,\n",
    "                                              depot_location: tuple[float, float],\n",
    "                                              initial_threshold: int = 800,\n",
    "                                              increment_threshold: int = 120,\n",
    "                                              unload_min_per_point: int = 10,\n",
    "                                              max_iterations: int = 10,\n",
    "                                              verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Algoritmo iterativo per clustering con gruppi pre-aggregati.\n",
    "    \n",
    "    Args:\n",
    "        aggregated_groups: dizionario {chiave: (centroid_id, [location_ids])}\n",
    "        delivery_points: DataFrame con TUTTI i punti originali\n",
    "        depot_location: tuple (lat, lon) del deposito\n",
    "        initial_threshold: soglia iniziale in minuti\n",
    "        increment_threshold: incremento della soglia per iterazione\n",
    "        unload_min_per_point: minuti di scarico per punto (default 10)\n",
    "        max_iterations: numero massimo di iterazioni\n",
    "        verbose: stampa messaggi di debug\n",
    "    \n",
    "    Returns:\n",
    "        all_results: dict con risultati intermedi\n",
    "        final_clusters_expanded: lista di cluster con TUTTI i location_id espansi\n",
    "        final_performance: DataFrame con performance calcolate sui punti reali\n",
    "        refinement_applied: bool indicante se Ã¨ stato applicato refinement\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"ðŸ”„ INIZIO clustering iterativo con gruppi pre-aggregati\")\n",
    "        print(f\" â€¢ Soglia iniziale: {initial_threshold} min\")\n",
    "        print(f\" â€¢ Incremento: +{increment_threshold} min per iterazione\")\n",
    "    \n",
    "    # Prepara i dati aggregati\n",
    "    aggregated_points_df, group_mapping, group_sizes = prepare_aggregated_data(\n",
    "        aggregated_groups, delivery_points\n",
    "    )\n",
    "    \n",
    "    all_results = {}\n",
    "    remaining_groups = aggregated_points_df.copy()\n",
    "    original_groups = set(aggregated_points_df['location_id'])\n",
    "    already_assigned_groups = set()\n",
    "    current_threshold = initial_threshold\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        if verbose:\n",
    "            print(f\"\\nðŸ”„ Iterazione {iteration + 1}: soglia = {current_threshold} min\")\n",
    "            print(f\" â€¢ Gruppi rimanenti: {len(remaining_groups)}\")\n",
    "        \n",
    "        if remaining_groups.empty:\n",
    "            if verbose:\n",
    "                print(\" âœ… Nessun gruppo rimanente - STOP\")\n",
    "            break\n",
    "        \n",
    "        # ================== CLUSTERING sui centroidi ==================\n",
    "        dm, tm, idx2loc, _ = pc.distance_matrix_creation('', remaining_groups)\n",
    "        \n",
    "        # Crea dizionario group_sizes solo per i gruppi rimanenti\n",
    "        remaining_group_sizes = {\n",
    "            cid: group_sizes[cid] for cid in remaining_groups['location_id']\n",
    "        }\n",
    "        \n",
    "        clusters_centroids = clustering_euristico_multicluster_aggregated(\n",
    "            tm, idx2loc, remaining_groups,\n",
    "            remaining_group_sizes,\n",
    "            max_time=current_threshold,\n",
    "            unloading_time_per_point=unload_min_per_point\n",
    "        )\n",
    "        \n",
    "        if not clusters_centroids:\n",
    "            if verbose:\n",
    "                print(\" âš ï¸ Nessun cluster generato - STOP\")\n",
    "            break\n",
    "        \n",
    "        # ================== ESPANDI cluster per routing e performance ==================\n",
    "        # Per il calcolo delle performance, espandi i centroidi in punti reali\n",
    "        clusters_expanded = [\n",
    "            expand_cluster_with_groups(cluster, group_mapping)\n",
    "            for cluster in clusters_centroids\n",
    "        ]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\" ðŸ“¦ Cluster creati: {len(clusters_expanded)}\")\n",
    "            # for i, cl in enumerate(clusters_expanded):\n",
    "            #     print(f\"    Cluster {i+1}: {len(cl)} punti reali\")\n",
    "        \n",
    "        # ================== PERFORMANCE sui punti reali ==================\n",
    "        performance_df = pc.calc_clusters_stats(\n",
    "            clusters_expanded, time_limit=3, parallel=True, max_workers=7, verbose=False\n",
    "        )\n",
    "        \n",
    "        # ================== VALUTAZIONE E ACCETTAZIONE ==================\n",
    "        # Valuta usando i centroidi, ma le performance sono sui punti reali\n",
    "        accepted_clusters_centroids, rejected_cluster_indices = evaluate_and_accept_clusters(\n",
    "            clusters_centroids, performance_df, verbose=verbose\n",
    "        )\n",
    "        \n",
    "        # Salva risultati dell'iterazione\n",
    "        all_results[iteration + 1] = {\n",
    "            'threshold': current_threshold,\n",
    "            'clusters': clusters_centroids,  # Salva centroidi\n",
    "            'clusters_expanded': clusters_expanded,  # Salva anche espansi\n",
    "            'performance_df': performance_df,\n",
    "            'accepted_clusters': accepted_clusters_centroids,\n",
    "            'rejected_indices': rejected_cluster_indices\n",
    "        }\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\" âœ… Cluster accettati: {len(accepted_clusters_centroids)}/{len(clusters_centroids)}\")\n",
    "        \n",
    "        # ================== RIMOZIONE GRUPPI ACCETTATI ==================\n",
    "        accepted_group_ids = set()\n",
    "        for cluster in accepted_clusters_centroids:\n",
    "            accepted_group_ids.update(cluster)\n",
    "        \n",
    "        existing_ids = set(remaining_groups['location_id'])\n",
    "        valid_accepted_ids = accepted_group_ids.intersection(existing_ids)\n",
    "        \n",
    "        already_assigned_groups.update(valid_accepted_ids)\n",
    "        remaining_groups = remaining_groups[\n",
    "            ~remaining_groups['location_id'].isin(valid_accepted_ids)\n",
    "        ].copy()\n",
    "        \n",
    "        # Se tutti i cluster sono stati accettati, abbiamo finito\n",
    "        if len(accepted_clusters_centroids) == len(clusters_centroids):\n",
    "            if verbose:\n",
    "                print(\" ðŸŽ¯ Tutti i cluster sono stati accettati - STOP\")\n",
    "            break\n",
    "        \n",
    "        # Incrementa soglia per prossima iterazione\n",
    "        current_threshold += increment_threshold\n",
    "    \n",
    "    # ================== ASSEMBLAGGIO FINALE ==================\n",
    "    if verbose:\n",
    "        print(f\"\\nðŸ”§ Assemblaggio soluzione finale...\")\n",
    "    \n",
    "    final_clusters_centroids = []\n",
    "    \n",
    "    # 1. Aggiungi tutti i cluster accettati\n",
    "    for iteration_results in all_results.values():\n",
    "        for cluster in iteration_results['accepted_clusters']:\n",
    "            final_clusters_centroids.append(cluster)\n",
    "    \n",
    "    # 2. Per i gruppi rimanenti, usa i cluster rifiutati dell'ultima iterazione\n",
    "    unassigned_groups = original_groups - already_assigned_groups\n",
    "    \n",
    "    if unassigned_groups:\n",
    "        if verbose:\n",
    "            print(f\" ðŸ“‹ Gruppi non assegnati: {len(unassigned_groups)}\")\n",
    "            print(\" ðŸ” Recuperando cluster rifiutati dall'ultima iterazione...\")\n",
    "        \n",
    "        last_iteration_clusters = get_last_iteration_rejected_clusters(\n",
    "            all_results, already_assigned_groups, verbose\n",
    "        )\n",
    "        \n",
    "        final_clusters_centroids.extend(last_iteration_clusters)\n",
    "        for cluster in last_iteration_clusters:\n",
    "            already_assigned_groups.update(cluster)\n",
    "    \n",
    "    # 3. ESPANDI TUTTI i cluster finali da centroidi a punti reali\n",
    "    final_clusters_expanded = [\n",
    "        expand_cluster_with_groups(cluster, group_mapping)\n",
    "        for cluster in final_clusters_centroids\n",
    "    ]\n",
    "    \n",
    "    # ================== PERFORMANCE FINALI sui punti reali ==================\n",
    "    if verbose:\n",
    "        print(f\"\\nðŸ“Š Calcolando performance finali per {len(final_clusters_expanded)} cluster...\")\n",
    "    \n",
    "    final_performance = pc.calc_clusters_stats(\n",
    "        final_clusters_expanded, time_limit=3, parallel=True, max_workers=7, verbose=False\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        total_groups = sum(len(cluster) for cluster in final_clusters_centroids)\n",
    "        total_points = sum(len(cluster) for cluster in final_clusters_expanded)\n",
    "        print(f\"\\nðŸ COMPLETATO:\")\n",
    "        print(f\" â€¢ Iterazioni eseguite: {len(all_results)}\")\n",
    "        print(f\" â€¢ Cluster finali totali: {len(final_clusters_expanded)}\")\n",
    "        print(f\" â€¢ Gruppi totali assegnati: {total_groups}\")\n",
    "        print(f\" â€¢ Punti totali assegnati: {total_points}\")\n",
    "        print(f\" â€¢ Copertura: {total_points}/{len(delivery_points)} punti\")\n",
    "    \n",
    "    # ================== CHECK OVERTIME ECCESSIVO E REFINEMENT ==================\n",
    "    excessive_overtime_count = count_clusters_with_excessive_overtime(final_performance, threshold=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nðŸ“Š Cluster con piÃ¹ di 1 giorno overtime: {excessive_overtime_count}\")\n",
    "    \n",
    "    refinement_needed = excessive_overtime_count > 5\n",
    "    \n",
    "    if refinement_needed:\n",
    "        if verbose:\n",
    "            print(f\"\\nðŸ”„ REFINEMENT NECESSARIO: {excessive_overtime_count} cluster con overtime > 1\")\n",
    "            print(\" ðŸš€ Avvio 5 cicli aggiuntivi con soglia 1400 min (+60/iterazione)...\")\n",
    "        \n",
    "        # Identifica i gruppi nei cluster problematici\n",
    "        problematic_groups = set()\n",
    "        for cluster_name, cluster_data in final_performance.groupby('cluster'):\n",
    "            total_overtime = cluster_data['n_overtime_days'].sum()\n",
    "            if total_overtime > 1:\n",
    "                cluster_index = int(cluster_name.split()[-1]) - 1\n",
    "                problematic_cluster_centroids = final_clusters_centroids[cluster_index]\n",
    "                problematic_groups.update(problematic_cluster_centroids)\n",
    "        \n",
    "        # Crea DataFrame con i gruppi problematici\n",
    "        refinement_groups_df = aggregated_points_df[\n",
    "            aggregated_points_df['location_id'].isin(problematic_groups)\n",
    "        ].copy()\n",
    "        \n",
    "        # Crea dizionario aggregated_groups solo per i gruppi problematici\n",
    "        refinement_aggregated_groups = {\n",
    "            k: v for k, v in aggregated_groups.items()\n",
    "            if v[0] in problematic_groups\n",
    "        }\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\" ðŸ“‹ Gruppi da riclusterare: {len(refinement_groups_df)}\")\n",
    "        \n",
    "        # Applica refinement (ricorsivo)\n",
    "        refinement_results, refinement_clusters, refinement_performance, _ = heuristic_iterative_clustering_aggregated(\n",
    "            aggregated_groups=refinement_aggregated_groups,\n",
    "            delivery_points=delivery_points,\n",
    "            depot_location=depot_location,\n",
    "            initial_threshold=1400,\n",
    "            increment_threshold=60,\n",
    "            unload_min_per_point=unload_min_per_point,\n",
    "            max_iterations=5,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        # Sostituisci i cluster problematici con quelli raffinati\n",
    "        problematic_points_expanded = set()\n",
    "        for gid in problematic_groups:\n",
    "            problematic_points_expanded.update(group_mapping[gid])\n",
    "        \n",
    "        final_clusters_expanded = [\n",
    "            cl for cl in final_clusters_expanded\n",
    "            if not any(p in problematic_points_expanded for p in cl)\n",
    "        ]\n",
    "        final_clusters_expanded.extend(refinement_clusters)\n",
    "        \n",
    "        # Ricalcola performance finali\n",
    "        final_performance = pc.calc_clusters_stats(\n",
    "            final_clusters_expanded, time_limit=3, parallel=True, max_workers=7, verbose=False\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nâœ… REFINEMENT COMPLETATO\")\n",
    "            print(f\" â€¢ Nuovi cluster totali: {len(final_clusters_expanded)}\")\n",
    "            excessive_after = count_clusters_with_excessive_overtime(final_performance, threshold=1)\n",
    "            print(f\" â€¢ Cluster con overtime > 1: {excessive_after} (prima: {excessive_overtime_count})\")\n",
    "        \n",
    "        return all_results, final_clusters_expanded, final_performance, True\n",
    "    \n",
    "    return all_results, final_clusters_expanded, final_performance, False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Esecuzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esegui il clustering\n",
    "all_results, final_clusters, final_performance, refinement_applied = heuristic_iterative_clustering_aggregated(\n",
    "    aggregated_groups=aggregated_groups,\n",
    "    delivery_points=pc.delivery_points,\n",
    "    depot_location=pc.depot_location,\n",
    "    initial_threshold=800,\n",
    "    increment_threshold=100,\n",
    "    unload_min_per_point=10,\n",
    "    max_iterations=15,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Stampa risultato refinement\n",
    "if refinement_applied:\n",
    "    print(\"\\nðŸŽ¯ REFINEMENT APPLICATO con successo!\")\n",
    "else:\n",
    "    print(\"\\nâœ… NESSUN REFINEMENT necessario - soluzione ottimale trovata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Salvataggio risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva i risultati\n",
    "final_performance.to_csv('clustering_methods_performances/euristic_aggregated.csv')\n",
    "\n",
    "with open('cluster_dicts/cluster_dict_euristic_aggregated.pkl', 'wb') as f:\n",
    "    pickle.dump(final_clusters, f)\n",
    "\n",
    "print(\"âœ… Risultati salvati con successo!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

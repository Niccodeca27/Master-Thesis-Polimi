{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Versione 5 - Con Gruppi Pre-Aggregati\n",
    "\n",
    "Adatta l'algoritmo euristico V5 per lavorare con gruppi pre-aggregati da `clusters_output_punti_simili_ON.pkl`.\n",
    "\n",
    "**Criteri V5:**\n",
    "- Split: max(mean_minutes) > 450 e almeno 5 GRUPPI, split in ceil(max/450) parti\n",
    "- Cluster buoni: max(mean_minutes) tra 350 e 450 incluso\n",
    "- Merge: somma mean_minutes < 450 per ogni giorno\n",
    "- Cluster troppo piccoli: quelli con meno di 3 GRUPPI (min_cluster_size=3)\n",
    "\n",
    "**Logica gruppi:**\n",
    "- I gruppi contano come singoli punti nel K-means (usando centroidi)\n",
    "- Per la stima dei tempi: ogni punto nel gruppo = 10 min di unloading\n",
    "- Per il routing: si espandono i gruppi in punti reali"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Import e setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "from itertools import combinations\n",
    "\n",
    "import import_ipynb\n",
    "import performance_calc as pc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Caricamento gruppi pre-aggregati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il dizionario dei gruppi pre-aggregati\n",
    "with open('clusters_output_punti_simili_ON.pkl', 'rb') as f:\n",
    "    aggregated_groups = pickle.load(f)\n",
    "\n",
    "print(f\"Gruppi caricati: {len(aggregated_groups)}\")\n",
    "print(f\"Esempio: {list(aggregated_groups.items())[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Funzioni di supporto per gruppi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_aggregated_data(aggregated_groups, delivery_points):\n",
    "    \"\"\"\n",
    "    Prepara i dati per clustering:\n",
    "    - group_mapping: centroid_id -> [location_ids nel gruppo]\n",
    "    - group_sizes: centroid_id -> numero di punti reali nel gruppo\n",
    "    - aggregated_points_df: DataFrame con solo i centroidi dei gruppi\n",
    "    \"\"\"\n",
    "    group_mapping = {}\n",
    "    group_sizes = {}\n",
    "    centroid_ids = []\n",
    "    \n",
    "    for key, (centroid_id, location_ids) in aggregated_groups.items():\n",
    "        group_mapping[centroid_id] = location_ids\n",
    "        group_sizes[centroid_id] = len(location_ids)\n",
    "        centroid_ids.append(centroid_id)\n",
    "    \n",
    "    # DataFrame con solo i centroidi\n",
    "    aggregated_points_df = delivery_points[\n",
    "        delivery_points['location_id'].isin(centroid_ids)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"üì¶ Gruppi totali: {len(group_mapping)}\")\n",
    "    print(f\"üìç Punti reali totali: {sum(group_sizes.values())}\")\n",
    "    print(f\"üìä Media punti per gruppo: {sum(group_sizes.values()) / len(group_sizes):.2f}\")\n",
    "    \n",
    "    return aggregated_points_df, group_mapping, group_sizes\n",
    "\n",
    "\n",
    "def expand_cluster_with_groups(cluster_centroid_ids, group_mapping):\n",
    "    \"\"\"\n",
    "    Espande un cluster di centroidi in tutti i location_id reali.\n",
    "    \"\"\"\n",
    "    expanded = []\n",
    "    for centroid_id in cluster_centroid_ids:\n",
    "        expanded.extend(group_mapping[centroid_id])\n",
    "    return expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Classe AdaptivePerformanceClustering con Gruppi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptivePerformanceClusteringAggregated:\n",
    "    def __init__(self, \n",
    "                 aggregated_groups: dict,\n",
    "                 delivery_points: pd.DataFrame,\n",
    "                 n_cores: int = None,\n",
    "                 cache_dir: str = \"./cluster_cache\",\n",
    "                 max_iterations: int = 15,\n",
    "                 max_execution_time_min: int = 500):\n",
    "        \n",
    "        self.n_cores = n_cores or max(1, mp.cpu_count() - 1)\n",
    "        self.cache_dir = cache_dir\n",
    "        self.max_iterations = max_iterations\n",
    "        self.max_execution_time_min = max_execution_time_min\n",
    "        \n",
    "        # Prepara i dati aggregati\n",
    "        self.aggregated_points_df, self.group_mapping, self.group_sizes = prepare_aggregated_data(\n",
    "            aggregated_groups, delivery_points\n",
    "        )\n",
    "        \n",
    "        self.final_clusters = {}  # Cluster definitivi (con centroidi)\n",
    "        self.cluster_performances = {}\n",
    "        self.min_cluster_size = 3  # Minimo 3 GRUPPI per cluster\n",
    "        \n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        print(f\"üöÄ AdaptivePerformanceClusteringAggregated avviato con {self.n_cores} core\")\n",
    "    \n",
    "    def _cache_key(self, centroid_ids):\n",
    "        return hash(tuple(sorted(centroid_ids)))\n",
    "    \n",
    "    def _compute_batch_performances(self, cluster_dict, verbose=True):\n",
    "        \"\"\"\n",
    "        Calcola performance per i cluster.\n",
    "        cluster_dict contiene CENTROIDI, ma il routing viene fatto sui PUNTI REALI espansi.\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"üîé Calcolo performance in batch ({len(cluster_dict)} cluster)...\")\n",
    "        \n",
    "        # Separa cluster validi da quelli troppo piccoli\n",
    "        valid_clusters = {k: v for k, v in cluster_dict.items() if len(v) >= self.min_cluster_size}\n",
    "        small_clusters = {k: v for k, v in cluster_dict.items() if len(v) < self.min_cluster_size}\n",
    "        \n",
    "        # ESPANDI i centroidi in punti reali per il calcolo delle performance\n",
    "        if valid_clusters:\n",
    "            clusters_expanded = [\n",
    "                expand_cluster_with_groups(centroid_ids, self.group_mapping)\n",
    "                for centroid_ids in valid_clusters.values()\n",
    "            ]\n",
    "            cluster_ids = list(valid_clusters.keys())\n",
    "            \n",
    "            # Chiamata al calcolo performance sui PUNTI REALI\n",
    "            performance_df = pc.calc_clusters_stats_ON(\n",
    "                clusters=clusters_expanded,\n",
    "                time_limit=3,\n",
    "                parallel=True,\n",
    "                max_workers=self.n_cores,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Salva performance\n",
    "            for i, cluster_id in enumerate(cluster_ids):\n",
    "                name = f'Cluster {i+1}'\n",
    "                cluster_data = performance_df[performance_df['cluster'] == name]\n",
    "                \n",
    "                if not cluster_data.empty:\n",
    "                    max_mean = cluster_data['mean_minutes'].max()\n",
    "                    self.cluster_performances[cluster_id] = {\n",
    "                        'dataframe': cluster_data,\n",
    "                        'max_mean_minutes': max_mean,\n",
    "                        'is_valid': True\n",
    "                    }\n",
    "                else:\n",
    "                    self.cluster_performances[cluster_id] = {\n",
    "                        'dataframe': None,\n",
    "                        'max_mean_minutes': float('inf'),\n",
    "                        'is_valid': False\n",
    "                    }\n",
    "        \n",
    "        # Gestione cluster troppo piccoli (sempre mergeabili)\n",
    "        for cluster_id in small_clusters.keys():\n",
    "            self.cluster_performances[cluster_id] = {\n",
    "                'dataframe': None,\n",
    "                'max_mean_minutes': 0,\n",
    "                'is_valid': True,\n",
    "                'too_small': True\n",
    "            }\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\" ‚úÖ Performance salvate in self.cluster_performances\")\n",
    "        return\n",
    "    \n",
    "    def _save_good_clusters(self, cluster_dict, verbose=True):\n",
    "        \"\"\"\n",
    "        CRITERIO V5: Salva cluster con 350 <= max(mean_minutes) <= 450.\n",
    "        Cluster con meno di min_cluster_size GRUPPI NON sono salvabili.\n",
    "        \"\"\"\n",
    "        saved_count = 0\n",
    "        remaining_clusters = {}\n",
    "        \n",
    "        for cluster_id, centroid_ids in cluster_dict.items():\n",
    "            perf = self.cluster_performances.get(cluster_id)\n",
    "            \n",
    "            # I cluster troppo piccoli NON possono essere salvati\n",
    "            if len(centroid_ids) < self.min_cluster_size:\n",
    "                remaining_clusters[cluster_id] = centroid_ids\n",
    "                continue\n",
    "            \n",
    "            if perf and perf['is_valid']:\n",
    "                max_mean = perf['max_mean_minutes']\n",
    "                \n",
    "                # CRITERIO V5: 350 <= max(mean_minutes) <= 450\n",
    "                if 350 <= max_mean <= 450:\n",
    "                    final_id = len(self.final_clusters) + 1\n",
    "                    self.final_clusters[final_id] = centroid_ids\n",
    "                    saved_count += 1\n",
    "                    \n",
    "                    if verbose:\n",
    "                        total_points = sum(self.group_sizes[cid] for cid in centroid_ids)\n",
    "                        print(f\" ‚úÖ Accettato cluster {cluster_id}: max(mean_minutes)={max_mean:.1f} min, \"\n",
    "                              f\"{len(centroid_ids)} gruppi ({total_points} punti)\")\n",
    "                    continue\n",
    "            \n",
    "            remaining_clusters[cluster_id] = centroid_ids\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\" üèÅ Salvati {saved_count} cluster ideali (350‚â§max‚â§450 min)\")\n",
    "        \n",
    "        return remaining_clusters\n",
    "    \n",
    "    def _split_oversized_clusters(self, cluster_dict, verbose=True):\n",
    "        \"\"\"\n",
    "        CRITERIO V5: Divide cluster con max(mean_minutes) > 450 in N parti,\n",
    "        dove N = ceil(max_mean_minutes / 450).\n",
    "        Solo se il cluster ha pi√π di 4 GRUPPI.\n",
    "        \"\"\"\n",
    "        new_clusters = {}\n",
    "        clusters_to_recalc = {}\n",
    "        \n",
    "        for cluster_id, centroid_ids in cluster_dict.items():\n",
    "            perf = self.cluster_performances.get(cluster_id)\n",
    "            max_mean = perf['max_mean_minutes'] if perf else float('inf')\n",
    "            size = len(centroid_ids)  # Numero di GRUPPI\n",
    "            \n",
    "            # CRITERIO V5: split solo se > 450 min E pi√π di 4 gruppi\n",
    "            if size > 4 and max_mean > 450:\n",
    "                n_splits = int(np.ceil(max_mean / 450))\n",
    "                \n",
    "                if verbose:\n",
    "                    total_points = sum(self.group_sizes[cid] for cid in centroid_ids)\n",
    "                    print(f\" ‚úÇÔ∏è Cluster {cluster_id}: {size} gruppi ({total_points} punti), \"\n",
    "                          f\"max(mean_minutes)={max_mean:.1f} ‚Üí split in {n_splits}\")\n",
    "                \n",
    "                chunk_size = int(np.ceil(size / n_splits))\n",
    "                for i in range(n_splits):\n",
    "                    start = i * chunk_size\n",
    "                    end = min(start + chunk_size, size)\n",
    "                    chunk = centroid_ids[start:end]\n",
    "                    new_id = f\"s_{cluster_id}_{i+1}\"\n",
    "                    new_clusters[new_id] = chunk\n",
    "                    clusters_to_recalc[new_id] = chunk\n",
    "            else:\n",
    "                new_clusters[cluster_id] = centroid_ids\n",
    "        \n",
    "        # Ricalcola performance solo per i cluster splittati\n",
    "        if clusters_to_recalc:\n",
    "            if verbose:\n",
    "                print(f\" üîÑ Ricalcolo routing per {len(clusters_to_recalc)} nuovi cluster splittati\")\n",
    "            self._compute_batch_performances(clusters_to_recalc, verbose=False)\n",
    "        \n",
    "        return new_clusters\n",
    "    \n",
    "    def _can_merge_clusters(self, cluster_id1, cluster_id2):\n",
    "        \"\"\"\n",
    "        CRITERIO V5: Merge solo se somma mean_minutes < 450 per OGNI giorno.\n",
    "        Cluster troppo piccoli: sempre mergeabili.\n",
    "        \"\"\"\n",
    "        perf1 = self.cluster_performances.get(cluster_id1)\n",
    "        perf2 = self.cluster_performances.get(cluster_id2)\n",
    "        \n",
    "        # Cluster troppo piccoli: sempre mergeabili\n",
    "        if (perf1 and perf1.get('too_small')) or (perf2 and perf2.get('too_small')):\n",
    "            return True\n",
    "        \n",
    "        if not (perf1 and perf2 and perf1['is_valid'] and perf2['is_valid']):\n",
    "            return False\n",
    "        \n",
    "        df1 = perf1['dataframe']\n",
    "        df2 = perf2['dataframe']\n",
    "        \n",
    "        days1 = set(df1['weekday'])\n",
    "        days2 = set(df2['weekday'])\n",
    "        common_days = days1 & days2\n",
    "        \n",
    "        if not common_days:\n",
    "            return False\n",
    "        \n",
    "        # CRITERIO V5: somma < 450 per OGNI giorno\n",
    "        for day in common_days:\n",
    "            m1 = df1[df1['weekday'] == day]['mean_minutes'].iloc[0]\n",
    "            m2 = df2[df2['weekday'] == day]['mean_minutes'].iloc[0]\n",
    "            if m1 + m2 >= 450:\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _divide_space_into_sectors(self, delivery_points):\n",
    "        \"\"\"\n",
    "        Divide lo spazio in 4 settori (NE, NW, SE, SW) usando mediane.\n",
    "        \"\"\"\n",
    "        lat_median = delivery_points['lat'].median()\n",
    "        lon_median = delivery_points['lon'].median()\n",
    "        \n",
    "        sectors = {'NE': [], 'NW': [], 'SE': [], 'SW': []}\n",
    "        \n",
    "        for cluster_id in delivery_points['cluster'].unique():\n",
    "            cluster_points = delivery_points[delivery_points['cluster'] == cluster_id]\n",
    "            if len(cluster_points) == 0:\n",
    "                continue\n",
    "            \n",
    "            center_lat = cluster_points['lat'].mean()\n",
    "            center_lon = cluster_points['lon'].mean()\n",
    "            \n",
    "            if center_lat >= lat_median and center_lon >= lon_median:\n",
    "                sector = 'NE'\n",
    "            elif center_lat >= lat_median and center_lon < lon_median:\n",
    "                sector = 'NW'\n",
    "            elif center_lat < lat_median and center_lon >= lon_median:\n",
    "                sector = 'SE'\n",
    "            else:\n",
    "                sector = 'SW'\n",
    "            \n",
    "            sectors[sector].append(cluster_id)\n",
    "        \n",
    "        return sectors\n",
    "    \n",
    "    def _merge_clusters_by_sector(self, cluster_dict, delivery_points, use_sectors=True, verbose=True):\n",
    "        \"\"\"\n",
    "        CRITERIO V5: Merge cluster secondo la regola somma < 450 per ogni giorno.\n",
    "        Prima priorit√† ai cluster piccoli, poi agli altri.\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"üîó Tentativo di merge cluster tra settori\" if use_sectors else \"üîó Merge globale\")\n",
    "        \n",
    "        if use_sectors:\n",
    "            sectors = self._divide_space_into_sectors(delivery_points)\n",
    "        else:\n",
    "            sectors = {'ALL': list(cluster_dict.keys())}\n",
    "        \n",
    "        merged_clusters = {}\n",
    "        merged_pairs = set()\n",
    "        clusters_for_recalc = {}\n",
    "        \n",
    "        small_clusters = set([k for k, v in cluster_dict.items() if len(v) < self.min_cluster_size])\n",
    "        \n",
    "        for sector, cluster_list in sectors.items():\n",
    "            avail = [c for c in cluster_list if c in cluster_dict and c not in merged_pairs]\n",
    "            sc = [c for c in avail if c in small_clusters]\n",
    "            nc = [c for c in avail if c not in small_clusters]\n",
    "            \n",
    "            # 1. Merge cluster piccoli con altri\n",
    "            for s in sc:\n",
    "                if s in merged_pairs:\n",
    "                    continue\n",
    "                for n in nc:\n",
    "                    if n in merged_pairs:\n",
    "                        continue\n",
    "                    \n",
    "                    if self._can_merge_clusters(s, n):\n",
    "                        merged_id = f\"m_{s}_{n}\"\n",
    "                        merged_clusters[merged_id] = cluster_dict[s] + cluster_dict[n]\n",
    "                        clusters_for_recalc[merged_id] = merged_clusters[merged_id]\n",
    "                        merged_pairs.update([s, n])\n",
    "                        \n",
    "                        if verbose:\n",
    "                            print(f\" üîó Merge {s} + {n} ‚Üí {merged_id}\")\n",
    "                        break\n",
    "            \n",
    "            # 2. Merge tra cluster normali\n",
    "            avail_norm = [c for c in nc if c not in merged_pairs]\n",
    "            for i, c1 in enumerate(avail_norm):\n",
    "                if c1 in merged_pairs:\n",
    "                    continue\n",
    "                for c2 in avail_norm[i+1:]:\n",
    "                    if c2 in merged_pairs:\n",
    "                        continue\n",
    "                    \n",
    "                    if self._can_merge_clusters(c1, c2):\n",
    "                        merged_id = f\"m_{c1}_{c2}\"\n",
    "                        merged_clusters[merged_id] = cluster_dict[c1] + cluster_dict[c2]\n",
    "                        clusters_for_recalc[merged_id] = merged_clusters[merged_id]\n",
    "                        merged_pairs.update([c1, c2])\n",
    "                        \n",
    "                        if verbose:\n",
    "                            print(f\" üîó Merge {c1} + {c2} ‚Üí {merged_id}\")\n",
    "                        break\n",
    "        \n",
    "        # Tieni quelli non mergiati\n",
    "        for k, v in cluster_dict.items():\n",
    "            if k not in merged_pairs:\n",
    "                merged_clusters[k] = v\n",
    "        \n",
    "        # Ricalcola performance solo per i cluster mergiati\n",
    "        if clusters_for_recalc:\n",
    "            if verbose:\n",
    "                print(f\" üîÅ Ricalcolo routing per {len(clusters_for_recalc)} nuovi cluster merged\")\n",
    "            self._compute_batch_performances(clusters_for_recalc, verbose=False)\n",
    "        \n",
    "        return merged_clusters\n",
    "    \n",
    "    def run_adaptive_clustering(self, initial_k=50, verbose=True):\n",
    "        \"\"\"\n",
    "        Esegue il ciclo completo: K-means, split, save, merge.\n",
    "        STOP: max iterazioni, timeout, o 10 iterazioni senza miglioramenti.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print(f\"üéØ START: AdaptivePerformanceClusteringAggregated\")\n",
    "        print(f\"   ‚Ä¢ Gruppi: {len(self.aggregated_points_df)}\")\n",
    "        print(f\"   ‚Ä¢ Punti reali totali: {sum(self.group_sizes.values())}\")\n",
    "        print(f\"   ‚Ä¢ K iniziale: {initial_k}\")\n",
    "        \n",
    "        # K-means sui CENTROIDI\n",
    "        scaler = StandardScaler()\n",
    "        points_scaled = scaler.fit_transform(self.aggregated_points_df[['lat', 'lon']])\n",
    "        kmeans = KMeans(n_clusters=initial_k, init='k-means++', n_init=1, random_state=42)\n",
    "        labels = kmeans.fit_predict(points_scaled)\n",
    "        self.aggregated_points_df['cluster'] = labels\n",
    "        \n",
    "        # Crea dizionario cluster (con centroidi)\n",
    "        cluster_dict = {}\n",
    "        for c in range(initial_k):\n",
    "            locations = self.aggregated_points_df.loc[\n",
    "                self.aggregated_points_df['cluster'] == c, 'location_id'\n",
    "            ].tolist()\n",
    "            if locations:\n",
    "                cluster_dict[c] = locations\n",
    "        \n",
    "        print(f\"‚úÖ K-means completato: {len(cluster_dict)} cluster iniziali\")\n",
    "        \n",
    "        # Calcola performance iniziali\n",
    "        self._compute_batch_performances(cluster_dict, verbose=verbose)\n",
    "        \n",
    "        no_improv = 0\n",
    "        best_remaining = len(cluster_dict)\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            elapsed = (time.time() - start_time) / 60\n",
    "            \n",
    "            if elapsed > self.max_execution_time_min:\n",
    "                print(f\"‚è∞ STOP: superato tempo massimo ({self.max_execution_time_min} minuti)\")\n",
    "                break\n",
    "            \n",
    "            print(f\"\\nüîÑ Iterazione {iteration+1}/{self.max_iterations} ({round(elapsed, 2)} min)\")\n",
    "            \n",
    "            # 1. Split cluster fuori soglia\n",
    "            cluster_dict = self._split_oversized_clusters(cluster_dict, verbose=verbose)\n",
    "            \n",
    "            # 2. Salva cluster buoni\n",
    "            cluster_dict = self._save_good_clusters(cluster_dict, verbose=verbose)\n",
    "            \n",
    "            # 3. Merge settoriale\n",
    "            cluster_dict = self._merge_clusters_by_sector(\n",
    "                cluster_dict, self.aggregated_points_df, use_sectors=True, verbose=verbose\n",
    "            )\n",
    "            \n",
    "            # 4. Merge globale (se necessario)\n",
    "            merged2 = self._merge_clusters_by_sector(\n",
    "                cluster_dict, self.aggregated_points_df, use_sectors=False, verbose=verbose\n",
    "            )\n",
    "            if len(merged2) < len(cluster_dict):\n",
    "                cluster_dict = merged2\n",
    "            \n",
    "            # Controlla miglioramenti\n",
    "            remaining = len(cluster_dict)\n",
    "            print(f\" ‚ÑπÔ∏è Cluster ancora da processare: {remaining}\")\n",
    "            \n",
    "            if remaining < best_remaining:\n",
    "                best_remaining = remaining\n",
    "                no_improv = 0\n",
    "            else:\n",
    "                no_improv += 1\n",
    "                if no_improv >= 10:\n",
    "                    print(\"üü° STOP: 10 iterazioni senza miglioramento\")\n",
    "                    break\n",
    "            \n",
    "            if not cluster_dict:\n",
    "                print(\"‚úÖ STOP: tutti i cluster sono buoni\")\n",
    "                break\n",
    "            \n",
    "            # Aggiorna mapping cluster\n",
    "            mapping = {loc: cid for cid, locs in cluster_dict.items() for loc in locs}\n",
    "            self.aggregated_points_df['cluster'] = self.aggregated_points_df['location_id'].map(mapping)\n",
    "        \n",
    "        # Conclusione\n",
    "        print(f\"\\nüèÅ Concluso. Cluster finali: {len(self.final_clusters)}\")\n",
    "        \n",
    "        # Aggiungi cluster rimanenti non accettati\n",
    "        print(f\"üìä Sono rimasti {len(cluster_dict)} cluster non accettati\")\n",
    "        for cid, locs in cluster_dict.items():\n",
    "            final_id = len(self.final_clusters) + 1\n",
    "            self.final_clusters[final_id] = locs\n",
    "        \n",
    "        # ESPANDI tutti i cluster finali da centroidi a punti reali\n",
    "        print(f\"üìä Espansione {len(self.final_clusters)} cluster da centroidi a punti reali...\")\n",
    "        final_clusters_expanded = {}\n",
    "        for cluster_id, centroid_ids in self.final_clusters.items():\n",
    "            expanded = expand_cluster_with_groups(centroid_ids, self.group_mapping)\n",
    "            final_clusters_expanded[cluster_id] = expanded\n",
    "        \n",
    "        sizes = [len(v) for v in final_clusters_expanded.values()]\n",
    "        if sizes:\n",
    "            print(f\"üìè Distribuzione cluster (punti reali):\")\n",
    "            print(f\" - min: {min(sizes)} max: {max(sizes)} media: {np.mean(sizes):.1f}\")\n",
    "            print(f\" - totale punti: {sum(sizes)}\")\n",
    "        \n",
    "        # Calcola performance finali sui punti reali\n",
    "        print(f\"üìä Calcolo performance finali sui punti reali...\")\n",
    "        perf_df = pc.calc_clusters_stats_ON(\n",
    "            list(final_clusters_expanded.values()),\n",
    "            time_limit=3,\n",
    "            parallel=True,\n",
    "            max_workers=self.n_cores,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        return final_clusters_expanded, perf_df\n",
    "\n",
    "\n",
    "def run_adaptive_performance_clustering_aggregated(\n",
    "    aggregated_groups, \n",
    "    delivery_points, \n",
    "    initial_k=50, \n",
    "    max_iterations=15, \n",
    "    n_cores=None, \n",
    "    max_execution_time_min=500\n",
    "):\n",
    "    \"\"\"\n",
    "    Funzione wrapper per eseguire l'algoritmo con gruppi pre-aggregati.\n",
    "    \"\"\"\n",
    "    clusterer = AdaptivePerformanceClusteringAggregated(\n",
    "        aggregated_groups=aggregated_groups,\n",
    "        delivery_points=delivery_points,\n",
    "        max_iterations=max_iterations,\n",
    "        max_execution_time_min=max_execution_time_min,\n",
    "        n_cores=n_cores\n",
    "    )\n",
    "    \n",
    "    return clusterer.run_adaptive_clustering(\n",
    "        initial_k=initial_k,\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Esecuzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esegui il clustering adattivo con gruppi pre-aggregati\n",
    "final_clusters, performance_df = run_adaptive_performance_clustering_aggregated(\n",
    "    aggregated_groups=aggregated_groups,\n",
    "    delivery_points=pc.delivery_points_ON,\n",
    "    initial_k=50,\n",
    "    max_iterations=100,\n",
    "    n_cores=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Salvataggio risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva i risultati\n",
    "performance_df.to_csv('clustering_methods_performances/k-means_euristics_aggregated_ON_v5.csv')\n",
    "\n",
    "with open('cluster_dicts/cluster_dict_k-means_euristics_aggregated_ON_v5.pkl', 'wb') as f:\n",
    "    pickle.dump(final_clusters, f)\n",
    "\n",
    "print(\"‚úÖ Risultati salvati con successo!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

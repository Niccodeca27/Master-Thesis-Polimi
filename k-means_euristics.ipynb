{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Versione 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Split: max(mean_minutes) > 450 e almeno 5 punti, split in ceil(max/450) parti (nome \"s_x_y\").\n",
    "\n",
    "Cluster buoni: max(mean_minutes) tra 350 e 450 incluso, vengono salvati.\n",
    "\n",
    "Merge: merge solo se somma per ogni giorno di mean_minutes < 450 (nome \"m_x_y\").\n",
    "\n",
    "Cluster troppo piccoli: quelli con 1 o 2 punti (min_cluster_size=3), sempre mergeabili.\n",
    "\n",
    "Routing: ricalcolato solo per cluster effettivamente cambiati (split/merge).\n",
    "\n",
    "Stop: max iterazioni, tempo massimo, oppure 3 iterazioni senza miglioramenti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "from itertools import combinations\n",
    "\n",
    "import import_ipynb\n",
    "import performance_calc as pc\n",
    "\n",
    "class AdaptivePerformanceClustering:\n",
    "    def __init__(self, \n",
    "                 n_cores: int = None,\n",
    "                 cache_dir: str = \"./cluster_cache\",\n",
    "                 max_iterations: int = 15,\n",
    "                 max_execution_time_min: int = 500):\n",
    "        # Inizializzazione parametri e strutture dati\n",
    "        self.n_cores = n_cores or max(1, mp.cpu_count() - 1)\n",
    "        self.cache_dir = cache_dir\n",
    "        self.max_iterations = max_iterations\n",
    "        self.max_execution_time_min = max_execution_time_min\n",
    "        self.final_clusters = {}\n",
    "        self.cluster_performances = {}\n",
    "        self.min_cluster_size = 3 # i troppo piccoli sono quelli con 1 o 2 punti\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        print(f\"üöÄ AdaptivePerformanceClustering avviato con {self.n_cores} core e timeout {max_execution_time_min} min\")\n",
    "    \n",
    "    def _cache_key(self, location_ids):\n",
    "        return hash(tuple(sorted(location_ids)))\n",
    "    \n",
    "    def _compute_batch_performances(self, cluster_dict, verbose=True):\n",
    "        \"\"\"\n",
    "        Calcola le performance di routing per tutti i cluster usando pi√π thread,\n",
    "        e salva tutto in self.cluster_performances.\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"üîé Calcolo performance in batch ({len(cluster_dict)} cluster)...\")\n",
    "        valid_clusters = { k:v for k,v in cluster_dict.items() if len(v) >= self.min_cluster_size }\n",
    "        small_clusters = { k:v for k,v in cluster_dict.items() if len(v) < self.min_cluster_size }\n",
    "        clusters_list = list(valid_clusters.values())\n",
    "        cluster_ids = list(valid_clusters.keys())\n",
    "        # Chiamata batch parallela\n",
    "        if valid_clusters:\n",
    "            performance_df = pc.calc_clusters_stats(\n",
    "                clusters=clusters_list,\n",
    "                time_limit=3,\n",
    "                parallel=True,\n",
    "                max_workers=self.n_cores,\n",
    "                verbose=False\n",
    "            )\n",
    "            for i, cluster_id in enumerate(cluster_ids):\n",
    "                name = f'Cluster {i+1}'\n",
    "                cluster_data = performance_df[performance_df['cluster'] == name]\n",
    "                if not cluster_data.empty:\n",
    "                    # Calcolo il massimo dei mean_minutes su tutti i giorni della settimana\n",
    "                    max_mean = cluster_data['mean_minutes'].max()\n",
    "                    self.cluster_performances[cluster_id] = {\n",
    "                        'dataframe': cluster_data,\n",
    "                        'max_mean_minutes': max_mean,\n",
    "                        'is_valid': True\n",
    "                    }\n",
    "                else:\n",
    "                    self.cluster_performances[cluster_id] = {\n",
    "                        'dataframe': None,\n",
    "                        'max_mean_minutes': float('inf'),\n",
    "                        'is_valid': False\n",
    "                    }\n",
    "        # Gestione cluster troppo piccoli\n",
    "        for cluster_id in small_clusters.keys():\n",
    "            self.cluster_performances[cluster_id] = {\n",
    "                'dataframe': None,\n",
    "                'max_mean_minutes': 0,\n",
    "                'is_valid': True,   # true cos√¨ non blocca i merge \n",
    "                'too_small': True\n",
    "            }\n",
    "        if verbose:\n",
    "            print(f\"    ‚úÖ Performance salvate in self.cluster_performances\")\n",
    "        return\n",
    "    \n",
    "    def _save_good_clusters(self, cluster_dict, verbose=True):\n",
    "        \"\"\"\n",
    "        Trova i cluster PERFETTI: solo se il valore massimo dei mean_minutes su tutti i giorni della settimana\n",
    "        √® COMPRESO tra 350 e 450 (inclusi). Aggiunge questi cluster a quelli finali e li rimuove dai temporanei.\n",
    "        \"\"\"\n",
    "        saved_count = 0\n",
    "        remaining_clusters = {}\n",
    "        for cluster_id, location_ids in cluster_dict.items():\n",
    "            perf = self.cluster_performances.get(cluster_id)\n",
    "            # Considera solo cluster che NON sono troppo piccoli\n",
    "            if len(location_ids) < self.min_cluster_size:\n",
    "                remaining_clusters[cluster_id] = location_ids\n",
    "                continue\n",
    "            if perf and perf['is_valid']:\n",
    "                max_mean = perf['max_mean_minutes']\n",
    "                # criterio: il pi√π grande dei mean_minutes per quel cluster e settimana\n",
    "                if 350 <= max_mean <= 450:\n",
    "                    # Salva come cluster perfetto\n",
    "                    final_id = len(self.final_clusters)+1\n",
    "                    self.final_clusters[final_id] = location_ids\n",
    "                    saved_count += 1\n",
    "                    if verbose:\n",
    "                        print(f\"  ‚úÖ Accettato cluster {cluster_id}: max(mean_minutes)={max_mean:.1f} min, punti={len(location_ids)}\")\n",
    "                    continue\n",
    "            # Se non perfetto, tienilo per round successivo\n",
    "            remaining_clusters[cluster_id] = location_ids\n",
    "        if verbose:\n",
    "            print(f\"  üèÅ Salvati {saved_count} cluster ideali ({350}‚â§max‚â§450 min)\")\n",
    "        return remaining_clusters\n",
    "\n",
    "    def _split_oversized_clusters(self, cluster_dict, verbose=True):\n",
    "        \"\"\"\n",
    "        Divide ogni cluster che ha max dei mean_minutes > 450 in N cluster,\n",
    "        dove N = ceil(max_mean_minutes / 450). (solo se pi√π di 4 punti)\n",
    "        \"\"\"\n",
    "        new_clusters = {}\n",
    "        clusters_to_recalc = {}\n",
    "        for cluster_id, location_ids in cluster_dict.items():\n",
    "            perf = self.cluster_performances.get(cluster_id)\n",
    "            max_mean = perf['max_mean_minutes']\n",
    "            size = len(location_ids)\n",
    "            # Split solo se supera la soglia e se pi√π di 4 punti\n",
    "            if size > 4 and max_mean > 450:\n",
    "                n_splits = int(np.ceil(max_mean / 450))\n",
    "                if verbose:\n",
    "                    print(f\"  ‚úÇÔ∏è Cluster {cluster_id}: {size} punti, max(mean_minutes)={max_mean:.1f} split in {n_splits}\")\n",
    "                chunk_size = int(np.ceil(size / n_splits))\n",
    "                for i in range(n_splits):\n",
    "                    start = i * chunk_size\n",
    "                    end = min(start + chunk_size, size)\n",
    "                    chunk = location_ids[start:end]\n",
    "                    new_id = f\"s_{cluster_id}_{i+1}\"\n",
    "                    new_clusters[new_id] = chunk\n",
    "                    clusters_to_recalc[new_id] = chunk\n",
    "            else:\n",
    "                new_clusters[cluster_id] = location_ids\n",
    "        # Ricalcola solo per i nuovi cluster splittati\n",
    "        if clusters_to_recalc:\n",
    "            if verbose:\n",
    "                print(f\"    üîÑ Ricalcolo routing per {len(clusters_to_recalc)} nuovi cluster splittati\")\n",
    "            self._compute_batch_performances(clusters_to_recalc, verbose=False)\n",
    "        return new_clusters\n",
    "\n",
    "    def _can_merge_clusters(self, cluster_id1, cluster_id2):\n",
    "        \"\"\"\n",
    "        Unisce solo se la somma dei mean_minutes per OGNI giorno della settimana √® <450.\n",
    "        Cluster troppo piccoli: merge sempre permesso.\n",
    "        \"\"\"\n",
    "        perf1 = self.cluster_performances.get(cluster_id1)\n",
    "        perf2 = self.cluster_performances.get(cluster_id2)\n",
    "        if (perf1 and perf1.get('too_small')) or (perf2 and perf2.get('too_small')):\n",
    "            return True\n",
    "        if not (perf1 and perf2 and perf1['is_valid'] and perf2['is_valid']):\n",
    "            return False\n",
    "        df1, df2 = perf1['dataframe'], perf2['dataframe']\n",
    "        days1, days2 = set(df1['weekday']), set(df2['weekday'])\n",
    "        common_days = days1 & days2\n",
    "        if not common_days:\n",
    "            return False\n",
    "        for day in common_days:\n",
    "            m1 = df1[df1['weekday'] == day]['mean_minutes'].iloc[0]\n",
    "            m2 = df2[df2['weekday'] == day]['mean_minutes'].iloc[0]\n",
    "            if m1 + m2 >= 450:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def _divide_space_into_sectors(self, delivery_points):\n",
    "        \"\"\"\n",
    "        Divide lo spazio geografico in 4 settori (NE, NW, SE, SW) usando le mediane\n",
    "        di latitudine e longitudine. Questo aiuta a mergeare cluster geograficamente vicini.\n",
    "        \"\"\"\n",
    "        lat_median = delivery_points['lat'].median()\n",
    "        lon_median = delivery_points['lon'].median()\n",
    "        \n",
    "        sectors = {'NE': [], 'NW': [], 'SE': [], 'SW': []}\n",
    "        \n",
    "        # Per ogni cluster trova il centroide e assegnalo al settore corrispondente\n",
    "        for cluster_id in delivery_points['cluster'].unique():\n",
    "            cluster_points = delivery_points[delivery_points['cluster'] == cluster_id]\n",
    "            if len(cluster_points) == 0:\n",
    "                continue\n",
    "                \n",
    "            center_lat = cluster_points['lat'].mean()\n",
    "            center_lon = cluster_points['lon'].mean()\n",
    "            \n",
    "            # Determina il settore basato su mediane\n",
    "            if center_lat >= lat_median and center_lon >= lon_median:\n",
    "                sector = 'NE'\n",
    "            elif center_lat >= lat_median and center_lon < lon_median:\n",
    "                sector = 'NW' \n",
    "            elif center_lat < lat_median and center_lon >= lon_median:\n",
    "                sector = 'SE'\n",
    "            else:\n",
    "                sector = 'SW'\n",
    "                \n",
    "            sectors[sector].append(cluster_id)\n",
    "        \n",
    "        return sectors\n",
    "\n",
    "\n",
    "    def _merge_clusters_by_sector(self, cluster_dict, delivery_points, use_sectors=True, verbose=True):\n",
    "        \"\"\"\n",
    "        Tenta merge di cluster piccoli e poi merge compatibili sempre secondo criterio < 450 min/giorno.\n",
    "        Crea i nuovi id 'm_x_y'. Ricalcola il routing solo per i merge eseguiti.\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"üîó Tentativo di merge cluster tra settori\" if use_sectors else \"üîó Merge globale\")\n",
    "\n",
    "        if use_sectors:\n",
    "            sectors = self._divide_space_into_sectors(delivery_points)\n",
    "        else:\n",
    "            sectors = {'ALL': list(cluster_dict.keys())}\n",
    "        merged_clusters, merged_pairs = {}, set()\n",
    "        clusters_for_recalc = {}\n",
    "\n",
    "        small_clusters = set([k for k, v in cluster_dict.items() if len(v) < self.min_cluster_size])\n",
    "        for sector, cluster_list in sectors.items():\n",
    "            avail = [c for c in cluster_list if c in cluster_dict and c not in merged_pairs]\n",
    "            sc = [c for c in avail if c in small_clusters]\n",
    "            nc = [c for c in avail if c not in small_clusters]\n",
    "            # 1. Merge cluster piccoli con altri\n",
    "            for s in sc:\n",
    "                if s in merged_pairs: continue\n",
    "                for n in nc:\n",
    "                    if n in merged_pairs: continue\n",
    "                    merged_id = f\"m_{s}_{n}\"\n",
    "                    can_merge = self._can_merge_clusters(s, n)\n",
    "                    if can_merge:\n",
    "                        merged_clusters[merged_id] = cluster_dict[s] + cluster_dict[n]\n",
    "                        clusters_for_recalc[merged_id] = merged_clusters[merged_id]\n",
    "                        merged_pairs.update([s, n])\n",
    "                        if verbose: print(f\"  üîó Merge {s} + {n} ‚ûî {merged_id}\")\n",
    "                        break\n",
    "            # 2. Merge tra cluster normali\n",
    "            avail_norm = [c for c in nc if c not in merged_pairs]\n",
    "            for i, c1 in enumerate(avail_norm):\n",
    "                if c1 in merged_pairs: continue\n",
    "                for c2 in avail_norm[i+1:]:\n",
    "                    if c2 in merged_pairs: continue\n",
    "                    merged_id = f\"m_{c1}_{c2}\"\n",
    "                    if self._can_merge_clusters(c1, c2):\n",
    "                        merged_clusters[merged_id] = cluster_dict[c1] + cluster_dict[c2]\n",
    "                        clusters_for_recalc[merged_id] = merged_clusters[merged_id]\n",
    "                        merged_pairs.update([c1,c2])\n",
    "                        if verbose: print(f\"  üîó Merge {c1} + {c2} ‚ûî {merged_id}\")\n",
    "                        break\n",
    "        # Tieni quelli che non sono stati uniti\n",
    "        for k, v in cluster_dict.items():\n",
    "            if k not in merged_pairs:\n",
    "                merged_clusters[k] = v\n",
    "        # Ricalcola solo per i nuovi mergiati\n",
    "        if clusters_for_recalc:\n",
    "            if verbose: print(f\"    üîÅ Ricalcolo routing per {len(clusters_for_recalc)} nuovi cluster merged\")\n",
    "            self._compute_batch_performances(clusters_for_recalc, verbose=False)\n",
    "        return merged_clusters\n",
    "\n",
    "    def run_adaptive_clustering(self, delivery_points: pd.DataFrame, initial_k=50, verbose=True):\n",
    "        \"\"\"\n",
    "        Esegue il ciclo: k-means, split dove max(mean_minutes)>450, merge <450, salva cluster buoni\n",
    "        STOP: max iterazioni, timeout, 3 iter senza miglioramenti.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        print(f\"üéØ START: AdaptivePerformanceClustering su {len(delivery_points)} punti (k={initial_k})\")\n",
    "        scaler = StandardScaler()\n",
    "        points_scaled = scaler.fit_transform(delivery_points[['lat', 'lon']])\n",
    "        kmeans = KMeans(n_clusters=initial_k, init='k-means++', n_init=1, random_state=42)\n",
    "        labels = kmeans.fit_predict(points_scaled)\n",
    "        delivery_points['cluster'] = labels\n",
    "\n",
    "        # Dizionario iniziale cluster (location_id)\n",
    "        cluster_dict = {}\n",
    "        for c in range(initial_k):\n",
    "            locations = delivery_points.loc[delivery_points['cluster'] == c, 'location_id'].tolist()\n",
    "            if locations:\n",
    "                cluster_dict[c] = locations\n",
    "\n",
    "        self._compute_batch_performances(cluster_dict, verbose=verbose)\n",
    "        no_improv = 0\n",
    "        best_remaining = len(cluster_dict)\n",
    "        for iteration in range(self.max_iterations):\n",
    "            elapsed = (time.time() - start_time) / 60\n",
    "            if elapsed > self.max_execution_time_min:\n",
    "                print(f\"‚è∞ STOP: superato tempo massimo ({self.max_execution_time_min} minuti)\")\n",
    "                break\n",
    "            print(f\"\\nüîÑ Iterazione {iteration+1}/{self.max_iterations} ({round(elapsed,2)} min)\")\n",
    "\n",
    "            # 1. Split cluster fuori soglia\n",
    "            cluster_dict = self._split_oversized_clusters(cluster_dict, verbose=verbose)\n",
    "            # 2. Salvare cluster \"buoni\" (finali)\n",
    "            cluster_dict = self._save_good_clusters(cluster_dict, verbose=verbose)\n",
    "            # 3. Merge settoriale\n",
    "            cluster_dict = self._merge_clusters_by_sector(cluster_dict, delivery_points, use_sectors=True, verbose=verbose)\n",
    "            # 4. Merge globale (se necessario)\n",
    "            merged2 = self._merge_clusters_by_sector(cluster_dict, delivery_points, use_sectors=False, verbose=verbose)\n",
    "            if len(merged2) < len(cluster_dict):\n",
    "                cluster_dict = merged2\n",
    "\n",
    "            # Controlla se la soluzione √® migliorata\n",
    "            remaining = len(cluster_dict)\n",
    "            print(f\"   ‚ÑπÔ∏è  Cluster ancora da processare: {remaining}\")\n",
    "            if remaining < best_remaining:\n",
    "                best_remaining = remaining\n",
    "                no_improv = 0\n",
    "            else:\n",
    "                no_improv += 1\n",
    "            if no_improv >= 10:\n",
    "                print(\"üü° STOP: 10 iterazioni senza miglioramento\")\n",
    "                break\n",
    "            if not cluster_dict:\n",
    "                print(\"‚úÖ STOP: tutti i cluster sono buoni\")\n",
    "                break\n",
    "\n",
    "            # Aggiorna cluster nel DataFrame\n",
    "            mapping = {loc: cid for cid, locs in cluster_dict.items() for loc in locs}\n",
    "            delivery_points['cluster'] = delivery_points['location_id'].map(mapping)\n",
    "\n",
    "        print(f\"\\nüèÅ Concluso. Cluster finali: {len(self.final_clusters)}\")\n",
    "        sizes = [len(v) for v in self.final_clusters.values()]\n",
    "        if sizes:\n",
    "            print(f\"   - min: {min(sizes)}  max: {max(sizes)}  media: {np.mean(sizes):.1f}\")\n",
    "\n",
    "        print(f\"üìä Sono rimasti {len(cluster_dict)} cluster non accettati, primo cluster non accettato con ID = {len(self.final_clusters)+1}\")\n",
    "        \n",
    "        # Inclusione nell'output anche dei cluster non accettati rimasti\n",
    "        for cid, locs in cluster_dict.items():\n",
    "            final_id = len(self.final_clusters) + 1\n",
    "            self.final_clusters[final_id] = locs\n",
    "\n",
    "        # Calcola performance finali\n",
    "        perf_df = pc.calc_clusters_stats(list(self.final_clusters.values()), time_limit=3, parallel=True, max_workers=self.n_cores, verbose=False)\n",
    "        return self.final_clusters, perf_df\n",
    "\n",
    "def run_adaptive_performance_clustering(delivery_points, initial_k=50, max_iterations=15, n_cores=None, max_execution_time_min=500):\n",
    "    clusterer = AdaptivePerformanceClustering(\n",
    "        max_iterations=max_iterations,\n",
    "        max_execution_time_min=max_execution_time_min,\n",
    "        n_cores=n_cores\n",
    "    )\n",
    "    return clusterer.run_adaptive_clustering(\n",
    "        delivery_points=delivery_points,\n",
    "        initial_k=initial_k,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "final_clusters, performance_df = run_adaptive_performance_clustering(\n",
    "    delivery_points=pc.delivery_points,\n",
    "    initial_k=50,\n",
    "    max_iterations=100)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Tempo di esecuzione algoritmo: {(end - start)/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df.to_csv('clustering_methods_performances/k-means_euristics_5.csv')\n",
    "\n",
    "with open('cluster_dicts/cluster_dict_k-means_euristics_5.pkl', 'wb') as f:\n",
    "    pickle.dump(final_clusters, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff620e",
   "metadata": {},
   "source": [
    "# run AS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69efe209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "from itertools import combinations\n",
    "\n",
    "import import_ipynb\n",
    "import performance_calc as pc\n",
    "\n",
    "class AdaptivePerformanceClustering:\n",
    "    def __init__(self, \n",
    "                 n_cores: int = None,\n",
    "                 cache_dir: str = \"./cluster_cache\",\n",
    "                 max_iterations: int = 15,\n",
    "                 max_execution_time_min: int = 500):\n",
    "        # Inizializzazione parametri e strutture dati\n",
    "        self.n_cores = n_cores or max(1, mp.cpu_count() - 1)\n",
    "        self.cache_dir = cache_dir\n",
    "        self.max_iterations = max_iterations\n",
    "        self.max_execution_time_min = max_execution_time_min\n",
    "        self.final_clusters = {}\n",
    "        self.cluster_performances = {}\n",
    "        self.min_cluster_size = 3 # i troppo piccoli sono quelli con 1 o 2 punti\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        print(f\"üöÄ AdaptivePerformanceClustering avviato con {self.n_cores} core e timeout {max_execution_time_min} min\")\n",
    "    \n",
    "    def _cache_key(self, location_ids):\n",
    "        return hash(tuple(sorted(location_ids)))\n",
    "    \n",
    "    def _compute_batch_performances(self, cluster_dict, verbose=True):\n",
    "        \"\"\"\n",
    "        Calcola le performance di routing per tutti i cluster usando pi√π thread,\n",
    "        e salva tutto in self.cluster_performances.\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"üîé Calcolo performance in batch ({len(cluster_dict)} cluster)...\")\n",
    "        valid_clusters = { k:v for k,v in cluster_dict.items() if len(v) >= self.min_cluster_size }\n",
    "        small_clusters = { k:v for k,v in cluster_dict.items() if len(v) < self.min_cluster_size }\n",
    "        clusters_list = list(valid_clusters.values())\n",
    "        cluster_ids = list(valid_clusters.keys())\n",
    "        # Chiamata batch parallela\n",
    "        if valid_clusters:\n",
    "            performance_df = pc.calc_clusters_stats_AS(\n",
    "                clusters=clusters_list,\n",
    "                time_limit=3,\n",
    "                parallel=True,\n",
    "                max_workers=self.n_cores,\n",
    "                verbose=False\n",
    "            )\n",
    "            for i, cluster_id in enumerate(cluster_ids):\n",
    "                name = f'Cluster {i+1}'\n",
    "                cluster_data = performance_df[performance_df['cluster'] == name]\n",
    "                if not cluster_data.empty:\n",
    "                    # Calcolo il massimo dei mean_minutes su tutti i giorni della settimana\n",
    "                    max_mean = cluster_data['mean_minutes'].max()\n",
    "                    self.cluster_performances[cluster_id] = {\n",
    "                        'dataframe': cluster_data,\n",
    "                        'max_mean_minutes': max_mean,\n",
    "                        'is_valid': True\n",
    "                    }\n",
    "                else:\n",
    "                    self.cluster_performances[cluster_id] = {\n",
    "                        'dataframe': None,\n",
    "                        'max_mean_minutes': float('inf'),\n",
    "                        'is_valid': False\n",
    "                    }\n",
    "        # Gestione cluster troppo piccoli\n",
    "        for cluster_id in small_clusters.keys():\n",
    "            self.cluster_performances[cluster_id] = {\n",
    "                'dataframe': None,\n",
    "                'max_mean_minutes': 0,\n",
    "                'is_valid': True,   # true cos√¨ non blocca i merge \n",
    "                'too_small': True\n",
    "            }\n",
    "        if verbose:\n",
    "            print(f\"    ‚úÖ Performance salvate in self.cluster_performances\")\n",
    "        return\n",
    "    \n",
    "    def _save_good_clusters(self, cluster_dict, verbose=True):\n",
    "        \"\"\"\n",
    "        Trova i cluster PERFETTI: solo se il valore massimo dei mean_minutes su tutti i giorni della settimana\n",
    "        √® COMPRESO tra 350 e 450 (inclusi). Aggiunge questi cluster a quelli finali e li rimuove dai temporanei.\n",
    "        \"\"\"\n",
    "        saved_count = 0\n",
    "        remaining_clusters = {}\n",
    "        for cluster_id, location_ids in cluster_dict.items():\n",
    "            perf = self.cluster_performances.get(cluster_id)\n",
    "            # Considera solo cluster che NON sono troppo piccoli\n",
    "            if len(location_ids) < self.min_cluster_size:\n",
    "                remaining_clusters[cluster_id] = location_ids\n",
    "                continue\n",
    "            if perf and perf['is_valid']:\n",
    "                max_mean = perf['max_mean_minutes']\n",
    "                # criterio: il pi√π grande dei mean_minutes per quel cluster e settimana\n",
    "                if 350 <= max_mean <= 450:\n",
    "                    # Salva come cluster perfetto\n",
    "                    final_id = len(self.final_clusters)+1\n",
    "                    self.final_clusters[final_id] = location_ids\n",
    "                    saved_count += 1\n",
    "                    if verbose:\n",
    "                        print(f\"  ‚úÖ Accettato cluster {cluster_id}: max(mean_minutes)={max_mean:.1f} min, punti={len(location_ids)}\")\n",
    "                    continue\n",
    "            # Se non perfetto, tienilo per round successivo\n",
    "            remaining_clusters[cluster_id] = location_ids\n",
    "        if verbose:\n",
    "            print(f\"  üèÅ Salvati {saved_count} cluster ideali ({350}‚â§max‚â§450 min)\")\n",
    "        return remaining_clusters\n",
    "\n",
    "    def _split_oversized_clusters(self, cluster_dict, verbose=True):\n",
    "        \"\"\"\n",
    "        Divide ogni cluster che ha max dei mean_minutes > 450 in N cluster,\n",
    "        dove N = ceil(max_mean_minutes / 450). (solo se pi√π di 4 punti)\n",
    "        \"\"\"\n",
    "        new_clusters = {}\n",
    "        clusters_to_recalc = {}\n",
    "        for cluster_id, location_ids in cluster_dict.items():\n",
    "            perf = self.cluster_performances.get(cluster_id)\n",
    "            max_mean = perf['max_mean_minutes']\n",
    "            size = len(location_ids)\n",
    "            # Split solo se supera la soglia e se pi√π di 4 punti\n",
    "            if size > 4 and max_mean > 450:\n",
    "                n_splits = int(np.ceil(max_mean / 450))\n",
    "                if verbose:\n",
    "                    print(f\"  ‚úÇÔ∏è Cluster {cluster_id}: {size} punti, max(mean_minutes)={max_mean:.1f} split in {n_splits}\")\n",
    "                chunk_size = int(np.ceil(size / n_splits))\n",
    "                for i in range(n_splits):\n",
    "                    start = i * chunk_size\n",
    "                    end = min(start + chunk_size, size)\n",
    "                    chunk = location_ids[start:end]\n",
    "                    new_id = f\"s_{cluster_id}_{i+1}\"\n",
    "                    new_clusters[new_id] = chunk\n",
    "                    clusters_to_recalc[new_id] = chunk\n",
    "            else:\n",
    "                new_clusters[cluster_id] = location_ids\n",
    "        # Ricalcola solo per i nuovi cluster splittati\n",
    "        if clusters_to_recalc:\n",
    "            if verbose:\n",
    "                print(f\"    üîÑ Ricalcolo routing per {len(clusters_to_recalc)} nuovi cluster splittati\")\n",
    "            self._compute_batch_performances(clusters_to_recalc, verbose=False)\n",
    "        return new_clusters\n",
    "\n",
    "    def _can_merge_clusters(self, cluster_id1, cluster_id2):\n",
    "        \"\"\"\n",
    "        Unisce solo se la somma dei mean_minutes per OGNI giorno della settimana √® <450.\n",
    "        Cluster troppo piccoli: merge sempre permesso.\n",
    "        \"\"\"\n",
    "        perf1 = self.cluster_performances.get(cluster_id1)\n",
    "        perf2 = self.cluster_performances.get(cluster_id2)\n",
    "        if (perf1 and perf1.get('too_small')) or (perf2 and perf2.get('too_small')):\n",
    "            return True\n",
    "        if not (perf1 and perf2 and perf1['is_valid'] and perf2['is_valid']):\n",
    "            return False\n",
    "        df1, df2 = perf1['dataframe'], perf2['dataframe']\n",
    "        days1, days2 = set(df1['weekday']), set(df2['weekday'])\n",
    "        common_days = days1 & days2\n",
    "        if not common_days:\n",
    "            return False\n",
    "        for day in common_days:\n",
    "            m1 = df1[df1['weekday'] == day]['mean_minutes'].iloc[0]\n",
    "            m2 = df2[df2['weekday'] == day]['mean_minutes'].iloc[0]\n",
    "            if m1 + m2 >= 450:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def _divide_space_into_sectors(self, delivery_points):\n",
    "        \"\"\"\n",
    "        Divide lo spazio geografico in 4 settori (NE, NW, SE, SW) usando le mediane\n",
    "        di latitudine e longitudine. Questo aiuta a mergeare cluster geograficamente vicini.\n",
    "        \"\"\"\n",
    "        lat_median = delivery_points['lat'].median()\n",
    "        lon_median = delivery_points['lon'].median()\n",
    "        \n",
    "        sectors = {'NE': [], 'NW': [], 'SE': [], 'SW': []}\n",
    "        \n",
    "        # Per ogni cluster trova il centroide e assegnalo al settore corrispondente\n",
    "        for cluster_id in delivery_points['cluster'].unique():\n",
    "            cluster_points = delivery_points[delivery_points['cluster'] == cluster_id]\n",
    "            if len(cluster_points) == 0:\n",
    "                continue\n",
    "                \n",
    "            center_lat = cluster_points['lat'].mean()\n",
    "            center_lon = cluster_points['lon'].mean()\n",
    "            \n",
    "            # Determina il settore basato su mediane\n",
    "            if center_lat >= lat_median and center_lon >= lon_median:\n",
    "                sector = 'NE'\n",
    "            elif center_lat >= lat_median and center_lon < lon_median:\n",
    "                sector = 'NW' \n",
    "            elif center_lat < lat_median and center_lon >= lon_median:\n",
    "                sector = 'SE'\n",
    "            else:\n",
    "                sector = 'SW'\n",
    "                \n",
    "            sectors[sector].append(cluster_id)\n",
    "        \n",
    "        return sectors\n",
    "\n",
    "\n",
    "    def _merge_clusters_by_sector(self, cluster_dict, delivery_points, use_sectors=True, verbose=True):\n",
    "        \"\"\"\n",
    "        Tenta merge di cluster piccoli e poi merge compatibili sempre secondo criterio < 450 min/giorno.\n",
    "        Crea i nuovi id 'm_x_y'. Ricalcola il routing solo per i merge eseguiti.\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"üîó Tentativo di merge cluster tra settori\" if use_sectors else \"üîó Merge globale\")\n",
    "\n",
    "        if use_sectors:\n",
    "            sectors = self._divide_space_into_sectors(delivery_points)\n",
    "        else:\n",
    "            sectors = {'ALL': list(cluster_dict.keys())}\n",
    "        merged_clusters, merged_pairs = {}, set()\n",
    "        clusters_for_recalc = {}\n",
    "\n",
    "        small_clusters = set([k for k, v in cluster_dict.items() if len(v) < self.min_cluster_size])\n",
    "        for sector, cluster_list in sectors.items():\n",
    "            avail = [c for c in cluster_list if c in cluster_dict and c not in merged_pairs]\n",
    "            sc = [c for c in avail if c in small_clusters]\n",
    "            nc = [c for c in avail if c not in small_clusters]\n",
    "            # 1. Merge cluster piccoli con altri\n",
    "            for s in sc:\n",
    "                if s in merged_pairs: continue\n",
    "                for n in nc:\n",
    "                    if n in merged_pairs: continue\n",
    "                    merged_id = f\"m_{s}_{n}\"\n",
    "                    can_merge = self._can_merge_clusters(s, n)\n",
    "                    if can_merge:\n",
    "                        merged_clusters[merged_id] = cluster_dict[s] + cluster_dict[n]\n",
    "                        clusters_for_recalc[merged_id] = merged_clusters[merged_id]\n",
    "                        merged_pairs.update([s, n])\n",
    "                        if verbose: print(f\"  üîó Merge {s} + {n} ‚ûî {merged_id}\")\n",
    "                        break\n",
    "            # 2. Merge tra cluster normali\n",
    "            avail_norm = [c for c in nc if c not in merged_pairs]\n",
    "            for i, c1 in enumerate(avail_norm):\n",
    "                if c1 in merged_pairs: continue\n",
    "                for c2 in avail_norm[i+1:]:\n",
    "                    if c2 in merged_pairs: continue\n",
    "                    merged_id = f\"m_{c1}_{c2}\"\n",
    "                    if self._can_merge_clusters(c1, c2):\n",
    "                        merged_clusters[merged_id] = cluster_dict[c1] + cluster_dict[c2]\n",
    "                        clusters_for_recalc[merged_id] = merged_clusters[merged_id]\n",
    "                        merged_pairs.update([c1,c2])\n",
    "                        if verbose: print(f\"  üîó Merge {c1} + {c2} ‚ûî {merged_id}\")\n",
    "                        break\n",
    "        # Tieni quelli che non sono stati uniti\n",
    "        for k, v in cluster_dict.items():\n",
    "            if k not in merged_pairs:\n",
    "                merged_clusters[k] = v\n",
    "        # Ricalcola solo per i nuovi mergiati\n",
    "        if clusters_for_recalc:\n",
    "            if verbose: print(f\"    üîÅ Ricalcolo routing per {len(clusters_for_recalc)} nuovi cluster merged\")\n",
    "            self._compute_batch_performances(clusters_for_recalc, verbose=False)\n",
    "        return merged_clusters\n",
    "\n",
    "    def run_adaptive_clustering(self, delivery_points: pd.DataFrame, initial_k=50, verbose=True):\n",
    "        \"\"\"\n",
    "        Esegue il ciclo: k-means, split dove max(mean_minutes)>450, merge <450, salva cluster buoni\n",
    "        STOP: max iterazioni, timeout, 3 iter senza miglioramenti.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        print(f\"üéØ START: AdaptivePerformanceClustering su {len(delivery_points)} punti (k={initial_k})\")\n",
    "        scaler = StandardScaler()\n",
    "        points_scaled = scaler.fit_transform(delivery_points[['lat', 'lon']])\n",
    "        kmeans = KMeans(n_clusters=initial_k, init='k-means++', n_init=1, random_state=42)\n",
    "        labels = kmeans.fit_predict(points_scaled)\n",
    "        delivery_points['cluster'] = labels\n",
    "\n",
    "        # Dizionario iniziale cluster (location_id)\n",
    "        cluster_dict = {}\n",
    "        for c in range(initial_k):\n",
    "            locations = delivery_points.loc[delivery_points['cluster'] == c, 'location_id'].tolist()\n",
    "            if locations:\n",
    "                cluster_dict[c] = locations\n",
    "\n",
    "        self._compute_batch_performances(cluster_dict, verbose=verbose)\n",
    "        no_improv = 0\n",
    "        best_remaining = len(cluster_dict)\n",
    "        for iteration in range(self.max_iterations):\n",
    "            elapsed = (time.time() - start_time) / 60\n",
    "            if elapsed > self.max_execution_time_min:\n",
    "                print(f\"‚è∞ STOP: superato tempo massimo ({self.max_execution_time_min} minuti)\")\n",
    "                break\n",
    "            print(f\"\\nüîÑ Iterazione {iteration+1}/{self.max_iterations} ({round(elapsed,2)} min)\")\n",
    "\n",
    "            # 1. Split cluster fuori soglia\n",
    "            cluster_dict = self._split_oversized_clusters(cluster_dict, verbose=verbose)\n",
    "            # 2. Salvare cluster \"buoni\" (finali)\n",
    "            cluster_dict = self._save_good_clusters(cluster_dict, verbose=verbose)\n",
    "            # 3. Merge settoriale\n",
    "            cluster_dict = self._merge_clusters_by_sector(cluster_dict, delivery_points, use_sectors=True, verbose=verbose)\n",
    "            # 4. Merge globale (se necessario)\n",
    "            merged2 = self._merge_clusters_by_sector(cluster_dict, delivery_points, use_sectors=False, verbose=verbose)\n",
    "            if len(merged2) < len(cluster_dict):\n",
    "                cluster_dict = merged2\n",
    "\n",
    "            # Controlla se la soluzione √® migliorata\n",
    "            remaining = len(cluster_dict)\n",
    "            print(f\"   ‚ÑπÔ∏è  Cluster ancora da processare: {remaining}\")\n",
    "            if remaining < best_remaining:\n",
    "                best_remaining = remaining\n",
    "                no_improv = 0\n",
    "            else:\n",
    "                no_improv += 1\n",
    "            if no_improv >= 10:\n",
    "                print(\"üü° STOP: 10 iterazioni senza miglioramento\")\n",
    "                break\n",
    "            if not cluster_dict:\n",
    "                print(\"‚úÖ STOP: tutti i cluster sono buoni\")\n",
    "                break\n",
    "\n",
    "            # Aggiorna cluster nel DataFrame\n",
    "            mapping = {loc: cid for cid, locs in cluster_dict.items() for loc in locs}\n",
    "            delivery_points['cluster'] = delivery_points['location_id'].map(mapping)\n",
    "\n",
    "        print(f\"\\nüèÅ Concluso. Cluster finali: {len(self.final_clusters)}\")\n",
    "        sizes = [len(v) for v in self.final_clusters.values()]\n",
    "        if sizes:\n",
    "            print(f\"   - min: {min(sizes)}  max: {max(sizes)}  media: {np.mean(sizes):.1f}\")\n",
    "\n",
    "        print(f\"üìä Sono rimasti {len(cluster_dict)} cluster non accettati, primo cluster non accettato con ID = {len(self.final_clusters)+1}\")\n",
    "        \n",
    "        # Inclusione nell'output anche dei cluster non accettati rimasti\n",
    "        for cid, locs in cluster_dict.items():\n",
    "            final_id = len(self.final_clusters) + 1\n",
    "            self.final_clusters[final_id] = locs\n",
    "\n",
    "        # Calcola performance finali\n",
    "        perf_df = pc.calc_clusters_stats_AS(list(self.final_clusters.values()), time_limit=3, parallel=True, max_workers=self.n_cores, verbose=False)\n",
    "        return self.final_clusters, perf_df\n",
    "\n",
    "def run_adaptive_performance_clustering(delivery_points, initial_k=50, max_iterations=15, n_cores=None, max_execution_time_min=500):\n",
    "    clusterer = AdaptivePerformanceClustering(\n",
    "        max_iterations=max_iterations,\n",
    "        max_execution_time_min=max_execution_time_min,\n",
    "        n_cores=n_cores\n",
    "    )\n",
    "    return clusterer.run_adaptive_clustering(\n",
    "        delivery_points=delivery_points,\n",
    "        initial_k=initial_k,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "final_clusters, performance_df = run_adaptive_performance_clustering(\n",
    "    delivery_points=pc.delivery_points_AS,\n",
    "    initial_k=50,\n",
    "    max_iterations=100)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Tempo di esecuzione algoritmo: {(end - start)/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f0b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df.to_csv('clustering_methods_performances/k-means_euristics_AS_5.csv')\n",
    "\n",
    "with open('cluster_dicts/cluster_dict_k-means_euristics_AS_5.pkl', 'wb') as f:\n",
    "    pickle.dump(final_clusters, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ca05a5",
   "metadata": {},
   "source": [
    "# run ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14330052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "from itertools import combinations\n",
    "\n",
    "import import_ipynb\n",
    "import performance_calc as pc\n",
    "\n",
    "class AdaptivePerformanceClustering:\n",
    "    def __init__(self, \n",
    "                 n_cores: int = None,\n",
    "                 cache_dir: str = \"./cluster_cache\",\n",
    "                 max_iterations: int = 15,\n",
    "                 max_execution_time_min: int = 500):\n",
    "        # Inizializzazione parametri e strutture dati\n",
    "        self.n_cores = n_cores or max(1, mp.cpu_count() - 1)\n",
    "        self.cache_dir = cache_dir\n",
    "        self.max_iterations = max_iterations\n",
    "        self.max_execution_time_min = max_execution_time_min\n",
    "        self.final_clusters = {}\n",
    "        self.cluster_performances = {}\n",
    "        self.min_cluster_size = 3 # i troppo piccoli sono quelli con 1 o 2 punti\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        print(f\"üöÄ AdaptivePerformanceClustering avviato con {self.n_cores} core e timeout {max_execution_time_min} min\")\n",
    "    \n",
    "    def _cache_key(self, location_ids):\n",
    "        return hash(tuple(sorted(location_ids)))\n",
    "    \n",
    "    def _compute_batch_performances(self, cluster_dict, verbose=True):\n",
    "        \"\"\"\n",
    "        Calcola le performance di routing per tutti i cluster usando pi√π thread,\n",
    "        e salva tutto in self.cluster_performances.\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"üîé Calcolo performance in batch ({len(cluster_dict)} cluster)...\")\n",
    "        valid_clusters = { k:v for k,v in cluster_dict.items() if len(v) >= self.min_cluster_size }\n",
    "        small_clusters = { k:v for k,v in cluster_dict.items() if len(v) < self.min_cluster_size }\n",
    "        clusters_list = list(valid_clusters.values())\n",
    "        cluster_ids = list(valid_clusters.keys())\n",
    "        # Chiamata batch parallela\n",
    "        if valid_clusters:\n",
    "            performance_df = pc.calc_clusters_stats_ON(\n",
    "                clusters=clusters_list,\n",
    "                time_limit=3,\n",
    "                parallel=True,\n",
    "                max_workers=self.n_cores,\n",
    "                verbose=False\n",
    "            )\n",
    "            for i, cluster_id in enumerate(cluster_ids):\n",
    "                name = f'Cluster {i+1}'\n",
    "                cluster_data = performance_df[performance_df['cluster'] == name]\n",
    "                if not cluster_data.empty:\n",
    "                    # Calcolo il massimo dei mean_minutes su tutti i giorni della settimana\n",
    "                    max_mean = cluster_data['mean_minutes'].max()\n",
    "                    self.cluster_performances[cluster_id] = {\n",
    "                        'dataframe': cluster_data,\n",
    "                        'max_mean_minutes': max_mean,\n",
    "                        'is_valid': True\n",
    "                    }\n",
    "                else:\n",
    "                    self.cluster_performances[cluster_id] = {\n",
    "                        'dataframe': None,\n",
    "                        'max_mean_minutes': float('inf'),\n",
    "                        'is_valid': False\n",
    "                    }\n",
    "        # Gestione cluster troppo piccoli\n",
    "        for cluster_id in small_clusters.keys():\n",
    "            self.cluster_performances[cluster_id] = {\n",
    "                'dataframe': None,\n",
    "                'max_mean_minutes': 0,\n",
    "                'is_valid': True,   # true cos√¨ non blocca i merge \n",
    "                'too_small': True\n",
    "            }\n",
    "        if verbose:\n",
    "            print(f\"    ‚úÖ Performance salvate in self.cluster_performances\")\n",
    "        return\n",
    "    \n",
    "    def _save_good_clusters(self, cluster_dict, verbose=True):\n",
    "        \"\"\"\n",
    "        Trova i cluster PERFETTI: solo se il valore massimo dei mean_minutes su tutti i giorni della settimana\n",
    "        √® COMPRESO tra 350 e 450 (inclusi). Aggiunge questi cluster a quelli finali e li rimuove dai temporanei.\n",
    "        \"\"\"\n",
    "        saved_count = 0\n",
    "        remaining_clusters = {}\n",
    "        for cluster_id, location_ids in cluster_dict.items():\n",
    "            perf = self.cluster_performances.get(cluster_id)\n",
    "            # Considera solo cluster che NON sono troppo piccoli\n",
    "            if len(location_ids) < self.min_cluster_size:\n",
    "                remaining_clusters[cluster_id] = location_ids\n",
    "                continue\n",
    "            if perf and perf['is_valid']:\n",
    "                max_mean = perf['max_mean_minutes']\n",
    "                # criterio: il pi√π grande dei mean_minutes per quel cluster e settimana\n",
    "                if 350 <= max_mean <= 450:\n",
    "                    # Salva come cluster perfetto\n",
    "                    final_id = len(self.final_clusters)+1\n",
    "                    self.final_clusters[final_id] = location_ids\n",
    "                    saved_count += 1\n",
    "                    if verbose:\n",
    "                        print(f\"  ‚úÖ Accettato cluster {cluster_id}: max(mean_minutes)={max_mean:.1f} min, punti={len(location_ids)}\")\n",
    "                    continue\n",
    "            # Se non perfetto, tienilo per round successivo\n",
    "            remaining_clusters[cluster_id] = location_ids\n",
    "        if verbose:\n",
    "            print(f\"  üèÅ Salvati {saved_count} cluster ideali ({350}‚â§max‚â§450 min)\")\n",
    "        return remaining_clusters\n",
    "\n",
    "    def _split_oversized_clusters(self, cluster_dict, verbose=True):\n",
    "        \"\"\"\n",
    "        Divide ogni cluster che ha max dei mean_minutes > 450 in N cluster,\n",
    "        dove N = ceil(max_mean_minutes / 450). (solo se pi√π di 4 punti)\n",
    "        \"\"\"\n",
    "        new_clusters = {}\n",
    "        clusters_to_recalc = {}\n",
    "        for cluster_id, location_ids in cluster_dict.items():\n",
    "            perf = self.cluster_performances.get(cluster_id)\n",
    "            max_mean = perf['max_mean_minutes']\n",
    "            size = len(location_ids)\n",
    "            # Split solo se supera la soglia e se pi√π di 4 punti\n",
    "            if size > 4 and max_mean > 450:\n",
    "                n_splits = int(np.ceil(max_mean / 450))\n",
    "                if verbose:\n",
    "                    print(f\"  ‚úÇÔ∏è Cluster {cluster_id}: {size} punti, max(mean_minutes)={max_mean:.1f} split in {n_splits}\")\n",
    "                chunk_size = int(np.ceil(size / n_splits))\n",
    "                for i in range(n_splits):\n",
    "                    start = i * chunk_size\n",
    "                    end = min(start + chunk_size, size)\n",
    "                    chunk = location_ids[start:end]\n",
    "                    new_id = f\"s_{cluster_id}_{i+1}\"\n",
    "                    new_clusters[new_id] = chunk\n",
    "                    clusters_to_recalc[new_id] = chunk\n",
    "            else:\n",
    "                new_clusters[cluster_id] = location_ids\n",
    "        # Ricalcola solo per i nuovi cluster splittati\n",
    "        if clusters_to_recalc:\n",
    "            if verbose:\n",
    "                print(f\"    üîÑ Ricalcolo routing per {len(clusters_to_recalc)} nuovi cluster splittati\")\n",
    "            self._compute_batch_performances(clusters_to_recalc, verbose=False)\n",
    "        return new_clusters\n",
    "\n",
    "    def _can_merge_clusters(self, cluster_id1, cluster_id2):\n",
    "        \"\"\"\n",
    "        Unisce solo se la somma dei mean_minutes per OGNI giorno della settimana √® <450.\n",
    "        Cluster troppo piccoli: merge sempre permesso.\n",
    "        \"\"\"\n",
    "        perf1 = self.cluster_performances.get(cluster_id1)\n",
    "        perf2 = self.cluster_performances.get(cluster_id2)\n",
    "        if (perf1 and perf1.get('too_small')) or (perf2 and perf2.get('too_small')):\n",
    "            return True\n",
    "        if not (perf1 and perf2 and perf1['is_valid'] and perf2['is_valid']):\n",
    "            return False\n",
    "        df1, df2 = perf1['dataframe'], perf2['dataframe']\n",
    "        days1, days2 = set(df1['weekday']), set(df2['weekday'])\n",
    "        common_days = days1 & days2\n",
    "        if not common_days:\n",
    "            return False\n",
    "        for day in common_days:\n",
    "            m1 = df1[df1['weekday'] == day]['mean_minutes'].iloc[0]\n",
    "            m2 = df2[df2['weekday'] == day]['mean_minutes'].iloc[0]\n",
    "            if m1 + m2 >= 450:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def _divide_space_into_sectors(self, delivery_points):\n",
    "        \"\"\"\n",
    "        Divide lo spazio geografico in 4 settori (NE, NW, SE, SW) usando le mediane\n",
    "        di latitudine e longitudine. Questo aiuta a mergeare cluster geograficamente vicini.\n",
    "        \"\"\"\n",
    "        lat_median = delivery_points['lat'].median()\n",
    "        lon_median = delivery_points['lon'].median()\n",
    "        \n",
    "        sectors = {'NE': [], 'NW': [], 'SE': [], 'SW': []}\n",
    "        \n",
    "        # Per ogni cluster trova il centroide e assegnalo al settore corrispondente\n",
    "        for cluster_id in delivery_points['cluster'].unique():\n",
    "            cluster_points = delivery_points[delivery_points['cluster'] == cluster_id]\n",
    "            if len(cluster_points) == 0:\n",
    "                continue\n",
    "                \n",
    "            center_lat = cluster_points['lat'].mean()\n",
    "            center_lon = cluster_points['lon'].mean()\n",
    "            \n",
    "            # Determina il settore basato su mediane\n",
    "            if center_lat >= lat_median and center_lon >= lon_median:\n",
    "                sector = 'NE'\n",
    "            elif center_lat >= lat_median and center_lon < lon_median:\n",
    "                sector = 'NW' \n",
    "            elif center_lat < lat_median and center_lon >= lon_median:\n",
    "                sector = 'SE'\n",
    "            else:\n",
    "                sector = 'SW'\n",
    "                \n",
    "            sectors[sector].append(cluster_id)\n",
    "        \n",
    "        return sectors\n",
    "\n",
    "\n",
    "    def _merge_clusters_by_sector(self, cluster_dict, delivery_points, use_sectors=True, verbose=True):\n",
    "        \"\"\"\n",
    "        Tenta merge di cluster piccoli e poi merge compatibili sempre secondo criterio < 450 min/giorno.\n",
    "        Crea i nuovi id 'm_x_y'. Ricalcola il routing solo per i merge eseguiti.\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"üîó Tentativo di merge cluster tra settori\" if use_sectors else \"üîó Merge globale\")\n",
    "\n",
    "        if use_sectors:\n",
    "            sectors = self._divide_space_into_sectors(delivery_points)\n",
    "        else:\n",
    "            sectors = {'ALL': list(cluster_dict.keys())}\n",
    "        merged_clusters, merged_pairs = {}, set()\n",
    "        clusters_for_recalc = {}\n",
    "\n",
    "        small_clusters = set([k for k, v in cluster_dict.items() if len(v) < self.min_cluster_size])\n",
    "        for sector, cluster_list in sectors.items():\n",
    "            avail = [c for c in cluster_list if c in cluster_dict and c not in merged_pairs]\n",
    "            sc = [c for c in avail if c in small_clusters]\n",
    "            nc = [c for c in avail if c not in small_clusters]\n",
    "            # 1. Merge cluster piccoli con altri\n",
    "            for s in sc:\n",
    "                if s in merged_pairs: continue\n",
    "                for n in nc:\n",
    "                    if n in merged_pairs: continue\n",
    "                    merged_id = f\"m_{s}_{n}\"\n",
    "                    can_merge = self._can_merge_clusters(s, n)\n",
    "                    if can_merge:\n",
    "                        merged_clusters[merged_id] = cluster_dict[s] + cluster_dict[n]\n",
    "                        clusters_for_recalc[merged_id] = merged_clusters[merged_id]\n",
    "                        merged_pairs.update([s, n])\n",
    "                        if verbose: print(f\"  üîó Merge {s} + {n} ‚ûî {merged_id}\")\n",
    "                        break\n",
    "            # 2. Merge tra cluster normali\n",
    "            avail_norm = [c for c in nc if c not in merged_pairs]\n",
    "            for i, c1 in enumerate(avail_norm):\n",
    "                if c1 in merged_pairs: continue\n",
    "                for c2 in avail_norm[i+1:]:\n",
    "                    if c2 in merged_pairs: continue\n",
    "                    merged_id = f\"m_{c1}_{c2}\"\n",
    "                    if self._can_merge_clusters(c1, c2):\n",
    "                        merged_clusters[merged_id] = cluster_dict[c1] + cluster_dict[c2]\n",
    "                        clusters_for_recalc[merged_id] = merged_clusters[merged_id]\n",
    "                        merged_pairs.update([c1,c2])\n",
    "                        if verbose: print(f\"  üîó Merge {c1} + {c2} ‚ûî {merged_id}\")\n",
    "                        break\n",
    "        # Tieni quelli che non sono stati uniti\n",
    "        for k, v in cluster_dict.items():\n",
    "            if k not in merged_pairs:\n",
    "                merged_clusters[k] = v\n",
    "        # Ricalcola solo per i nuovi mergiati\n",
    "        if clusters_for_recalc:\n",
    "            if verbose: print(f\"    üîÅ Ricalcolo routing per {len(clusters_for_recalc)} nuovi cluster merged\")\n",
    "            self._compute_batch_performances(clusters_for_recalc, verbose=False)\n",
    "        return merged_clusters\n",
    "\n",
    "    def run_adaptive_clustering(self, delivery_points: pd.DataFrame, initial_k=50, verbose=True):\n",
    "        \"\"\"\n",
    "        Esegue il ciclo: k-means, split dove max(mean_minutes)>450, merge <450, salva cluster buoni\n",
    "        STOP: max iterazioni, timeout, 3 iter senza miglioramenti.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        print(f\"üéØ START: AdaptivePerformanceClustering su {len(delivery_points)} punti (k={initial_k})\")\n",
    "        scaler = StandardScaler()\n",
    "        points_scaled = scaler.fit_transform(delivery_points[['lat', 'lon']])\n",
    "        kmeans = KMeans(n_clusters=initial_k, init='k-means++', n_init=1, random_state=42)\n",
    "        labels = kmeans.fit_predict(points_scaled)\n",
    "        delivery_points['cluster'] = labels\n",
    "\n",
    "        # Dizionario iniziale cluster (location_id)\n",
    "        cluster_dict = {}\n",
    "        for c in range(initial_k):\n",
    "            locations = delivery_points.loc[delivery_points['cluster'] == c, 'location_id'].tolist()\n",
    "            if locations:\n",
    "                cluster_dict[c] = locations\n",
    "\n",
    "        self._compute_batch_performances(cluster_dict, verbose=verbose)\n",
    "        no_improv = 0\n",
    "        best_remaining = len(cluster_dict)\n",
    "        for iteration in range(self.max_iterations):\n",
    "            elapsed = (time.time() - start_time) / 60\n",
    "            if elapsed > self.max_execution_time_min:\n",
    "                print(f\"‚è∞ STOP: superato tempo massimo ({self.max_execution_time_min} minuti)\")\n",
    "                break\n",
    "            print(f\"\\nüîÑ Iterazione {iteration+1}/{self.max_iterations} ({round(elapsed,2)} min)\")\n",
    "\n",
    "            # 1. Split cluster fuori soglia\n",
    "            cluster_dict = self._split_oversized_clusters(cluster_dict, verbose=verbose)\n",
    "            # 2. Salvare cluster \"buoni\" (finali)\n",
    "            cluster_dict = self._save_good_clusters(cluster_dict, verbose=verbose)\n",
    "            # 3. Merge settoriale\n",
    "            cluster_dict = self._merge_clusters_by_sector(cluster_dict, delivery_points, use_sectors=True, verbose=verbose)\n",
    "            # 4. Merge globale (se necessario)\n",
    "            merged2 = self._merge_clusters_by_sector(cluster_dict, delivery_points, use_sectors=False, verbose=verbose)\n",
    "            if len(merged2) < len(cluster_dict):\n",
    "                cluster_dict = merged2\n",
    "\n",
    "            # Controlla se la soluzione √® migliorata\n",
    "            remaining = len(cluster_dict)\n",
    "            print(f\"   ‚ÑπÔ∏è  Cluster ancora da processare: {remaining}\")\n",
    "            if remaining < best_remaining:\n",
    "                best_remaining = remaining\n",
    "                no_improv = 0\n",
    "            else:\n",
    "                no_improv += 1\n",
    "            if no_improv >= 10:\n",
    "                print(\"üü° STOP: 10 iterazioni senza miglioramento\")\n",
    "                break\n",
    "            if not cluster_dict:\n",
    "                print(\"‚úÖ STOP: tutti i cluster sono buoni\")\n",
    "                break\n",
    "\n",
    "            # Aggiorna cluster nel DataFrame\n",
    "            mapping = {loc: cid for cid, locs in cluster_dict.items() for loc in locs}\n",
    "            delivery_points['cluster'] = delivery_points['location_id'].map(mapping)\n",
    "\n",
    "        print(f\"\\nüèÅ Concluso. Cluster finali: {len(self.final_clusters)}\")\n",
    "        sizes = [len(v) for v in self.final_clusters.values()]\n",
    "        if sizes:\n",
    "            print(f\"   - min: {min(sizes)}  max: {max(sizes)}  media: {np.mean(sizes):.1f}\")\n",
    "\n",
    "        print(f\"üìä Sono rimasti {len(cluster_dict)} cluster non accettati, primo cluster non accettato con ID = {len(self.final_clusters)+1}\")\n",
    "        \n",
    "        # Inclusione nell'output anche dei cluster non accettati rimasti\n",
    "        for cid, locs in cluster_dict.items():\n",
    "            final_id = len(self.final_clusters) + 1\n",
    "            self.final_clusters[final_id] = locs\n",
    "\n",
    "        # Calcola performance finali\n",
    "        perf_df = pc.calc_clusters_stats_ON(list(self.final_clusters.values()), time_limit=3, parallel=True, max_workers=self.n_cores, verbose=False)\n",
    "        return self.final_clusters, perf_df\n",
    "\n",
    "def run_adaptive_performance_clustering(delivery_points, initial_k=50, max_iterations=15, n_cores=None, max_execution_time_min=500):\n",
    "    clusterer = AdaptivePerformanceClustering(\n",
    "        max_iterations=max_iterations,\n",
    "        max_execution_time_min=max_execution_time_min,\n",
    "        n_cores=n_cores\n",
    "    )\n",
    "    return clusterer.run_adaptive_clustering(\n",
    "        delivery_points=delivery_points,\n",
    "        initial_k=initial_k,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "final_clusters, performance_df = run_adaptive_performance_clustering(\n",
    "    delivery_points=pc.delivery_points_ON,\n",
    "    initial_k=50,\n",
    "    max_iterations=100)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Tempo di esecuzione algoritmo: {(end - start)/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0da933",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df.to_csv('clustering_methods_performances/k-means_euristics_ON_5.csv')\n",
    "\n",
    "with open('cluster_dicts/cluster_dict_k-means_euristics_ON_5.pkl', 'wb') as f:\n",
    "    pickle.dump(final_clusters, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

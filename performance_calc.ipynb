{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Preparazione dati sia storici che non"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_data = pd.read_csv('delivery_history.csv').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Veloce overview del contenuto del csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_data.info()\n",
    "delivery_data.describe()\n",
    "delivery_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Conversione dei dati nei relativi tipi di dati specifici e controllare se tutti i valori sono stati convertiti correttamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Conversione sicura di 'delivery_date'\n",
    "delivery_date_parsed = pd.to_datetime(delivery_data['delivery_date'], errors='coerce').dt.date\n",
    "\n",
    "# Trova valori non convertibili\n",
    "invalid_delivery_dates = delivery_data.loc[delivery_date_parsed.isna(), 'delivery_date'].unique()\n",
    "print(\"❌ Valori non convertiti in 'delivery_date':\")\n",
    "print(invalid_delivery_dates)\n",
    "\n",
    "# Applica conversione solo se valida\n",
    "delivery_data['delivery_date'] = delivery_date_parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Conversione sicura di 'is_event' a booleano\n",
    "# Prima controlliamo che tutti i valori siano 0 o 1 (o equivalenti)\n",
    "valid_bool_mask = delivery_data['is_event'].isin([0, 1, True, False])\n",
    "\n",
    "# Stampa eventuali valori invalidi\n",
    "invalid_is_event = delivery_data.loc[~valid_bool_mask, 'is_event'].unique()\n",
    "print(\"\\n❌ Valori non convertibili in booleani in 'is_event':\")\n",
    "print(invalid_is_event)\n",
    "\n",
    "# Applica conversione solo ai validi (opzionale: puoi forzare tutto a bool, ma questo è più sicuro)\n",
    "delivery_data.loc[valid_bool_mask, 'is_event'] = delivery_data.loc[valid_bool_mask, 'is_event'].astype(bool)\n",
    "\n",
    "# Filtra i dati per mantenere solo le righe con 'is_event' = True\n",
    "delivery_data = delivery_data[delivery_data['is_event'] == True].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: tenta la conversione usando pd.to_datetime in modo vettorizzato\n",
    "start_times = pd.to_datetime(delivery_data['window_start_0'], format='%H:%M:%S.%f', errors='coerce')\n",
    "end_times = pd.to_datetime(delivery_data['window_end_0'], format='%H:%M:%S.%f', errors='coerce')\n",
    "\n",
    "# Step 2: crea solo HH:MM (datetime.time con secondi e microsecondi azzerati)\n",
    "delivery_data['window_start_0'] = start_times.dt.time.apply(lambda t: t.replace(second=0, microsecond=0) if pd.notnull(t) else None)\n",
    "delivery_data['window_end_0'] = end_times.dt.time.apply(lambda t: t.replace(second=0, microsecond=0) if pd.notnull(t) else None)\n",
    "\n",
    "# Step 3: estrai i valori che non sono stati convertiti (NaT → errore)\n",
    "invalid_start = delivery_data.loc[start_times.isna(), 'window_start_0'].unique()\n",
    "invalid_end = delivery_data.loc[end_times.isna(), 'window_end_0'].unique()\n",
    "\n",
    "# Step 4: stampa risultati\n",
    "print(\"❌ Valori non convertiti in window_start_0:\")\n",
    "print(invalid_start)\n",
    "\n",
    "print(\"\\n❌ Valori non convertiti in window_end_0:\")\n",
    "print(invalid_end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Eliminazione dei sabati dai delivery data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escludi sabati e domeniche in modo vettorizzato e compatto\n",
    "delivery_data = delivery_data[delivery_data['delivery_date'].map(lambda d: d.weekday() < 5)].copy()\n",
    "\n",
    "# Convertiamo la colonna data in datetime\n",
    "delivery_data['delivery_date'] = pd.to_datetime(delivery_data['delivery_date'], errors='coerce')\n",
    "\n",
    "# Drop righe con date non valide\n",
    "delivery_data = delivery_data.dropna(subset=['delivery_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_data.info()\n",
    "delivery_data.describe()\n",
    "delivery_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Preparazione di altre variabili rappresentanti i vincoli del problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "capacity = 2100  # Capacità massima del veicolo\n",
    "depot_location = (45.399332372533415, 9.28379118387267)  # Coordinate del magazzino (latitudine, longitudine)\n",
    "shift_duration = 8 * 60  # Durata massima del percorso in minuti (8 ore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Generare la lista di tutti i possibili punti di consegna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Generazione base di dati per il confronto agosto-settembre (AS) vs ottobre-novembre (ON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra solo i mesi agosto-settembre e ottobre-novembre\n",
    "\n",
    "delivery_data_AS = delivery_data[delivery_data['delivery_date'].dt.month.isin([8, 9])]\n",
    "delivery_data_ON = delivery_data[delivery_data['delivery_date'].dt.month.isin([10, 11])]\n",
    "delivery_data_OND = delivery_data[delivery_data['delivery_date'].dt.month.isin([10, 11, 12])]\n",
    "\n",
    "delivery_points_AS = delivery_data_AS[['location_id', 'lat', 'lon']].drop_duplicates().reset_index(drop=True)\n",
    "delivery_points_ON = delivery_data_ON[['location_id', 'lat', 'lon']].drop_duplicates().reset_index(drop=True)\n",
    "delivery_points_OND = delivery_data_OND[['location_id', 'lat', 'lon']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# eliminazione dei punti troppo lontani\n",
    "delivery_points_AS = delivery_points_AS[(delivery_points_AS['location_id'] != 14930) & (delivery_points_AS['location_id'] != 15133)]\n",
    "delivery_points_ON = delivery_points_ON[(delivery_points_ON['location_id'] != 14930) & (delivery_points_ON['location_id'] != 15133)]\n",
    "delivery_points_OND = delivery_points_OND[(delivery_points_OND['location_id'] != 14930) & (delivery_points_OND['location_id'] != 15133)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutti i delivery point unici\n",
    "delivery_points = delivery_data[['location_id', 'lat', 'lon']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"Punti di consegna unici trovati:\", len(delivery_points))\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Filtra i dati (elimina quelli troppo lontani)\n",
    "# -----------------------------\n",
    "delivery_points = delivery_points[(delivery_points['location_id'] != 14930) & (delivery_points['location_id'] != 15133)]  # rimuovi record troppo lontani\n",
    "\n",
    "\n",
    "print(\"Dopo l'eliminazione di punti troppo distanti, sono rimasti\", len(delivery_points), \"punti di consegna\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"delivery points esempio:\\n{delivery_points.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Creazione della matrice delle distanze da utilizzare nell'OR-tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Prende in input un singolo cluster e ne restituisce la matrice delle distanze (espresse in minuti di percorrenza) e due variabili utili per il mapping tra index e location_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix_creation(period, delivery_points_custom=None):\n",
    "\n",
    "\t# period può essere = 'AS' o 'ON' o 'OND' o qualsiasi altro valore (tutti i punti)\n",
    "\t\n",
    "\t# 1) Costruisco la lista \"depot + punti\"\n",
    "\t\n",
    "\tpoints = pd.DataFrame([{\"location_id\": 0, \"lat\": depot_location[0], \"lon\": depot_location[1]}])\n",
    "\n",
    "\tif delivery_points_custom is not None:\n",
    "\t\tpoints = pd.concat([points, delivery_points_custom[['location_id','lat','lon']]], ignore_index=True)\n",
    "\telif period == 'AS':\n",
    "\t\tpoints = pd.concat([points, delivery_points_AS[['location_id','lat','lon']]], ignore_index=True)\n",
    "\telif period == 'ON':\n",
    "\t\tpoints = pd.concat([points, delivery_points_ON[['location_id','lat','lon']]], ignore_index=True)\n",
    "\telif period == 'OND':\n",
    "\t\tpoints = pd.concat([points, delivery_points_OND[['location_id','lat','lon']]], ignore_index=True)\n",
    "\telse:\n",
    "\t\tpoints = pd.concat([points, delivery_points[['location_id','lat','lon']]], ignore_index=True)\n",
    "\t\n",
    "\n",
    "\t# 2) Haversine vectorized -> matrice NxN in metri (int)\n",
    "\tR = 6371000.0  # raggio medio terrestre in metri\n",
    "\n",
    "\tlat = np.radians(points['lat'].values)   # shape (N,)\n",
    "\tlon = np.radians(points['lon'].values)   # shape (N,)\n",
    "\n",
    "\t# broadcasting: differenze tutte-vs-tutte\n",
    "\tdlat = lat[:, None] - lat[None, :]\n",
    "\tdlon = lon[:, None] - lon[None, :]\n",
    "\n",
    "\ta = np.sin(dlat/2.0)**2 + np.cos(lat)[:, None] * np.cos(lat)[None, :] * np.sin(dlon/2.0)**2\n",
    "\tc = 2.0 * np.arctan2(np.sqrt(a), np.sqrt(1.0 - a))\n",
    "\tdistance_matrix_metri = (R * c).astype(int)  # metri\n",
    "\n",
    "\t# trasformo le distanze in tempi di percorrenza in minuti\n",
    "\n",
    "\t# 1.1) metri -> km\n",
    "\tdist_km = distance_matrix_metri.astype(float) / 1000.0\n",
    "\n",
    "\t# 2.1) modello di velocità (km/h) per fasce di distanza\n",
    "\t#    [0–5] km -> 30 km/h  |  (5–20] km -> 50 km/h  |  >20 km -> 70 km/h\n",
    "\tspeed_kmh = np.full_like(dist_km, 50.0)                     # riempimento di default della matrice delle velocità\n",
    "\tspeed_kmh[dist_km <= 5.0] = 30.0                            # urbano\n",
    "\tspeed_kmh[dist_km > 20.0] = 70.0                            # extraurbano\n",
    "\n",
    "\t# Evita divisione per zero sulla diagonale (il tempo lì lo forziamo a 0)\n",
    "\tnp.fill_diagonal(speed_kmh, 1.0)\n",
    "\n",
    "\t# 3.1) tempo = (distanza / velocità) * 60  --> minuti\n",
    "\ttime_min = (dist_km / speed_kmh) * 60.0\n",
    "\n",
    "\t# Diagonale a 0 (stesso punto)\n",
    "\tnp.fill_diagonal(time_min, 0.0)\n",
    "\n",
    "\t# 4.1) Arrotondamento a interi (meglio CEIL per non sottostimare i tempi)\n",
    "\ttime_mat_min = np.ceil(time_min).astype(int)      # matrice delle distanze trasformate in minuti di percorrenza\n",
    "\n",
    "\t# cambio del tipo di variabile per risparmiare memoria\n",
    "\tdistance_matrix_metri = distance_matrix_metri.astype(np.uint16)\n",
    "\ttime_mat_min = time_mat_min.astype(np.uint16)\n",
    "\n",
    "\n",
    "\t# mapping che serve a risalire dall'indice al location_id originale\n",
    "\tindex_to_location_id = points['location_id'].to_list()\n",
    "\tlocation_id_to_index = {int(lid): i for i, lid in enumerate(points['location_id'])}\n",
    "\n",
    "\treturn distance_matrix_metri, time_mat_min, index_to_location_id, location_id_to_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "try to create unique files of matrixes in order to not saturate the ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('utility_memory/distance_matrix_OND.pkl', 'wb') as f:\n",
    "#     pickle.dump(distance_matrix_creation('OND'), f)\n",
    "\n",
    "# with open('utility_memory/distance_matrix_AS.pkl', 'wb') as f:\n",
    "#     pickle.dump(distance_matrix_creation('AS'), f)\n",
    "\n",
    "# with open('utility_memory/distance_matrix_ON.pkl', 'wb') as f:\n",
    "#     pickle.dump(distance_matrix_creation('ON'), f)\n",
    "\n",
    "# with open('utility_memory/full_distance_matrix.pkl', 'rb') as f:\n",
    "#     full_distance_matrix = pickle.load(f)\n",
    "#     # vedi il tipo di variabile presente all'interno di distance_matrix_metri e time_mat_min\n",
    "#     print(type(full_distance_matrix[0][0][0]))  # tipo del primo elemento della matrice\n",
    "#     print(type(full_distance_matrix[1][0][0]))  # tipo del primo elemento della matrice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## TSP con OR-Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ortools.constraint_solver import pywrapcp, routing_enums_pb2\n",
    "\n",
    "def solve_tsp(distance_matrix_metri, time_mat_min, index_to_location_id=None, time_limit_seconds=30, minutes_per_delivery=10):\n",
    "    \"\"\"\n",
    "    Risolve il TSP (1 veicolo, ritorno al depot) usando la distance_matrix in input_data.\n",
    "    Ritorna: dict con route in indici, in location_id e distanza totale in metri.\n",
    "    \"\"\"\n",
    "    distance_matrix = time_mat_min\n",
    "    num_vehicles = 1          # deve essere 1 per TSP\n",
    "    depot_index = 0           # depot sempre in posizione 0\n",
    "\n",
    "    # 1) Manager e modello\n",
    "    manager = pywrapcp.RoutingIndexManager(len(distance_matrix), num_vehicles, depot_index)\n",
    "    routing = pywrapcp.RoutingModel(manager)\n",
    "\n",
    "    # 2) Callback costi (distanze in minuti, interi)\n",
    "    def distance_callback(from_index, to_index):\n",
    "        from_node = manager.IndexToNode(from_index)\n",
    "        to_node   = manager.IndexToNode(to_index)\n",
    "        return int(distance_matrix[from_node][to_node])\n",
    "\n",
    "    transit_cb_index = routing.RegisterTransitCallback(distance_callback)\n",
    "    routing.SetArcCostEvaluatorOfAllVehicles(transit_cb_index)\n",
    "\n",
    "    # 3) Parametri di ricerca\n",
    "    search_params = pywrapcp.DefaultRoutingSearchParameters()\n",
    "    search_params.first_solution_strategy = routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC\n",
    "    search_params.local_search_metaheuristic = routing_enums_pb2.LocalSearchMetaheuristic.GUIDED_LOCAL_SEARCH\n",
    "    search_params.time_limit.FromSeconds(int(time_limit_seconds))\n",
    "\n",
    "    # 4) Risoluzione\n",
    "    solution = routing.SolveWithParameters(search_params)\n",
    "    if solution is None:\n",
    "        # se non ho trovato una soluzione aumento il time limit e riprovo\n",
    "        search_params.time_limit.FromSeconds(int(time_limit_seconds)*3)\n",
    "        solution = routing.SolveWithParameters(search_params)\n",
    "        if solution is None:\n",
    "            raise RuntimeError(\"Nessuna soluzione trovata (aumenta il time_limit_seconds).\")\n",
    "        \n",
    "\n",
    "    # 5) Estrazione del tour\n",
    "    route_indices = []\n",
    "    travel_time_min = 0\n",
    "    index = routing.Start(0)\n",
    "    while not routing.IsEnd(index):\n",
    "        node = manager.IndexToNode(index)\n",
    "        route_indices.append(node)\n",
    "        previous_index = index\n",
    "        index = solution.Value(routing.NextVar(index))\n",
    "        travel_time_min += routing.GetArcCostForVehicle(previous_index, index, 0)\n",
    "    # chiudi il ciclo con il depot finale\n",
    "    route_indices.append(manager.IndexToNode(index))\n",
    "\n",
    "    # # 6) Mapping (se fornito): indici -> location_id\n",
    "    # route_location_ids = None\n",
    "    # if index_to_location_id is not None:\n",
    "    #     route_location_ids = [index_to_location_id[i] for i in route_indices]\n",
    "\n",
    "    # 7) Stampa leggibile\n",
    "    def fmt_route(ids_or_idx):\n",
    "        return \" -> \".join(map(str, ids_or_idx))\n",
    "\n",
    "    \n",
    "    # print(\"TSP – sequenza di nodi (indici):\")\n",
    "    # print(fmt_route(route_indices))\n",
    "    # if route_location_ids is not None:\n",
    "    #     print(\"\\nTSP – sequenza di location_id:\")\n",
    "    #     print(fmt_route(route_location_ids))\n",
    "    \n",
    "\n",
    "    dist_m = np.asarray(distance_matrix_metri, dtype=float)\n",
    "    # Somma delle distanze sui successivi archi del tour\n",
    "    total_distance_m = 0.0\n",
    "\n",
    "    # Somma archi consecutivi\n",
    "    for i, j in zip(route_indices[:-1], route_indices[1:]):\n",
    "        total_distance_m += dist_m[i, j]\n",
    "\n",
    "    # tempo per lo scarico merci (minuti) - 10 minuti per consegna\n",
    "    unloading_time = (len(route_indices)-2)*minutes_per_delivery\n",
    "\n",
    "    #print(f\"\\nTempo di percorrenza del percorso: {travel_time_min:,} minuti -> {travel_time_min/60:.2f} ore\")\n",
    "    #print(f\"\\nTempo per effettuare le {len(route_location_ids)-2} (consegne): {unloading_time} minuti -> {unloading_time/60:.2f} ore\")\n",
    "    #print(f\"\\nTempo totale stimato (percorrenza + consegne): {travel_time_min + unloading_time} minuti -> {(travel_time_min + unloading_time)/60:.2f} ore\")\n",
    "\n",
    "    return {\n",
    "        # se serve è la lista ordinata degli index dei delivery points -> \"route_indices\": route_indices,\n",
    "        # se serve è la lista ordinata degli location_id -> \"route_location_ids\": route_location_ids,\n",
    "        \"total_distance_m\": total_distance_m,         # in metri\n",
    "        \"num_deliveries\": len(route_indices)-2,       # escludi depot iniziale e finale\n",
    "        \"travel_time_min\": travel_time_min,\n",
    "        \"unloading_time_min\": unloading_time,\n",
    "        \"total_delivery_time\": travel_time_min + unloading_time\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# Calcolo delle performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Esecuzione del routing dato un cluster specifico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def get_weekday_name(weekday_num):\n",
    "    return [\"Lunedì\",\"Martedì\",\"Mercoledì\",\"Giovedì\",\"Venerdì\",\"Sabato\",\"Domenica\"][weekday_num]\n",
    "\n",
    "def single_cluster_stats_with_cache(\n",
    "    cluster_location_ids,\n",
    "    time_limit = 7,                   # time limit per ogni TSP giornaliero\n",
    "    cache: dict | None = None,        # cache riutilizzabile tra chiamate\n",
    "    # return_cache = False,             # se True ritorna anche la cache aggiornata\n",
    "    verbose: bool = False,            # per stampare i dati per debugging\n",
    "    max_workers: int | None = None,   # parallelismo: None=default\n",
    "):\n",
    "    route_cache = cache if cache is not None else {}\n",
    "\n",
    "    # 1) Filtra il delivery_data al cluster (assumi is_event già True e delivery_date già date)\n",
    "    dd = delivery_data[delivery_data['location_id'].isin(set(cluster_location_ids))].copy()\n",
    "\n",
    "    # 2) Costruisci UNA SOLA VOLTA le matrici del cluster\n",
    "    with open('utility_memory/full_distance_matrix.pkl', 'rb') as f:\n",
    "        full_distance_matrix = pickle.load(f)\n",
    "    distance_matrix_metri, time_mat_min, index_to_location_id, location_id_to_index = full_distance_matrix\n",
    "    full_mat_min = np.asarray(time_mat_min, dtype=float)          # minuti\n",
    "    full_mat_meters = np.asarray(distance_matrix_metri, dtype=float)  # metri\n",
    "    depot_global_index = 0  # depot sempre in posizione 0\n",
    "\n",
    "    # 3) Mappa date -> key (set ordinato di location_id); raccogli i set unici non in cache\n",
    "    date_to_key: dict = {}\n",
    "    unique_keys_needed: set = set()\n",
    "\n",
    "    for the_date, df_day in dd.groupby('delivery_date', sort=True):\n",
    "        todays_ids = df_day['location_id'].unique().tolist()\n",
    "        if not todays_ids:\n",
    "            continue\n",
    "        key = tuple(sorted(int(x) for x in todays_ids))\n",
    "        date_to_key[the_date] = key\n",
    "        if key not in route_cache:\n",
    "            unique_keys_needed.add(key)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Set unici da risolvere (non in cache): {len(unique_keys_needed)}\")\n",
    "\n",
    "    # 4) Prepara i job per i soli set mancanti (costruisci le sub-matrici una volta qui)\n",
    "    jobs = []\n",
    "    for key in unique_keys_needed:\n",
    "        day_idx_global = [int(location_id_to_index[lid]) for lid in key]\n",
    "        index_map = [depot_global_index] + day_idx_global\n",
    "        sub_mat_min = full_mat_min[np.ix_(index_map, index_map)]\n",
    "        sub_mat_meters = full_mat_meters[np.ix_(index_map, index_map)]\n",
    "        sub_index_to_location_id = [index_to_location_id[i] for i in index_map]\n",
    "\n",
    "        jobs.append({\n",
    "            \"key\": key,\n",
    "            \"sub_mat_meters\": sub_mat_meters.tolist(),\n",
    "            \"sub_mat_min\": sub_mat_min.tolist(),\n",
    "            \"sub_index_to_location_id\": sub_index_to_location_id\n",
    "        })\n",
    "\n",
    "    # 5) Esegui i TSP dei set unici in PARALLELO (thread)\n",
    "    futures = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        for job in jobs:\n",
    "            fut = ex.submit(\n",
    "                solve_tsp,\n",
    "                job[\"sub_mat_meters\"],\n",
    "                job[\"sub_mat_min\"],\n",
    "                job[\"sub_index_to_location_id\"],\n",
    "                time_limit,                      # secondi di time limit per il TSP giornaliero\n",
    "                10                                # minuti per consegna\n",
    "            )\n",
    "            futures.append((job[\"key\"], fut))\n",
    "\n",
    "        for key, fut in futures:\n",
    "            result = fut.result()  # se c’è errore, esplode qui (utile per debug)\n",
    "            route_cache[key] = {\n",
    "                \"total_delivery_time\": float(result[\"total_delivery_time\"]),\n",
    "                \"distance_m\": float(result[\"total_distance_m\"]),\n",
    "                \"num_deliveries\": result[\"num_deliveries\"],\n",
    "                \"travel_time_min\": float(result[\"travel_time_min\"]),\n",
    "                \"unloading_time_min\": float(result[\"unloading_time_min\"]),\n",
    "            }\n",
    "            if verbose:\n",
    "                print(f\"Set {key} risolto in parallelo -> {route_cache[key]['total_delivery_time']:.1f} min\")\n",
    "\n",
    "    # 6) Costruisci le righe giornaliere usando SEMPRE la cache (ora completa)\n",
    "    rows = []\n",
    "    for the_date, key in sorted(date_to_key.items(), key=lambda x: x[0]):\n",
    "        entry = route_cache[key]\n",
    "        rows.append({\n",
    "            \"delivery_date\": the_date,\n",
    "            \"weekday\": the_date.weekday(),\n",
    "            \"weekday_name\": get_weekday_name(the_date.weekday()),\n",
    "            \"total_delivery_time\": entry[\"total_delivery_time\"],\n",
    "            \"distance_m\": entry[\"distance_m\"],\n",
    "            \"num_deliveries\": entry[\"num_deliveries\"],\n",
    "            \"travel_time_min\": entry[\"travel_time_min\"],\n",
    "            \"unloading_time_min\": entry[\"unloading_time_min\"],\n",
    "            \"quantity_delivered\": dd[dd['delivery_date'] == the_date]['quantity'].sum()\n",
    "        })\n",
    "\n",
    "    if not rows:\n",
    "        return (None, None)\n",
    "\n",
    "    # 7) Output DataFrame ordinati\n",
    "    daily_results = (\n",
    "        pd.DataFrame(rows)\n",
    "        .sort_values(\"delivery_date\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Calcolare quante volte il fattorino supera le 8 ore (480 minuti) per giorno della settimana\n",
    "    daily_results['overtime'] = daily_results['total_delivery_time'] > shift_duration\n",
    "\n",
    "    # Calcolare la media del tempo extra (solo dove supera 8 ore) per weekday\n",
    "    daily_results['overtime_minutes'] = np.where(daily_results['overtime'], daily_results['total_delivery_time'] - shift_duration, 0)\n",
    "\n",
    "\n",
    "\n",
    "    stats_df = (\n",
    "        daily_results\n",
    "        .groupby([\"weekday\", \"weekday_name\"], as_index=False)\n",
    "        .agg(\n",
    "            min_minutes=(\"total_delivery_time\", \"min\"),\n",
    "            max_minutes=(\"total_delivery_time\", \"max\"),\n",
    "            mean_minutes=(\"total_delivery_time\", \"mean\"),\n",
    "            min_distance_m=(\"distance_m\", \"min\"),\n",
    "            max_distance_m=(\"distance_m\", \"max\"),\n",
    "            mean_distance_m=(\"distance_m\", \"mean\"),\n",
    "            min_num_deliveries=(\"num_deliveries\", \"min\"),\n",
    "            max_num_deliveries=(\"num_deliveries\", \"max\"),\n",
    "            mean_num_deliveries=(\"num_deliveries\", \"mean\"),\n",
    "            min_travel_time_min=(\"travel_time_min\", \"min\"),\n",
    "            max_travel_time_min=(\"travel_time_min\", \"max\"),\n",
    "            mean_travel_time_min=(\"travel_time_min\", \"mean\"),\n",
    "            min_unloading_time_min=(\"unloading_time_min\", \"min\"),\n",
    "            max_unloading_time_min=(\"unloading_time_min\", \"max\"),\n",
    "            mean_unloading_time_min=(\"unloading_time_min\", \"mean\"),\n",
    "            n_days=(\"total_delivery_time\", \"count\"),\n",
    "            n_overtime_days=(\"overtime\", \"sum\"),\n",
    "            max_overtime_minutes=(\"overtime_minutes\", \"max\"),\n",
    "            mean_overtime_minutes=(\"overtime_minutes\", lambda x: x[x > 0].mean() if (x > 0).any() else 0),\n",
    "            min_num_packages=(\"quantity_delivered\", \"min\"),\n",
    "            max_num_packages=(\"quantity_delivered\", \"max\"),\n",
    "            mean_num_packages=(\"quantity_delivered\", \"mean\")\n",
    "        )\n",
    "        .sort_values(\"weekday\")\n",
    "        .rename(columns={\"weekday_name\": \"weekday\"})\n",
    "        [[\"weekday\", \"min_minutes\", \"max_minutes\", \"mean_minutes\",\n",
    "          \"min_distance_m\", \"max_distance_m\", \"mean_distance_m\",\n",
    "          \"min_num_deliveries\", \"max_num_deliveries\", \"mean_num_deliveries\",\n",
    "          \"min_travel_time_min\", \"max_travel_time_min\", \"mean_travel_time_min\",\n",
    "          \"min_unloading_time_min\", \"max_unloading_time_min\", \"mean_unloading_time_min\",\n",
    "          \"n_days\", \"n_overtime_days\", \"max_overtime_minutes\", \"mean_overtime_minutes\",\n",
    "          \"min_num_packages\", \"max_num_packages\", \"mean_num_packages\"]]\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return stats_df, daily_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### prova per un singolo cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats_df, daily_results = single_cluster_stats_with_cache(\n",
    "#                 [3836, 3840, 3845, 3851, 3876, 3877, 3881, 3888, 3919, 4730, 4738, 4772, 4773, 4774, 4775, 4776, 4777, 4778, 4779, 4780, 4781, 4782, 4783, 4785, 4786, 4787, 6578, 6598, 6682, 6847, 6971, 6972, 7030, 7104, 7170, 7208, 7210, 7219, 7220, 7222, 7237, 7255, 7269, 7270, 7282, 7290, 7301, 7332, 7333, 7349, 7351, 7394, 7409, 7455, 7995, 9548, 9549, 9756, 12994, 14835, 14953, 16055, 16072, 16278, 16287],\n",
    "#                 3)\n",
    "\n",
    "# stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Metodo principale per il calcolo delle performances di un gruppo di clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def calc_clusters_stats(\n",
    "    clusters: list[list[int]],\n",
    "    time_limit: int = 3,\n",
    "    parallel: bool = False,\n",
    "    max_workers: int | None = None,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calcola le performances per più cluster e\n",
    "    ritorna un unico DataFrame con colonne:\n",
    "    ['cluster','weekday','min_minutes','max_minutes','mean_minutes',\n",
    "     'min_distance_m','max_distance_m','mean_distance_m','min_num_deliveries', ...]\n",
    "    dove ogni riga rappresenta (Cluster, Weekday).\n",
    "\n",
    "    Se desideri una riga sola per cluster aggregando su tutti i weekday,\n",
    "    vedi il blocco commentato alla fine.\n",
    "    \"\"\"\n",
    "    def _job(name: str, loc_ids: list[int]):\n",
    "        # Deduplica preservando ordine\n",
    "        loc_ids = list(dict.fromkeys(loc_ids))\n",
    "        stats_df, _daily_results = single_cluster_stats_with_cache(\n",
    "            cluster_location_ids=loc_ids,\n",
    "            time_limit=time_limit,\n",
    "            cache=None,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        # aggiunge il nome cluster\n",
    "        stats_df = stats_df.copy()\n",
    "        stats_df.insert(0, \"cluster\", name)\n",
    "        # rinomina eventuale colonna duplicata 'weekday' (stringa) in 'weekday_name' se necessario\n",
    "        cols = list(stats_df.columns)\n",
    "        if cols.count(\"weekday\") == 2:\n",
    "            # tipicamente stats_df ha ['weekday','min...'] già pulite; ma se ci fosse il nome,\n",
    "            # lo rinominiamo per chiarezza\n",
    "            stats_df.columns = [\"cluster\", \"weekday\", \"weekday_name\"] + cols[3:]\n",
    "        return stats_df\n",
    "\n",
    "    pieces = []\n",
    "    if parallel:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futs = []\n",
    "            for i, loc_ids in enumerate(clusters, start=1):\n",
    "                name = f\"Cluster {i}\"\n",
    "                futs.append(ex.submit(_job, name, loc_ids))\n",
    "            for f in as_completed(futs):\n",
    "                pieces.append(f.result())\n",
    "    else:\n",
    "        for i, loc_ids in enumerate(clusters, start=1):\n",
    "            name = f\"Cluster {i}\"\n",
    "            pieces.append(_job(name, loc_ids))\n",
    "\n",
    "    if not pieces:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    out = pd.concat(pieces, ignore_index=True)\n",
    "\n",
    "    # Ordina in modo leggibile\n",
    "    order_cols = [c for c in [\n",
    "        \"cluster\", \"weekday\", \"weekday_name\",\n",
    "        \"min_minutes\", \"max_minutes\", \"mean_minutes\",\n",
    "        \"min_distance_m\", \"max_distance_m\", \"mean_distance_m\",\n",
    "        \"min_num_deliveries\", \"max_num_deliveries\", \"mean_num_deliveries\",\n",
    "        \"min_travel_time_min\", \"max_travel_time_min\", \"mean_travel_time_min\",\n",
    "        \"min_unloading_time_min\", \"max_unloading_time_min\", \"mean_unloading_time_min\",\n",
    "        \"n_days\", \"n_overtime_days\", \"max_overtime_minutes\", \"mean_overtime_minutes\",\n",
    "        \"min_num_packages\", \"max_num_packages\", \"mean_num_packages\"\n",
    "    ] if c in out.columns]\n",
    "    out = out[order_cols].sort_values([\"cluster\", \"weekday\"]).reset_index(drop=True)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "# Calcolo performances ott-nov-dic (OND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def get_weekday_name(weekday_num):\n",
    "    return [\"Lunedì\",\"Martedì\",\"Mercoledì\",\"Giovedì\",\"Venerdì\",\"Sabato\",\"Domenica\"][weekday_num]\n",
    "\n",
    "def single_cluster_stats_with_cache_OND(\n",
    "    cluster_location_ids,\n",
    "    time_limit = 7,                   # time limit per ogni TSP giornaliero\n",
    "    cache: dict | None = None,        # cache riutilizzabile tra chiamate\n",
    "    # return_cache = False,             # se True ritorna anche la cache aggiornata\n",
    "    verbose: bool = False,            # per stampare i dati per debugging\n",
    "    max_workers: int | None = None,   # parallelismo: None=default\n",
    "):\n",
    "    route_cache = cache if cache is not None else {}\n",
    "\n",
    "    # 1) Filtra il delivery_data al cluster (assumi is_event già True e delivery_date già date)\n",
    "    dd = delivery_data[delivery_data['location_id'].isin(set(cluster_location_ids))].copy()\n",
    "    # Filtra per ordini solo a ottobre, novembre o dicembre\n",
    "    dd = dd[dd['delivery_date'].map(lambda d: d.month in {10, 11, 12})].copy()\n",
    "\n",
    "    # 2) Costruisci UNA SOLA VOLTA le matrici del cluster\n",
    "    with open('utility_memory/distance_matrix_OND.pkl', 'rb') as f:\n",
    "        full_distance_matrix = pickle.load(f)\n",
    "    distance_matrix_metri, time_mat_min, index_to_location_id, location_id_to_index = full_distance_matrix\n",
    "    full_mat_min = np.asarray(time_mat_min, dtype=float)          # minuti\n",
    "    full_mat_meters = np.asarray(distance_matrix_metri, dtype=float)  # metri\n",
    "    depot_global_index = 0  # depot sempre in posizione 0\n",
    "\n",
    "    # 3) Mappa date -> key (set ordinato di location_id); raccogli i set unici non in cache\n",
    "    date_to_key: dict = {}\n",
    "    unique_keys_needed: set = set()\n",
    "\n",
    "    for the_date, df_day in dd.groupby('delivery_date', sort=True):\n",
    "        todays_ids = df_day['location_id'].unique().tolist()\n",
    "        if not todays_ids:\n",
    "            continue\n",
    "        key = tuple(sorted(int(x) for x in todays_ids))\n",
    "        date_to_key[the_date] = key\n",
    "        if key not in route_cache:\n",
    "            unique_keys_needed.add(key)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Set unici da risolvere (non in cache): {len(unique_keys_needed)}\")\n",
    "\n",
    "    # 4) Prepara i job per i soli set mancanti (costruisci le sub-matrici una volta qui)\n",
    "    jobs = []\n",
    "    for key in unique_keys_needed:\n",
    "        day_idx_global = [int(location_id_to_index[lid]) for lid in key]\n",
    "        index_map = [depot_global_index] + day_idx_global\n",
    "        sub_mat_min = full_mat_min[np.ix_(index_map, index_map)]\n",
    "        sub_mat_meters = full_mat_meters[np.ix_(index_map, index_map)]\n",
    "        sub_index_to_location_id = [index_to_location_id[i] for i in index_map]\n",
    "\n",
    "        jobs.append({\n",
    "            \"key\": key,\n",
    "            \"sub_mat_meters\": sub_mat_meters.tolist(),\n",
    "            \"sub_mat_min\": sub_mat_min.tolist(),\n",
    "            \"sub_index_to_location_id\": sub_index_to_location_id\n",
    "        })\n",
    "\n",
    "    # 5) Esegui i TSP dei set unici in PARALLELO (thread)\n",
    "    futures = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        for job in jobs:\n",
    "            fut = ex.submit(\n",
    "                solve_tsp,\n",
    "                job[\"sub_mat_meters\"],\n",
    "                job[\"sub_mat_min\"],\n",
    "                job[\"sub_index_to_location_id\"],\n",
    "                time_limit,                      # secondi di time limit per il TSP giornaliero\n",
    "                10                                # minuti per consegna\n",
    "            )\n",
    "            futures.append((job[\"key\"], fut))\n",
    "\n",
    "        for key, fut in futures:\n",
    "            result = fut.result()  # se c’è errore, esplode qui (utile per debug)\n",
    "            route_cache[key] = {\n",
    "                \"total_delivery_time\": float(result[\"total_delivery_time\"]),\n",
    "                \"distance_m\": float(result[\"total_distance_m\"]),\n",
    "                \"num_deliveries\": result[\"num_deliveries\"],\n",
    "                \"travel_time_min\": float(result[\"travel_time_min\"]),\n",
    "                \"unloading_time_min\": float(result[\"unloading_time_min\"]),\n",
    "            }\n",
    "            if verbose:\n",
    "                print(f\"Set {key} risolto in parallelo -> {route_cache[key]['total_delivery_time']:.1f} min\")\n",
    "\n",
    "    # 6) Costruisci le righe giornaliere usando SEMPRE la cache (ora completa)\n",
    "    rows = []\n",
    "    for the_date, key in sorted(date_to_key.items(), key=lambda x: x[0]):\n",
    "        entry = route_cache[key]\n",
    "        rows.append({\n",
    "            \"delivery_date\": the_date,\n",
    "            \"weekday\": the_date.weekday(),\n",
    "            \"weekday_name\": get_weekday_name(the_date.weekday()),\n",
    "            \"total_delivery_time\": entry[\"total_delivery_time\"],\n",
    "            \"distance_m\": entry[\"distance_m\"],\n",
    "            \"num_deliveries\": entry[\"num_deliveries\"],\n",
    "            \"travel_time_min\": entry[\"travel_time_min\"],\n",
    "            \"unloading_time_min\": entry[\"unloading_time_min\"],\n",
    "            \"quantity_delivered\": dd[dd['delivery_date'] == the_date]['quantity'].sum()\n",
    "        })\n",
    "\n",
    "    if not rows:\n",
    "        return (None, None)\n",
    "\n",
    "    # 7) Output DataFrame ordinati\n",
    "    daily_results = (\n",
    "        pd.DataFrame(rows)\n",
    "        .sort_values(\"delivery_date\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Calcolare quante volte il fattorino supera le 8 ore (480 minuti) per giorno della settimana\n",
    "    daily_results['overtime'] = daily_results['total_delivery_time'] > shift_duration\n",
    "\n",
    "    # Calcolare la media del tempo extra (solo dove supera 8 ore) per weekday\n",
    "    daily_results['overtime_minutes'] = np.where(daily_results['overtime'], daily_results['total_delivery_time'] - shift_duration, 0)\n",
    "\n",
    "\n",
    "\n",
    "    stats_df = (\n",
    "        daily_results\n",
    "        .groupby([\"weekday\", \"weekday_name\"], as_index=False)\n",
    "        .agg(\n",
    "            min_minutes=(\"total_delivery_time\", \"min\"),\n",
    "            max_minutes=(\"total_delivery_time\", \"max\"),\n",
    "            mean_minutes=(\"total_delivery_time\", \"mean\"),\n",
    "            min_distance_m=(\"distance_m\", \"min\"),\n",
    "            max_distance_m=(\"distance_m\", \"max\"),\n",
    "            mean_distance_m=(\"distance_m\", \"mean\"),\n",
    "            min_num_deliveries=(\"num_deliveries\", \"min\"),\n",
    "            max_num_deliveries=(\"num_deliveries\", \"max\"),\n",
    "            mean_num_deliveries=(\"num_deliveries\", \"mean\"),\n",
    "            min_travel_time_min=(\"travel_time_min\", \"min\"),\n",
    "            max_travel_time_min=(\"travel_time_min\", \"max\"),\n",
    "            mean_travel_time_min=(\"travel_time_min\", \"mean\"),\n",
    "            min_unloading_time_min=(\"unloading_time_min\", \"min\"),\n",
    "            max_unloading_time_min=(\"unloading_time_min\", \"max\"),\n",
    "            mean_unloading_time_min=(\"unloading_time_min\", \"mean\"),\n",
    "            n_days=(\"total_delivery_time\", \"count\"),\n",
    "            n_overtime_days=(\"overtime\", \"sum\"),\n",
    "            max_overtime_minutes=(\"overtime_minutes\", \"max\"),\n",
    "            mean_overtime_minutes=(\"overtime_minutes\", lambda x: x[x > 0].mean() if (x > 0).any() else 0),\n",
    "            min_num_packages=(\"quantity_delivered\", \"min\"),\n",
    "            max_num_packages=(\"quantity_delivered\", \"max\"),\n",
    "            mean_num_packages=(\"quantity_delivered\", \"mean\")\n",
    "        )\n",
    "        .sort_values(\"weekday\")\n",
    "        .rename(columns={\"weekday_name\": \"weekday\"})\n",
    "        [[\"weekday\", \"min_minutes\", \"max_minutes\", \"mean_minutes\",\n",
    "          \"min_distance_m\", \"max_distance_m\", \"mean_distance_m\",\n",
    "          \"min_num_deliveries\", \"max_num_deliveries\", \"mean_num_deliveries\",\n",
    "          \"min_travel_time_min\", \"max_travel_time_min\", \"mean_travel_time_min\",\n",
    "          \"min_unloading_time_min\", \"max_unloading_time_min\", \"mean_unloading_time_min\",\n",
    "          \"n_days\", \"n_overtime_days\", \"max_overtime_minutes\", \"mean_overtime_minutes\",\n",
    "          \"min_num_packages\", \"max_num_packages\", \"mean_num_packages\"]]\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return stats_df, daily_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats_df, daily_results = single_cluster_stats_with_cache_OND(\n",
    "#                 [3836, 3840, 3845, 3851, 3876, 3877, 3881, 3888, 3919, 4730, 4738, 4772, 4773, 4774, 4775, 4776, 4777, 4778, 4779, 4780, 4781, 4782, 4783, 4785, 4786, 4787, 6578, 6598, 6682, 6847, 6971, 6972, 7030, 7104, 7170, 7208, 7210, 7219, 7220, 7222, 7237, 7255, 7269, 7270, 7282, 7290, 7301, 7332, 7333, 7349, 7351, 7394, 7409, 7455, 7995, 9548, 9549, 9756, 12994, 14835, 14953, 16055, 16072, 16278, 16287],\n",
    "#                 3)\n",
    "\n",
    "# stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def calc_clusters_stats_OND(\n",
    "    clusters: list[list[int]],\n",
    "    time_limit: int = 3,\n",
    "    parallel: bool = False,\n",
    "    max_workers: int | None = None,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calcola le performances per più cluster e\n",
    "    ritorna un unico DataFrame con colonne:\n",
    "    ['cluster','weekday','min_minutes','max_minutes','mean_minutes',\n",
    "     'min_distance_m','max_distance_m','mean_distance_m','min_num_deliveries', ...]\n",
    "    dove ogni riga rappresenta (Cluster, Weekday).\n",
    "\n",
    "    Se desideri una riga sola per cluster aggregando su tutti i weekday,\n",
    "    vedi il blocco commentato alla fine.\n",
    "    \"\"\"\n",
    "    def _job(name: str, loc_ids: list[int]):\n",
    "        # Deduplica preservando ordine\n",
    "        loc_ids = list(dict.fromkeys(loc_ids))\n",
    "        stats_df, _daily_results = single_cluster_stats_with_cache_OND(\n",
    "            cluster_location_ids=loc_ids,\n",
    "            time_limit=time_limit,\n",
    "            cache=None,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        # aggiunge il nome cluster\n",
    "        stats_df = stats_df.copy()\n",
    "        stats_df.insert(0, \"cluster\", name)\n",
    "        # rinomina eventuale colonna duplicata 'weekday' (stringa) in 'weekday_name' se necessario\n",
    "        cols = list(stats_df.columns)\n",
    "        if cols.count(\"weekday\") == 2:\n",
    "            # tipicamente stats_df ha ['weekday','min...'] già pulite; ma se ci fosse il nome,\n",
    "            # lo rinominiamo per chiarezza\n",
    "            stats_df.columns = [\"cluster\", \"weekday\", \"weekday_name\"] + cols[3:]\n",
    "        return stats_df\n",
    "\n",
    "    pieces = []\n",
    "    if parallel:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futs = []\n",
    "            for i, loc_ids in enumerate(clusters, start=1):\n",
    "                name = f\"Cluster {i}\"\n",
    "                futs.append(ex.submit(_job, name, loc_ids))\n",
    "            for f in as_completed(futs):\n",
    "                pieces.append(f.result())\n",
    "    else:\n",
    "        for i, loc_ids in enumerate(clusters, start=1):\n",
    "            name = f\"Cluster {i}\"\n",
    "            pieces.append(_job(name, loc_ids))\n",
    "\n",
    "    if not pieces:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    out = pd.concat(pieces, ignore_index=True)\n",
    "\n",
    "    # Ordina in modo leggibile\n",
    "    order_cols = [c for c in [\n",
    "        \"cluster\", \"weekday\", \"weekday_name\",\n",
    "        \"min_minutes\", \"max_minutes\", \"mean_minutes\",\n",
    "        \"min_distance_m\", \"max_distance_m\", \"mean_distance_m\",\n",
    "        \"min_num_deliveries\", \"max_num_deliveries\", \"mean_num_deliveries\",\n",
    "        \"min_travel_time_min\", \"max_travel_time_min\", \"mean_travel_time_min\",\n",
    "        \"min_unloading_time_min\", \"max_unloading_time_min\", \"mean_unloading_time_min\",\n",
    "        \"n_days\", \"n_overtime_days\", \"max_overtime_minutes\", \"mean_overtime_minutes\",\n",
    "        \"min_num_packages\", \"max_num_packages\", \"mean_num_packages\"\n",
    "    ] if c in out.columns]\n",
    "    out = out[order_cols].sort_values([\"cluster\", \"weekday\"]).reset_index(drop=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "# Calcolo performances agosto-settembre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def get_weekday_name(weekday_num):\n",
    "    return [\"Lunedì\",\"Martedì\",\"Mercoledì\",\"Giovedì\",\"Venerdì\",\"Sabato\",\"Domenica\"][weekday_num]\n",
    "\n",
    "def single_cluster_stats_with_cache_AS(\n",
    "    cluster_location_ids,\n",
    "    time_limit = 7,                   # time limit per ogni TSP giornaliero\n",
    "    cache: dict | None = None,        # cache riutilizzabile tra chiamate\n",
    "    # return_cache = False,             # se True ritorna anche la cache aggiornata\n",
    "    verbose: bool = False,            # per stampare i dati per debugging\n",
    "    max_workers: int | None = None,   # parallelismo: None=default\n",
    "):\n",
    "    route_cache = cache if cache is not None else {}\n",
    "\n",
    "    # 1) Filtra il delivery_data_AS al cluster (assumi is_event già True e delivery_date già date)\n",
    "    dd = delivery_data_AS[delivery_data_AS['location_id'].isin(set(cluster_location_ids))].copy()\n",
    "\n",
    "    # 2) Costruisci UNA SOLA VOLTA le matrici del cluster\n",
    "    with open('utility_memory/distance_matrix_AS.pkl', 'rb') as f:\n",
    "        distance_matrix_AS = pickle.load(f)\n",
    "    distance_matrix_metri, time_mat_min, index_to_location_id, location_id_to_index = distance_matrix_AS\n",
    "    full_mat_min = np.asarray(time_mat_min, dtype=float)          # minuti\n",
    "    full_mat_meters = np.asarray(distance_matrix_metri, dtype=float)  # metri\n",
    "    depot_global_index = 0  # depot sempre in posizione 0\n",
    "\n",
    "    # 3) Mappa date -> key (set ordinato di location_id); raccogli i set unici non in cache\n",
    "    date_to_key: dict = {}\n",
    "    unique_keys_needed: set = set()\n",
    "\n",
    "    for the_date, df_day in dd.groupby('delivery_date', sort=True):\n",
    "        todays_ids = df_day['location_id'].unique().tolist()\n",
    "        if not todays_ids:\n",
    "            continue\n",
    "        key = tuple(sorted(int(x) for x in todays_ids))\n",
    "        date_to_key[the_date] = key\n",
    "        if key not in route_cache:\n",
    "            unique_keys_needed.add(key)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Set unici da risolvere (non in cache): {len(unique_keys_needed)}\")\n",
    "\n",
    "    # 4) Prepara i job per i soli set mancanti (costruisci le sub-matrici una volta qui)\n",
    "    jobs = []\n",
    "    for key in unique_keys_needed:\n",
    "        day_idx_global = [int(location_id_to_index[lid]) for lid in key]\n",
    "        index_map = [depot_global_index] + day_idx_global\n",
    "        sub_mat_min = full_mat_min[np.ix_(index_map, index_map)]\n",
    "        sub_mat_meters = full_mat_meters[np.ix_(index_map, index_map)]\n",
    "        sub_index_to_location_id = [index_to_location_id[i] for i in index_map]\n",
    "\n",
    "        jobs.append({\n",
    "            \"key\": key,\n",
    "            \"sub_mat_meters\": sub_mat_meters.tolist(),\n",
    "            \"sub_mat_min\": sub_mat_min.tolist(),\n",
    "            \"sub_index_to_location_id\": sub_index_to_location_id\n",
    "        })\n",
    "\n",
    "    # 5) Esegui i TSP dei set unici in PARALLELO (thread)\n",
    "    futures = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        for job in jobs:\n",
    "            fut = ex.submit(\n",
    "                solve_tsp,\n",
    "                job[\"sub_mat_meters\"],\n",
    "                job[\"sub_mat_min\"],\n",
    "                job[\"sub_index_to_location_id\"],\n",
    "                time_limit,                      # secondi di time limit per il TSP giornaliero\n",
    "                10                                # minuti per consegna\n",
    "            )\n",
    "            futures.append((job[\"key\"], fut))\n",
    "\n",
    "        for key, fut in futures:\n",
    "            result = fut.result()  # se c’è errore, esplode qui (utile per debug)\n",
    "            route_cache[key] = {\n",
    "                \"total_delivery_time\": float(result[\"total_delivery_time\"]),\n",
    "                \"distance_m\": float(result[\"total_distance_m\"]),\n",
    "                \"num_deliveries\": result[\"num_deliveries\"],\n",
    "                \"travel_time_min\": float(result[\"travel_time_min\"]),\n",
    "                \"unloading_time_min\": float(result[\"unloading_time_min\"]),\n",
    "            }\n",
    "            if verbose:\n",
    "                print(f\"Set {key} risolto in parallelo -> {route_cache[key]['total_delivery_time']:.1f} min\")\n",
    "\n",
    "    # 6) Costruisci le righe giornaliere usando SEMPRE la cache (ora completa)\n",
    "    rows = []\n",
    "    for the_date, key in sorted(date_to_key.items(), key=lambda x: x[0]):\n",
    "        entry = route_cache[key]\n",
    "        rows.append({\n",
    "            \"delivery_date\": the_date,\n",
    "            \"weekday\": the_date.weekday(),\n",
    "            \"weekday_name\": get_weekday_name(the_date.weekday()),\n",
    "            \"total_delivery_time\": entry[\"total_delivery_time\"],\n",
    "            \"distance_m\": entry[\"distance_m\"],\n",
    "            \"num_deliveries\": entry[\"num_deliveries\"],\n",
    "            \"travel_time_min\": entry[\"travel_time_min\"],\n",
    "            \"unloading_time_min\": entry[\"unloading_time_min\"],\n",
    "            \"quantity_delivered\": dd[dd['delivery_date'] == the_date]['quantity'].sum()\n",
    "        })\n",
    "\n",
    "    if not rows:\n",
    "        return (None, None)\n",
    "\n",
    "    # 7) Output DataFrame ordinati\n",
    "    daily_results = (\n",
    "        pd.DataFrame(rows)\n",
    "        .sort_values(\"delivery_date\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Calcolare quante volte il fattorino supera le 8 ore (480 minuti) per giorno della settimana\n",
    "    daily_results['overtime'] = daily_results['total_delivery_time'] > shift_duration\n",
    "\n",
    "    # Calcolare la media del tempo extra (solo dove supera 8 ore) per weekday\n",
    "    daily_results['overtime_minutes'] = np.where(daily_results['overtime'], daily_results['total_delivery_time'] - shift_duration, 0)\n",
    "\n",
    "\n",
    "\n",
    "    stats_df = (\n",
    "        daily_results\n",
    "        .groupby([\"weekday\", \"weekday_name\"], as_index=False)\n",
    "        .agg(\n",
    "            min_minutes=(\"total_delivery_time\", \"min\"),\n",
    "            max_minutes=(\"total_delivery_time\", \"max\"),\n",
    "            mean_minutes=(\"total_delivery_time\", \"mean\"),\n",
    "            min_distance_m=(\"distance_m\", \"min\"),\n",
    "            max_distance_m=(\"distance_m\", \"max\"),\n",
    "            mean_distance_m=(\"distance_m\", \"mean\"),\n",
    "            min_num_deliveries=(\"num_deliveries\", \"min\"),\n",
    "            max_num_deliveries=(\"num_deliveries\", \"max\"),\n",
    "            mean_num_deliveries=(\"num_deliveries\", \"mean\"),\n",
    "            min_travel_time_min=(\"travel_time_min\", \"min\"),\n",
    "            max_travel_time_min=(\"travel_time_min\", \"max\"),\n",
    "            mean_travel_time_min=(\"travel_time_min\", \"mean\"),\n",
    "            min_unloading_time_min=(\"unloading_time_min\", \"min\"),\n",
    "            max_unloading_time_min=(\"unloading_time_min\", \"max\"),\n",
    "            mean_unloading_time_min=(\"unloading_time_min\", \"mean\"),\n",
    "            n_days=(\"total_delivery_time\", \"count\"),\n",
    "            n_overtime_days=(\"overtime\", \"sum\"),\n",
    "            max_overtime_minutes=(\"overtime_minutes\", \"max\"),\n",
    "            mean_overtime_minutes=(\"overtime_minutes\", lambda x: x[x > 0].mean() if (x > 0).any() else 0),\n",
    "            min_num_packages=(\"quantity_delivered\", \"min\"),\n",
    "            max_num_packages=(\"quantity_delivered\", \"max\"),\n",
    "            mean_num_packages=(\"quantity_delivered\", \"mean\")\n",
    "        )\n",
    "        .sort_values(\"weekday\")\n",
    "        .rename(columns={\"weekday_name\": \"weekday\"})\n",
    "        [[\"weekday\", \"min_minutes\", \"max_minutes\", \"mean_minutes\",\n",
    "          \"min_distance_m\", \"max_distance_m\", \"mean_distance_m\",\n",
    "          \"min_num_deliveries\", \"max_num_deliveries\", \"mean_num_deliveries\",\n",
    "          \"min_travel_time_min\", \"max_travel_time_min\", \"mean_travel_time_min\",\n",
    "          \"min_unloading_time_min\", \"max_unloading_time_min\", \"mean_unloading_time_min\",\n",
    "          \"n_days\", \"n_overtime_days\", \"max_overtime_minutes\", \"mean_overtime_minutes\",\n",
    "          \"min_num_packages\", \"max_num_packages\", \"mean_num_packages\"]]\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return stats_df, daily_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def calc_clusters_stats_AS(\n",
    "    clusters: list[list[int]],\n",
    "    time_limit: int = 3,\n",
    "    parallel: bool = False,\n",
    "    max_workers: int | None = None,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calcola le performances per più cluster e\n",
    "    ritorna un unico DataFrame con colonne:\n",
    "    ['cluster','weekday','min_minutes','max_minutes','mean_minutes',\n",
    "     'min_distance_m','max_distance_m','mean_distance_m','min_num_deliveries', ...]\n",
    "    dove ogni riga rappresenta (Cluster, Weekday).\n",
    "\n",
    "    Se desideri una riga sola per cluster aggregando su tutti i weekday,\n",
    "    vedi il blocco commentato alla fine.\n",
    "    \"\"\"\n",
    "    def _job(name: str, loc_ids: list[int]):\n",
    "        # Deduplica preservando ordine\n",
    "        loc_ids = list(dict.fromkeys(loc_ids))\n",
    "        stats_df, _daily_results = single_cluster_stats_with_cache_AS(\n",
    "            cluster_location_ids=loc_ids,\n",
    "            time_limit=time_limit,\n",
    "            cache=None,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        # aggiunge il nome cluster\n",
    "        stats_df = stats_df.copy()\n",
    "        stats_df.insert(0, \"cluster\", name)\n",
    "        # rinomina eventuale colonna duplicata 'weekday' (stringa) in 'weekday_name' se necessario\n",
    "        cols = list(stats_df.columns)\n",
    "        if cols.count(\"weekday\") == 2:\n",
    "            # tipicamente stats_df ha ['weekday','min...'] già pulite; ma se ci fosse il nome,\n",
    "            # lo rinominiamo per chiarezza\n",
    "            stats_df.columns = [\"cluster\", \"weekday\", \"weekday_name\"] + cols[3:]\n",
    "        return stats_df\n",
    "\n",
    "    pieces = []\n",
    "    if parallel:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futs = []\n",
    "            for i, loc_ids in enumerate(clusters, start=1):\n",
    "                name = f\"Cluster {i}\"\n",
    "                futs.append(ex.submit(_job, name, loc_ids))\n",
    "            for f in as_completed(futs):\n",
    "                pieces.append(f.result())\n",
    "    else:\n",
    "        for i, loc_ids in enumerate(clusters, start=1):\n",
    "            name = f\"Cluster {i}\"\n",
    "            pieces.append(_job(name, loc_ids))\n",
    "\n",
    "    if not pieces:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    out = pd.concat(pieces, ignore_index=True)\n",
    "\n",
    "    # Ordina in modo leggibile\n",
    "    order_cols = [c for c in [\n",
    "        \"cluster\", \"weekday\", \"weekday_name\",\n",
    "        \"min_minutes\", \"max_minutes\", \"mean_minutes\",\n",
    "        \"min_distance_m\", \"max_distance_m\", \"mean_distance_m\",\n",
    "        \"min_num_deliveries\", \"max_num_deliveries\", \"mean_num_deliveries\",\n",
    "        \"min_travel_time_min\", \"max_travel_time_min\", \"mean_travel_time_min\",\n",
    "        \"min_unloading_time_min\", \"max_unloading_time_min\", \"mean_unloading_time_min\",\n",
    "        \"n_days\", \"n_overtime_days\", \"max_overtime_minutes\", \"mean_overtime_minutes\",\n",
    "        \"min_num_packages\", \"max_num_packages\", \"mean_num_packages\"\n",
    "    ] if c in out.columns]\n",
    "    out = out[order_cols].sort_values([\"cluster\", \"weekday\"]).reset_index(drop=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "# Calcolo performances ottobre-novembre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def get_weekday_name(weekday_num):\n",
    "    return [\"Lunedì\",\"Martedì\",\"Mercoledì\",\"Giovedì\",\"Venerdì\",\"Sabato\",\"Domenica\"][weekday_num]\n",
    "\n",
    "def single_cluster_stats_with_cache_ON(\n",
    "    cluster_location_ids,\n",
    "    time_limit = 7,                   # time limit per ogni TSP giornaliero\n",
    "    cache: dict | None = None,        # cache riutilizzabile tra chiamate\n",
    "    # return_cache = False,             # se True ritorna anche la cache aggiornata\n",
    "    verbose: bool = False,            # per stampare i dati per debugging\n",
    "    max_workers: int | None = None,   # parallelismo: None=default\n",
    "):\n",
    "    route_cache = cache if cache is not None else {}\n",
    "\n",
    "    # 1) Filtra il delivery_data_ON al cluster (assumi is_event già True e delivery_date già date)\n",
    "    dd = delivery_data_ON[delivery_data_ON['location_id'].isin(set(cluster_location_ids))].copy()\n",
    "\n",
    "    # 2) Costruisci UNA SOLA VOLTA le matrici del cluster\n",
    "    with open('utility_memory/distance_matrix_ON.pkl', 'rb') as f:\n",
    "        distance_matrix_ON = pickle.load(f)\n",
    "    distance_matrix_metri, time_mat_min, index_to_location_id, location_id_to_index = distance_matrix_ON\n",
    "    full_mat_min = np.asarray(time_mat_min, dtype=float)          # minuti\n",
    "    full_mat_meters = np.asarray(distance_matrix_metri, dtype=float)  # metri\n",
    "    depot_global_index = 0  # depot sempre in posizione 0\n",
    "\n",
    "    # 3) Mappa date -> key (set ordinato di location_id); raccogli i set unici non in cache\n",
    "    date_to_key: dict = {}\n",
    "    unique_keys_needed: set = set()\n",
    "\n",
    "    for the_date, df_day in dd.groupby('delivery_date', sort=True):\n",
    "        todays_ids = df_day['location_id'].unique().tolist()\n",
    "        if not todays_ids:\n",
    "            continue\n",
    "        key = tuple(sorted(int(x) for x in todays_ids))\n",
    "        date_to_key[the_date] = key\n",
    "        if key not in route_cache:\n",
    "            unique_keys_needed.add(key)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Set unici da risolvere (non in cache): {len(unique_keys_needed)}\")\n",
    "\n",
    "    # 4) Prepara i job per i soli set mancanti (costruisci le sub-matrici una volta qui)\n",
    "    jobs = []\n",
    "    for key in unique_keys_needed:\n",
    "        day_idx_global = [int(location_id_to_index[lid]) for lid in key]\n",
    "        index_map = [depot_global_index] + day_idx_global\n",
    "        sub_mat_min = full_mat_min[np.ix_(index_map, index_map)]\n",
    "        sub_mat_meters = full_mat_meters[np.ix_(index_map, index_map)]\n",
    "        sub_index_to_location_id = [index_to_location_id[i] for i in index_map]\n",
    "\n",
    "        jobs.append({\n",
    "            \"key\": key,\n",
    "            \"sub_mat_meters\": sub_mat_meters.tolist(),\n",
    "            \"sub_mat_min\": sub_mat_min.tolist(),\n",
    "            \"sub_index_to_location_id\": sub_index_to_location_id\n",
    "        })\n",
    "\n",
    "    # 5) Esegui i TSP dei set unici in PARALLELO (thread)\n",
    "    futures = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        for job in jobs:\n",
    "            fut = ex.submit(\n",
    "                solve_tsp,\n",
    "                job[\"sub_mat_meters\"],\n",
    "                job[\"sub_mat_min\"],\n",
    "                job[\"sub_index_to_location_id\"],\n",
    "                time_limit,                      # secondi di time limit per il TSP giornaliero\n",
    "                10                                # minuti per consegna\n",
    "            )\n",
    "            futures.append((job[\"key\"], fut))\n",
    "\n",
    "        for key, fut in futures:\n",
    "            result = fut.result()  # se c’è errore, esplode qui (utile per debug)\n",
    "            route_cache[key] = {\n",
    "                \"total_delivery_time\": float(result[\"total_delivery_time\"]),\n",
    "                \"distance_m\": float(result[\"total_distance_m\"]),\n",
    "                \"num_deliveries\": result[\"num_deliveries\"],\n",
    "                \"travel_time_min\": float(result[\"travel_time_min\"]),\n",
    "                \"unloading_time_min\": float(result[\"unloading_time_min\"]),\n",
    "            }\n",
    "            if verbose:\n",
    "                print(f\"Set {key} risolto in parallelo -> {route_cache[key]['total_delivery_time']:.1f} min\")\n",
    "\n",
    "    # 6) Costruisci le righe giornaliere usando SEMPRE la cache (ora completa)\n",
    "    rows = []\n",
    "    for the_date, key in sorted(date_to_key.items(), key=lambda x: x[0]):\n",
    "        entry = route_cache[key]\n",
    "        rows.append({\n",
    "            \"delivery_date\": the_date,\n",
    "            \"weekday\": the_date.weekday(),\n",
    "            \"weekday_name\": get_weekday_name(the_date.weekday()),\n",
    "            \"total_delivery_time\": entry[\"total_delivery_time\"],\n",
    "            \"distance_m\": entry[\"distance_m\"],\n",
    "            \"num_deliveries\": entry[\"num_deliveries\"],\n",
    "            \"travel_time_min\": entry[\"travel_time_min\"],\n",
    "            \"unloading_time_min\": entry[\"unloading_time_min\"],\n",
    "            \"quantity_delivered\": dd[dd['delivery_date'] == the_date]['quantity'].sum()\n",
    "        })\n",
    "\n",
    "    if not rows:\n",
    "        return (None, None)\n",
    "\n",
    "    # 7) Output DataFrame ordinati\n",
    "    daily_results = (\n",
    "        pd.DataFrame(rows)\n",
    "        .sort_values(\"delivery_date\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Calcolare quante volte il fattorino supera le 8 ore (480 minuti) per giorno della settimana\n",
    "    daily_results['overtime'] = daily_results['total_delivery_time'] > shift_duration\n",
    "\n",
    "    # Calcolare la media del tempo extra (solo dove supera 8 ore) per weekday\n",
    "    daily_results['overtime_minutes'] = np.where(daily_results['overtime'], daily_results['total_delivery_time'] - shift_duration, 0)\n",
    "\n",
    "\n",
    "\n",
    "    stats_df = (\n",
    "        daily_results\n",
    "        .groupby([\"weekday\", \"weekday_name\"], as_index=False)\n",
    "        .agg(\n",
    "            min_minutes=(\"total_delivery_time\", \"min\"),\n",
    "            max_minutes=(\"total_delivery_time\", \"max\"),\n",
    "            mean_minutes=(\"total_delivery_time\", \"mean\"),\n",
    "            min_distance_m=(\"distance_m\", \"min\"),\n",
    "            max_distance_m=(\"distance_m\", \"max\"),\n",
    "            mean_distance_m=(\"distance_m\", \"mean\"),\n",
    "            min_num_deliveries=(\"num_deliveries\", \"min\"),\n",
    "            max_num_deliveries=(\"num_deliveries\", \"max\"),\n",
    "            mean_num_deliveries=(\"num_deliveries\", \"mean\"),\n",
    "            min_travel_time_min=(\"travel_time_min\", \"min\"),\n",
    "            max_travel_time_min=(\"travel_time_min\", \"max\"),\n",
    "            mean_travel_time_min=(\"travel_time_min\", \"mean\"),\n",
    "            min_unloading_time_min=(\"unloading_time_min\", \"min\"),\n",
    "            max_unloading_time_min=(\"unloading_time_min\", \"max\"),\n",
    "            mean_unloading_time_min=(\"unloading_time_min\", \"mean\"),\n",
    "            n_days=(\"total_delivery_time\", \"count\"),\n",
    "            n_overtime_days=(\"overtime\", \"sum\"),\n",
    "            max_overtime_minutes=(\"overtime_minutes\", \"max\"),\n",
    "            mean_overtime_minutes=(\"overtime_minutes\", lambda x: x[x > 0].mean() if (x > 0).any() else 0),\n",
    "            min_num_packages=(\"quantity_delivered\", \"min\"),\n",
    "            max_num_packages=(\"quantity_delivered\", \"max\"),\n",
    "            mean_num_packages=(\"quantity_delivered\", \"mean\")\n",
    "        )\n",
    "        .sort_values(\"weekday\")\n",
    "        .rename(columns={\"weekday_name\": \"weekday\"})\n",
    "        [[\"weekday\", \"min_minutes\", \"max_minutes\", \"mean_minutes\",\n",
    "          \"min_distance_m\", \"max_distance_m\", \"mean_distance_m\",\n",
    "          \"min_num_deliveries\", \"max_num_deliveries\", \"mean_num_deliveries\",\n",
    "          \"min_travel_time_min\", \"max_travel_time_min\", \"mean_travel_time_min\",\n",
    "          \"min_unloading_time_min\", \"max_unloading_time_min\", \"mean_unloading_time_min\",\n",
    "          \"n_days\", \"n_overtime_days\", \"max_overtime_minutes\", \"mean_overtime_minutes\",\n",
    "          \"min_num_packages\", \"max_num_packages\", \"mean_num_packages\"]]\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return stats_df, daily_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def calc_clusters_stats_ON(\n",
    "    clusters: list[list[int]],\n",
    "    time_limit: int = 3,\n",
    "    parallel: bool = False,\n",
    "    max_workers: int | None = None,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calcola le performances per più cluster e\n",
    "    ritorna un unico DataFrame con colonne:\n",
    "    ['cluster','weekday','min_minutes','max_minutes','mean_minutes',\n",
    "     'min_distance_m','max_distance_m','mean_distance_m','min_num_deliveries', ...]\n",
    "    dove ogni riga rappresenta (Cluster, Weekday).\n",
    "\n",
    "    Se desideri una riga sola per cluster aggregando su tutti i weekday,\n",
    "    vedi il blocco commentato alla fine.\n",
    "    \"\"\"\n",
    "    def _job(name: str, loc_ids: list[int]):\n",
    "        # Deduplica preservando ordine\n",
    "        loc_ids = list(dict.fromkeys(loc_ids))\n",
    "        stats_df, _daily_results = single_cluster_stats_with_cache_ON(\n",
    "            cluster_location_ids=loc_ids,\n",
    "            time_limit=time_limit,\n",
    "            cache=None,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        # aggiunge il nome cluster\n",
    "        stats_df = stats_df.copy()\n",
    "        stats_df.insert(0, \"cluster\", name)\n",
    "        # rinomina eventuale colonna duplicata 'weekday' (stringa) in 'weekday_name' se necessario\n",
    "        cols = list(stats_df.columns)\n",
    "        if cols.count(\"weekday\") == 2:\n",
    "            # tipicamente stats_df ha ['weekday','min...'] già pulite; ma se ci fosse il nome,\n",
    "            # lo rinominiamo per chiarezza\n",
    "            stats_df.columns = [\"cluster\", \"weekday\", \"weekday_name\"] + cols[3:]\n",
    "        return stats_df\n",
    "\n",
    "    pieces = []\n",
    "    if parallel:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futs = []\n",
    "            for i, loc_ids in enumerate(clusters, start=1):\n",
    "                name = f\"Cluster {i}\"\n",
    "                futs.append(ex.submit(_job, name, loc_ids))\n",
    "            for f in as_completed(futs):\n",
    "                pieces.append(f.result())\n",
    "    else:\n",
    "        for i, loc_ids in enumerate(clusters, start=1):\n",
    "            name = f\"Cluster {i}\"\n",
    "            pieces.append(_job(name, loc_ids))\n",
    "\n",
    "    if not pieces:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    out = pd.concat(pieces, ignore_index=True)\n",
    "\n",
    "    # Ordina in modo leggibile\n",
    "    order_cols = [c for c in [\n",
    "        \"cluster\", \"weekday\", \"weekday_name\",\n",
    "        \"min_minutes\", \"max_minutes\", \"mean_minutes\",\n",
    "        \"min_distance_m\", \"max_distance_m\", \"mean_distance_m\",\n",
    "        \"min_num_deliveries\", \"max_num_deliveries\", \"mean_num_deliveries\",\n",
    "        \"min_travel_time_min\", \"max_travel_time_min\", \"mean_travel_time_min\",\n",
    "        \"min_unloading_time_min\", \"max_unloading_time_min\", \"mean_unloading_time_min\",\n",
    "        \"n_days\", \"n_overtime_days\", \"max_overtime_minutes\", \"mean_overtime_minutes\",\n",
    "        \"min_num_packages\", \"max_num_packages\", \"mean_num_packages\"\n",
    "    ] if c in out.columns]\n",
    "    out = out[order_cols].sort_values([\"cluster\", \"weekday\"]).reset_index(drop=True)\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Importazione pacchetti e dati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## importare i pacchetti e i dati che servono: att run anche otto-nov-dic se si vogliono solo quei dati altrimenti è full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Carica i dati\n",
    "# -----------------------------\n",
    "delivery_data = pd.read_csv('delivery_history.csv')\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Rendiamo is_event un booleano sicuro\n",
    "# -----------------------------\n",
    "# Controlliamo che is_event abbia solo valori convertibili in booleano\n",
    "valid_bool_mask = delivery_data['is_event'].isin([0, 1, True, False])\n",
    "invalid_is_event = delivery_data.loc[~valid_bool_mask, 'is_event'].unique()\n",
    "print(\"\\n❌ Valori non convertibili in booleani in 'is_event':\", invalid_is_event)\n",
    "\n",
    "# Convertiamo solo i valori validi a booleano\n",
    "delivery_data.loc[valid_bool_mask, 'is_event'] = delivery_data.loc[valid_bool_mask, 'is_event'].astype(bool)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Escludiamo righe dove is_event non è True\n",
    "# -----------------------------\n",
    "delivery_data = delivery_data[delivery_data['is_event'] == True].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Filtra coordinate latitudine e longitudine (escludi points fuori range)\n",
    "# -----------------------------\n",
    "delivery_data = delivery_data[\n",
    "    (delivery_data['lon'] >= 5) & (delivery_data['lon'] <= 12.5)\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Escludiamo sabati e domeniche\n",
    "# Supponendo che ci sia una colonna datetime 'date' o simile\n",
    "# Se non c'è, bisognerebbe creare o specificare come avere il campo data\n",
    "# -----------------------------\n",
    "# Convertiamo la colonna data in datetime\n",
    "delivery_data['delivery_date'] = pd.to_datetime(delivery_data['delivery_date'], errors='coerce')\n",
    "\n",
    "# Drop righe con date non valide\n",
    "delivery_data = delivery_data.dropna(subset=['delivery_date'])\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "# Prendiamo solo i giorni della settimana da lun (0) a ven (4)\n",
    "delivery_data = delivery_data[delivery_data['delivery_date'].dt.dayofweek < 5]\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Elimina duplicati per location_id lasciandone uno solo\n",
    "# -----------------------------\n",
    "delivery_points = delivery_data[['location_id', 'lat', 'lon']].drop_duplicates(subset='location_id').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Stampa info per verifica\n",
    "print(f\"\\nDataset preprocessato: {len(delivery_points)} punti univoci dopo filtri e rimozione weekend filtrando solo per ottobre, novembre e dicembre.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## run questo sollo se si vuole i punti ottobre novembre dicembre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 6. Filtra solo ottobre,novembre,dicembre\n",
    "\n",
    "# Filtra solo i mesi ottobre, novembre e dicembre\n",
    "delivery_data = delivery_data[delivery_data['delivery_date'].dt.month.isin([10, 11, 12])]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Elimina duplicati per location_id lasciandone uno solo\n",
    "# -----------------------------\n",
    "delivery_points = delivery_data[['location_id', 'lat', 'lon']].drop_duplicates(subset='location_id').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Stampa info per verifica\n",
    "print(f\"\\nDataset preprocessato: {len(delivery_points)} punti univoci dopo filtri e rimozione weekend filtrando solo per ottobre, novembre e dicembre.\")\n",
    "\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Run questo se si vuole i soli punti di Agosto-Settembre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 6. Filtra solo Agosto-settembre\n",
    "\n",
    "# Filtra solo i mesi ottobre, novembre e dicembre\n",
    "delivery_data = delivery_data[delivery_data['delivery_date'].dt.month.isin([8,9])]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Elimina duplicati per location_id lasciandone uno solo\n",
    "# -----------------------------\n",
    "delivery_points = delivery_data[['location_id', 'lat', 'lon']].drop_duplicates(subset='location_id').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Stampa info per verifica\n",
    "print(f\"\\nDataset preprocessato: {len(delivery_points)} punti univoci dopo filtri e rimozione weekend filtrando solo per ottobre, novembre e dicembre.\")\n",
    "\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## run questo per i soli punti di ottobre-novembre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 6. Filtra solo Ottobre-Novembre\n",
    "\n",
    "# Filtra solo i mesi ottobre, novembre e dicembre\n",
    "delivery_data = delivery_data[delivery_data['delivery_date'].dt.month.isin([10,11])]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Elimina duplicati per location_id lasciandone uno solo\n",
    "# -----------------------------\n",
    "delivery_points = delivery_data[['location_id', 'lat', 'lon']].drop_duplicates(subset='location_id').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Stampa info per verifica\n",
    "print(f\"\\nDataset preprocessato: {len(delivery_points)} punti univoci dopo filtri e rimozione weekend filtrando solo per ottobre, novembre e dicembre.\")\n",
    "\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Analisi punti simili"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Metriche dati: location_id, freq & order_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = (\n",
    "    delivery_data.groupby('location_id')\n",
    "    .agg(\n",
    "        freq=('delivery_date', 'count'),\n",
    "        order_days=('delivery_date', lambda x: tuple(sorted(x.dt.dayofweek.unique())))\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(metrics_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# Codice per unire i punti con caratteristiche uguali"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Matrice delle distante in tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix_creation_no_depot(delivery_points):\n",
    "    \"\"\"\n",
    "    Calcola la matrice NxN di tempi in minuti tra tutti i punti di consegna (senza deposito).\n",
    "    \n",
    "    Parameters:\n",
    "    - delivery_points: DataFrame con ['location_id', 'lat', 'lon']\n",
    "    \n",
    "    Returns:\n",
    "    - distance_matrix_metri: matrice NxN distanze in metri (int)\n",
    "    - time_mat_min: matrice NxN tempi di percorrenza in minuti (int)\n",
    "    - index_to_location_id: lista indice->location_id\n",
    "    - location_id_to_index: dict location_id->indice\n",
    "    \"\"\"\n",
    "\n",
    "    points = delivery_points[['location_id', 'lat', 'lon']].reset_index(drop=True)\n",
    "\n",
    "    R = 6371000.0  # raggio terrestre medio in metri\n",
    "    lat = np.radians(points['lat'].values)\n",
    "    lon = np.radians(points['lon'].values)\n",
    "\n",
    "    dlat = lat[:, None] - lat[None, :]\n",
    "    dlon = lon[:, None] - lon[None, :]\n",
    "\n",
    "    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat)[:, None] * np.cos(lat)[None, :] * np.sin(dlon / 2.0) ** 2\n",
    "    c = 2.0 * np.arctan2(np.sqrt(a), np.sqrt(1.0 - a))\n",
    "    distance_matrix_metri = (R * c).astype(int)\n",
    "\n",
    "    dist_km = distance_matrix_metri.astype(float) / 1000.0\n",
    "\n",
    "    speed_kmh = np.full_like(dist_km, 50.0)\n",
    "    speed_kmh[dist_km <= 5.0] = 30.0\n",
    "    speed_kmh[dist_km > 20.0] = 70.0\n",
    "\n",
    "    np.fill_diagonal(speed_kmh, 1.0)  # evitare zero sulla diagonale\n",
    "\n",
    "    time_min = (dist_km / speed_kmh) * 60.0\n",
    "    np.fill_diagonal(time_min, 0.0)\n",
    "\n",
    "    time_mat_min = np.ceil(time_min).astype(int)\n",
    "\n",
    "    index_to_location_id = points['location_id'].tolist()\n",
    "    location_id_to_index = {int(lid): i for i, lid in enumerate(points['location_id'])}\n",
    "\n",
    "    return distance_matrix_metri, time_mat_min, index_to_location_id, location_id_to_index\n",
    "\n",
    "\n",
    "# Supponendo di avere il DataFrame delivery_points con tutti i punti e colonne ['location_id', 'lat', 'lon']\n",
    "\n",
    "# Calcolo matrice tempi e distanze\n",
    "distance_matrix_metri, time_mat_min, index_to_location_id, location_id_to_index = distance_matrix_creation_no_depot(delivery_points)\n",
    "\n",
    "# QUI si crea il mapping location_id -> indice rapidamente da usare nelle funzioni di clustering\n",
    "location_id_to_index = {lid: idx for idx, lid in enumerate(index_to_location_id)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Calcolo della densità dei vari punti con divisione in base al quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Prepara i dati per analisi spaziale\n",
    "# -----------------------------\n",
    "# Trasformiamo i tuoi punti in un GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    delivery_points,\n",
    "    geometry=gpd.points_from_xy(delivery_points[\"lon\"], delivery_points[\"lat\"]),\n",
    "    crs=\"EPSG:4326\"   # lat/lon WGS84\n",
    ")\n",
    "\n",
    "# Convertiamo in metri (Web Mercator) per calcolare distanze reali\n",
    "gdf = gdf.to_crs(epsg=3857)\n",
    "\n",
    "# Estraggo coordinate in array\n",
    "coords = np.array([(geom.x, geom.y) for geom in gdf.geometry])\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Calcolo densità locale con BallTree\n",
    "# -----------------------------\n",
    "# Costruisco albero per ricerche veloci\n",
    "tree = BallTree(coords, leaf_size=15, metric=\"euclidean\")\n",
    "\n",
    "# Raggio di vicinanza in metri (puoi modificarlo: es. 500 m)\n",
    "raggio = 500  \n",
    "\n",
    "# Conta quanti punti ci sono entro il raggio per ciascun punto\n",
    "densita = tree.query_radius(coords, r=raggio, count_only=True)\n",
    "\n",
    "# Aggiungo la colonna densità al GeoDataFrame\n",
    "gdf[\"densita\"] = densita\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Classificazione bassa/media/alta densità\n",
    "# -----------------------------\n",
    "# Calcoliamo i quantili 33% e 66%\n",
    "q1 = np.quantile(densita, 0.33)\n",
    "q2 = np.quantile(densita, 0.66)\n",
    "\n",
    "def classifica_densita(val):\n",
    "    if val <= q1:\n",
    "        return \"bassa densità\"\n",
    "    elif val <= q2:\n",
    "        return \"media densità\"\n",
    "    else:\n",
    "        return \"alta densità\"\n",
    "\n",
    "gdf[\"classe\"] = gdf[\"densita\"].apply(classifica_densita)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Salvataggio / output\n",
    "# -----------------------------\n",
    "# Torniamo a EPSG:4326 per lat/lon\n",
    "gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "# Esportiamo risultati\n",
    "risultati = gdf[[\"location_id\", \"lat\", \"lon\", \"densita\", \"classe\"]]\n",
    "risultati.to_csv(\"delivery_points_densita.csv\", index=False)\n",
    "\n",
    "print(\"✅ Calcolo completato!\")\n",
    "print(risultati.head())\n",
    "print(f\"\\nSoglie densità usate: q1={q1}, q2={q2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Aggregazione dei punti tramite centroide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_points_by_day_tuples_centroid(metrics_df, delivery_data, time_mat_min, location_id_to_index, density_classes):\n",
    "    \"\"\"\n",
    "    Clustering con soglie di tempo diverse in base alla densità locale.\n",
    "\n",
    "    density_classes: dict {location_id: 'bassa densità' | 'media densità' | 'alta densità'}\n",
    "    \"\"\"\n",
    "\n",
    "    # Soglie per ciascuna classe di densità (in minuti)\n",
    "    thresholds = {\n",
    "        \"bassa densità\": 2,\n",
    "        \"media densità\": 0.8,\n",
    "        \"alta densità\": 0.2\n",
    "    }\n",
    "\n",
    "    cluster_list = []\n",
    "    points_assigned = set()\n",
    "\n",
    "    # Raggruppo i punti per giorno/i\n",
    "    day_groups = metrics_df.groupby('order_days')['location_id'].apply(list)\n",
    "\n",
    "    for day_tuple, loc_ids in day_groups.items():\n",
    "        loc_ids_set = set(loc_ids)\n",
    "        not_assigned = loc_ids_set - points_assigned\n",
    "\n",
    "        while not_assigned:\n",
    "            start_point = not_assigned.pop()\n",
    "            cluster = {start_point}\n",
    "\n",
    "            def compute_centroid(cluster):\n",
    "                subset = delivery_data[delivery_data['location_id'].isin(cluster)]\n",
    "                return subset['lat'].mean(), subset['lon'].mean()\n",
    "\n",
    "            frontier = {start_point}\n",
    "\n",
    "            while frontier:\n",
    "                centroid_lat, centroid_lon = compute_centroid(cluster)\n",
    "                cluster_points = list(cluster)\n",
    "                cluster_indices = [location_id_to_index[p] for p in cluster_points]\n",
    "\n",
    "                # Trova il punto del cluster più vicino al centroide\n",
    "                center_idx = min(\n",
    "                    cluster_indices,\n",
    "                    key=lambda idx: np.sqrt(\n",
    "                        (delivery_data.loc[idx, 'lat'] - centroid_lat) ** 2 +\n",
    "                        (delivery_data.loc[idx, 'lon'] - centroid_lon) ** 2\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # Determino la soglia in base alla densità del centro\n",
    "                center_loc_id = delivery_data.loc[center_idx, 'location_id']\n",
    "                dens_class = density_classes[center_loc_id]\n",
    "                time_threshold = thresholds[dens_class]\n",
    "\n",
    "                new_frontier = set()\n",
    "                for q in not_assigned:\n",
    "                    idx_q = location_id_to_index[q]\n",
    "                    if time_mat_min[center_idx, idx_q] <= time_threshold:\n",
    "                        cluster.add(q)\n",
    "                        new_frontier.add(q)\n",
    "                not_assigned -= new_frontier\n",
    "                frontier = new_frontier\n",
    "\n",
    "            cluster_list.append(list(cluster))\n",
    "            points_assigned.update(cluster)\n",
    "\n",
    "    # Ordina i cluster per dimensione decrescente\n",
    "    cluster_list.sort(key=len, reverse=True)\n",
    "    return cluster_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_classes = dict(zip(gdf[\"location_id\"], gdf[\"classe\"]))\n",
    "\n",
    "# --- 1. Esegui clustering (la tua funzione modificata) ---\n",
    "clusters_centroid = cluster_points_by_day_tuples_centroid(\n",
    "    metrics_df,\n",
    "    delivery_points,\n",
    "    time_mat_min,\n",
    "    location_id_to_index,\n",
    "    density_classes\n",
    ")\n",
    "\n",
    "# --- 2. Stampa i cluster e i punti ---\n",
    "for i, cluster in enumerate(clusters_centroid, start=1):\n",
    "    print(f\"\\nCluster {i} ({len(cluster)} punti):\")\n",
    "    for loc_id in cluster:\n",
    "        # opzionale: mostra anche lat/lon\n",
    "        point = delivery_points[delivery_points['location_id'] == loc_id].iloc[0]\n",
    "        print(f\"  {loc_id} -> lat: {point['lat']}, lon: {point['lon']}\")\n",
    "\n",
    "# --- 3. Salva in una variabile in memoria ---\n",
    "# Dizionario: cluster_id -> lista location_id\n",
    "clusters_dict = {i+1: cluster for i, cluster in enumerate(clusters_centroid)}\n",
    "\n",
    "# Ora puoi accedere in memoria\n",
    "print(\"\\nVariabile clusters_dict pronta:\")\n",
    "print(clusters_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Analisi generali finali e salvataggio del file pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numero totale di clienti (punti di partenza):\", len(delivery_points))\n",
    "print(\"Numero totale di cluster (finali)\", len(clusters_dict))\n",
    "# Ordina i cluster per numero di punti (decrescente)\n",
    "clusters_sorted = sorted(clusters_dict.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "# Prendi i primi 5 cluster\n",
    "top5_clusters = clusters_sorted[:5]\n",
    "\n",
    "print(\"Top 5 cluster con più punti:\")\n",
    "for cluster_id, points in top5_clusters:\n",
    "    print(f\"\\nCluster {cluster_id} ({len(points)} punti): {points}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Funzione per calcolare il punto di riferimento (quello più vicino al centroide)\n",
    "def trova_punto_riferimento(cluster, delivery_points, location_id_to_index):\n",
    "    subset = delivery_points[delivery_points['location_id'].isin(cluster)]\n",
    "    centroid_lat = subset['lat'].mean()\n",
    "    centroid_lon = subset['lon'].mean()\n",
    "\n",
    "    # Trova il punto del cluster più vicino al centroide\n",
    "    ref_id = min(\n",
    "        cluster,\n",
    "        key=lambda loc_id: np.sqrt(\n",
    "            (delivery_points.loc[location_id_to_index[loc_id], 'lat'] - centroid_lat) ** 2 +\n",
    "            (delivery_points.loc[location_id_to_index[loc_id], 'lon'] - centroid_lon) ** 2\n",
    "        )\n",
    "    )\n",
    "    return ref_id\n",
    "\n",
    "# Crea dizionario finale\n",
    "clusters_pickle_dict = {}\n",
    "for i, cluster in enumerate(clusters_centroid, start=1):\n",
    "    ref_point = trova_punto_riferimento(cluster, delivery_points, location_id_to_index)\n",
    "    clusters_pickle_dict[i] = (ref_point, cluster)\n",
    "\n",
    "# Salva su file pickle\n",
    "with open(\"clusters_output_punti_simili_OND.pkl\", \"wb\") as f:\n",
    "    pickle.dump(clusters_pickle_dict, f)\n",
    "\n",
    "print(\"✅ File clusters_output.pkl salvato con successo!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# Open file e Mappa dei soli cluster con almeno due punti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "##########################\n",
    "# Full\n",
    "##########################\n",
    "# ----------------------\n",
    "percorso_file= 'clusters_output_punti_simili_ON.pkl'\n",
    "\n",
    "# Apri il file pickle in lettura binaria\n",
    "with open(percorso_file, 'rb') as f:\n",
    "    clusters_pickle_dict = pickle.load(f)\n",
    "\n",
    "# Stampa il contenuto del dizionario\n",
    "print(clusters_pickle_dict)\n",
    "\n",
    "# Conta i cluster\n",
    "num_clusters = len(clusters_pickle_dict)\n",
    "\n",
    "# Conta i punti totali (sommando la lunghezza delle liste di punti in ogni cluster)\n",
    "total_points = sum(len(t[1]) for t in clusters_pickle_dict.values())\n",
    "\n",
    "print('Numero cluster:', num_clusters)\n",
    "print('Totale punti:', total_points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo un dizionario location_id -> (lat, lon) per accesso rapido\n",
    "loc_dict = delivery_points.set_index('location_id')[['lat', 'lon']].to_dict(orient='index')\n",
    "\n",
    "lat_list = []\n",
    "lon_list = []\n",
    "colors = []\n",
    "\n",
    "cluster_count = 0\n",
    "points_count = 0\n",
    "\n",
    "for cluster_id, (ref, points) in clusters_pickle_dict.items():\n",
    "    if len(points) > 1:  # solo cluster con almeno 2 punti\n",
    "        cluster_count += 1\n",
    "        points_count += len(points)\n",
    "        # Genera colore casuale per cluster\n",
    "        color = (random.random(), random.random(), random.random())\n",
    "        for pid in points:\n",
    "            if pid in loc_dict:\n",
    "                lat_list.append(loc_dict[pid]['lat'])\n",
    "                lon_list.append(loc_dict[pid]['lon'])\n",
    "                colors.append(color)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(lon_list, lat_list, s=10, alpha=0.6, c=colors)\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Points of Clusters with at least 2 Members ON')\n",
    "plt.show()\n",
    "\n",
    "print(f'Number of clusters with >1 point: {cluster_count}')\n",
    "print(f'Total number of points in these clusters: {points_count}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
